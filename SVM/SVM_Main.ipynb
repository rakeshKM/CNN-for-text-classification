{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import numpy as np\n",
    "import argparse\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading split information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n"
     ]
    }
   ],
   "source": [
    "split_ind = []\n",
    "with open('../Datasets/SST1_dataset/datasetSplit.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = line.split(',')\n",
    "        split_ind.append(int(entry[1]))\n",
    "\n",
    "print(len(split_ind))\n",
    "\n",
    "# Merging validation set to training data\n",
    "for i in range(len(split_ind)):\n",
    "    if split_ind[i] == 3:\n",
    "        split_ind[i] = 1\n",
    "        \n",
    "N_train = split_ind.count(1)\n",
    "N_test = split_ind.count(2)\n",
    "N_category = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Phrase -> Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232 14058\n"
     ]
    }
   ],
   "source": [
    "phr_to_ind = dict()\n",
    "\n",
    "with open('../Datasets/SST1_dataset/dictionary.txt') as f:\n",
    "    for line in f:\n",
    "        entry = line.split('|')\n",
    "        phr_to_ind[entry[0]] = int(entry[1])\n",
    "\n",
    "keys = phr_to_ind.keys();\n",
    "\n",
    "print(len(phr_to_ind), phr_to_ind['Good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9645 2210\n"
     ]
    }
   ],
   "source": [
    "# Without doing the below computation directly load the stored output\n",
    "x_train_sent = []\n",
    "x_test_sent = []\n",
    "sentiment = []\n",
    "\n",
    "counter = 0\n",
    "with open('../Datasets/SST1_dataset/SentenceWithCorrection.txt') as f:\n",
    "    for line in f:\n",
    "        sent = line[:-1]\n",
    "        if(split_ind[counter] == 1):\n",
    "            x_train_sent.append(sent)\n",
    "        else:\n",
    "            x_test_sent.append(sent)\n",
    "        \n",
    "        sentiment.append(phr_to_ind[sent])\n",
    "        counter += 1\n",
    "\n",
    "print(len(x_train_sent), len(x_test_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading sentiment information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9645 2210\n"
     ]
    }
   ],
   "source": [
    "ind_to_senti = dict()\n",
    "\n",
    "with open('../Datasets/SST1_dataset/sentiment_labels.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = line.split('|')\n",
    "        ind_to_senti[int(entry[0])] = float(entry[1])\n",
    "\n",
    "y_label = []\n",
    "\n",
    "for ind in sentiment:\n",
    "    val = ind_to_senti[ind]\n",
    "    if val >= 0.0 and val <= 0.2:\n",
    "        y_label.append(0);\n",
    "    elif val > 0.2 and val <= 0.4:\n",
    "        y_label.append(1)\n",
    "    elif val > 0.4 and val <= 0.6:\n",
    "        y_label.append(2)\n",
    "    elif val > 0.6 and val <= 0.8:\n",
    "        y_label.append(3)\n",
    "    else:\n",
    "        y_label.append(4)\n",
    "\n",
    "y_train, y_test = [], []\n",
    "\n",
    "for i in range(len(y_label)):\n",
    "    label = y_label[i]\n",
    "    if split_ind[i] == 1:\n",
    "        y_train.append(label)\n",
    "    else:\n",
    "        y_test.append(label)\n",
    "        \n",
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tokenize operation\n",
    "def tokenize(sentence, grams):\n",
    "    words = sentence.split()\n",
    "    tokens = []\n",
    "    for gram in grams:\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            tokens += [\"_*_\".join(words[i:i+gram])]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def compute_ratio(poscounts, negcounts, alpha=1):\n",
    "    pos_keys = list(poscounts.keys())\n",
    "    neg_keys = list(negcounts.keys())\n",
    "    \n",
    "    alltokens = list(set( pos_keys + neg_keys))\n",
    "    dic = dict((t, i) for i, t in enumerate(alltokens))\n",
    "    d = len(dic)\n",
    "    p, q = np.ones(d) * alpha , np.ones(d) * alpha\n",
    "    for t in alltokens:\n",
    "        p[dic[t]] += poscounts[t]\n",
    "        q[dic[t]] += negcounts[t]\n",
    "    p /= abs(p).sum()\n",
    "    q /= abs(q).sum()\n",
    "    r = np.log(p/q)\n",
    "    return dic, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train and test input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams = [1,2,3]\n",
    "\n",
    "# Getting count of words belonging to positive and negative class\n",
    "poscounts = Counter()\n",
    "negcounts = Counter()\n",
    "\n",
    "counter = 0\n",
    "for sent in x_train_sent:\n",
    "    if y_train[counter] == 0:\n",
    "        poscounts.update(tokenize(sent, ngrams))\n",
    "    else:\n",
    "        negcounts.update(tokenize(sent, ngrams))\n",
    "    counter += 1\n",
    "\n",
    "dic, r = compute_ratio(poscounts, negcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length 147\n",
      "(9645, 147) (2210, 147)\n"
     ]
    }
   ],
   "source": [
    "# Arrange train data\n",
    "x_train = []\n",
    "for sent in x_train_sent:\n",
    "    tokens = tokenize(sent, ngrams)\n",
    "    indexes = []\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            indexes += [dic[t]]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    indexes = list(set(indexes))\n",
    "    indexes.sort()\n",
    "\n",
    "    data = []\n",
    "    for i in indexes:\n",
    "        data.append(r[i])\n",
    "    x_train.append(data)\n",
    "\n",
    "# Arrange test data\n",
    "x_test = []\n",
    "\n",
    "for sent in x_test_sent:\n",
    "    tokens = tokenize(sent, ngrams)\n",
    "    indexes = []\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            indexes += [dic[t]]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    indexes = list(set(indexes))\n",
    "    indexes.sort()\n",
    "\n",
    "    data = []\n",
    "    for i in indexes:\n",
    "        data.append(r[i])\n",
    "    x_test.append(data)\n",
    "\n",
    "# Get max sentence length\n",
    "max_sent_len = -1\n",
    "for i in range(len(x_train)):\n",
    "    sent_len = len(x_train[i])\n",
    "    max_sent_len = max(max_sent_len, sent_len)\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    sent_len = len(x_test[i])\n",
    "    max_sent_len = max(max_sent_len, sent_len)\n",
    "    \n",
    "print('Max sentence length', max_sent_len)\n",
    "\n",
    "X_train = np.zeros( (len(x_train_sent), max_sent_len), np.float64)\n",
    "for i in range(len(x_train)):\n",
    "    res = x_train[i]\n",
    "    X_train[i, :len(res)] = np.float64(res)\n",
    "\n",
    "X_test = np.zeros( (len(x_test_sent), max_sent_len), np.float64)\n",
    "for i in range(len(x_test)):\n",
    "    res = x_test[i]\n",
    "    X_test[i, :len(res)] = np.float64(res)\n",
    "    \n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(y_label)):\n",
    "    y_label[i] = (0) if y_label[i] == 0 else (1)\n",
    "\n",
    "svm_class = svm.SVC()\n",
    "svm_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred = svm_class.predict(X_train)\n",
    "print( 'Train Accuracy', np.sum(pred == y_train)/ len(y_train))\n",
    "\n",
    "pred_test = svm_class.predict(X_test)\n",
    "print( 'Test Accuracy', np.sum(pred_test == y_test)/ len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
