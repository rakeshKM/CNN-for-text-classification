{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras import optimizers\n",
    "\n",
    "import numpy as np\n",
    "import pickle, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Phrase -> Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232 14058\n"
     ]
    }
   ],
   "source": [
    "phr_to_ind = dict()\n",
    "\n",
    "with open('../Datasets/SST1_dataset/dictionary.txt') as f:\n",
    "    for line in f:\n",
    "        entry = line.split('|')\n",
    "        phr_to_ind[entry[0]] = int(entry[1])\n",
    "\n",
    "keys = phr_to_ind.keys();\n",
    "\n",
    "print(len(phr_to_ind), phr_to_ind['Good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n"
     ]
    }
   ],
   "source": [
    "# Without doing the below computation directly load the stored output\n",
    "sentence_list = []\n",
    "sentiment = []\n",
    "\n",
    "with open('../Datasets/SST1_dataset/SentenceWithCorrection.txt') as f:\n",
    "    for line in f:\n",
    "        sent = line[:-1]\n",
    "        sentence_list.append(sent)\n",
    "        sentiment.append(phr_to_ind[sent])\n",
    "\n",
    "print(len(sentence_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading sentiment values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232\n"
     ]
    }
   ],
   "source": [
    "ind_to_senti = dict()\n",
    "\n",
    "with open('../Datasets/SST1_dataset/sentiment_labels.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = line.split('|')\n",
    "        ind_to_senti[int(entry[0])] = float(entry[1])\n",
    "\n",
    "print(len(ind_to_senti))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Learning information about data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n",
      "9645 2210 0\n"
     ]
    }
   ],
   "source": [
    "split_ind = []\n",
    "with open('../Datasets/SST1_dataset/datasetSplit.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = line.split(',')\n",
    "        split_ind.append(int(entry[1]))\n",
    "\n",
    "print(len(split_ind))\n",
    "\n",
    "for i in range(len(split_ind)):\n",
    "    if split_ind[i] == 3:\n",
    "        split_ind[i] = 1\n",
    "        \n",
    "N_train = split_ind.count(1)\n",
    "N_test = split_ind.count(2)\n",
    "N_valid = split_ind.count(3)\n",
    "print (N_train, N_test, N_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get label based on sentiment value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510 3140 2242 3111\n"
     ]
    }
   ],
   "source": [
    "N_sent = len(sentence_list);\n",
    "N_category = 5\n",
    "\n",
    "y_label = []\n",
    "\n",
    "for ind in sentiment:\n",
    "    val = ind_to_senti[ind]\n",
    "    if val >= 0.0 and val <= 0.2:\n",
    "        y_label.append(0);\n",
    "    elif val > 0.2 and val <= 0.4:\n",
    "        y_label.append(1)\n",
    "    elif val > 0.4 and val <= 0.6:\n",
    "        y_label.append(2)\n",
    "    elif val > 0.6 and val <= 0.8:\n",
    "        y_label.append(3)\n",
    "    else:\n",
    "        y_label.append(4)\n",
    "\n",
    "print(y_label.count(0), y_label.count(1), y_label.count(2), y_label.count(3))\n",
    "\n",
    "# Labels in one-hot encoding\n",
    "y_train = np.zeros((N_train, N_category), np.uint8)\n",
    "y_test  = np.zeros((N_test , N_category), np.uint8)\n",
    "y_valid = np.zeros((N_valid, N_category), np.uint8)\n",
    "\n",
    "c1,c2,c3 = 0,0,0\n",
    "for i in range(len(y_label)):\n",
    "    label = y_label[i]\n",
    "    if split_ind[i] == 1:\n",
    "        y_train[c1, label] = 1;  c1 += 1\n",
    "    elif split_ind[i] == 2:\n",
    "        y_test [c2, label] = 1;  c2 += 1\n",
    "    else:\n",
    "        y_valid[c3, label] = 1;  c3 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Arrange data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232 21699\n"
     ]
    }
   ],
   "source": [
    "x_all = []\n",
    "max_sent_len = -1;\n",
    "max_wrd_len = -1\n",
    "wrd_to_ind = dict()\n",
    "\n",
    "ind_new = 1;\n",
    "for sent in sentence_list:\n",
    "    wrds = sent.split()\n",
    "    vec = []\n",
    "    for wrd in wrds:\n",
    "        if wrd not in wrd_to_ind.keys():\n",
    "            wrd_to_ind[wrd] = ind_new\n",
    "            ind_new += 1\n",
    "            \n",
    "        ind = wrd_to_ind[wrd]\n",
    "        vec.append(ind)\n",
    "            \n",
    "    max_sent_len = max(len(vec), max_sent_len)\n",
    "    x_all.append(vec)\n",
    "\n",
    "# Get inverse dictionary\n",
    "ind_to_wrd = dict((v, k) for k, v in wrd_to_ind.items())\n",
    "ind_to_wrd[0] = \"<PAD/>\"\n",
    "\n",
    "print(len(phr_to_ind), len(wrd_to_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9645 2210 0\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.zeros((N_train, max_sent_len), np.int32)\n",
    "# x_test  = np.zeros((N_test,  max_sent_len), np.int32)\n",
    "# x_valid = np.zeros((N_valid, max_sent_len), np.int32)\n",
    "\n",
    "x_train = []\n",
    "x_test = []\n",
    "x_valid = []\n",
    "\n",
    "c1, c2, c3 = 0,0,0\n",
    "for i in range(len(x_all)):\n",
    "    vec = x_all[i]\n",
    "    if split_ind[i] == 1:\n",
    "        x_train.append(vec);\n",
    "        c1 += 1\n",
    "    elif split_ind[i] == 2:\n",
    "        x_test.append(vec)\n",
    "        c2 += 1\n",
    "    else:\n",
    "        x_valid.append(vec)\n",
    "        c3 += 1\n",
    "\n",
    "print(c1, c2, c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(x_train, maxlen=max_sent_len)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "maxlen = max_sent_len\n",
    "max_features = len(ind_to_wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# max_features = 20000\n",
    "# maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "# batch_size = 32\n",
    "\n",
    "# print('Loading data...')\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "# print(len(x_train), 'train sequences')\n",
    "# print(len(x_test), 'test sequences')\n",
    "\n",
    "# print('Pad sequences (samples x time)')\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 128)         2777600   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,909,829.0\n",
      "Trainable params: 2,909,829\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9645 samples, validate on 2210 samples\n",
      "Epoch 1/2000\n",
      "33s - loss: 1.4223 - acc: 0.3806 - val_loss: 1.4880 - val_acc: 0.3525\n",
      "Epoch 2/2000\n",
      "32s - loss: 1.4112 - acc: 0.3896 - val_loss: 1.4540 - val_acc: 0.3588\n",
      "Epoch 3/2000\n",
      "32s - loss: 1.4105 - acc: 0.3968 - val_loss: 1.4822 - val_acc: 0.3416\n",
      "Epoch 4/2000\n",
      "32s - loss: 1.4039 - acc: 0.3976 - val_loss: 1.4394 - val_acc: 0.3738\n",
      "Epoch 5/2000\n",
      "32s - loss: 1.3943 - acc: 0.3982 - val_loss: 1.4993 - val_acc: 0.3538\n",
      "Epoch 6/2000\n",
      "32s - loss: 1.3897 - acc: 0.4012 - val_loss: 1.4640 - val_acc: 0.3493\n",
      "Epoch 7/2000\n",
      "32s - loss: 1.3852 - acc: 0.4023 - val_loss: 1.4325 - val_acc: 0.3747\n",
      "Epoch 8/2000\n",
      "32s - loss: 1.3769 - acc: 0.4104 - val_loss: 1.4407 - val_acc: 0.3733\n",
      "Epoch 9/2000\n",
      "32s - loss: 1.3618 - acc: 0.4131 - val_loss: 1.4598 - val_acc: 0.3561\n",
      "Epoch 10/2000\n",
      "32s - loss: 1.3530 - acc: 0.4188 - val_loss: 1.4777 - val_acc: 0.3611\n",
      "Epoch 11/2000\n",
      "32s - loss: 1.3558 - acc: 0.4139 - val_loss: 1.4338 - val_acc: 0.3683\n",
      "Epoch 12/2000\n",
      "32s - loss: 1.3452 - acc: 0.4187 - val_loss: 1.4466 - val_acc: 0.3579\n",
      "Epoch 13/2000\n",
      "32s - loss: 1.3363 - acc: 0.4217 - val_loss: 1.5117 - val_acc: 0.3624\n",
      "Epoch 14/2000\n",
      "32s - loss: 1.3299 - acc: 0.4321 - val_loss: 1.4710 - val_acc: 0.3462\n",
      "Epoch 15/2000\n",
      "32s - loss: 1.3241 - acc: 0.4299 - val_loss: 1.4828 - val_acc: 0.3484\n",
      "Epoch 16/2000\n",
      "32s - loss: 1.3087 - acc: 0.4335 - val_loss: 1.4102 - val_acc: 0.3932\n",
      "Epoch 17/2000\n",
      "32s - loss: 1.2970 - acc: 0.4412 - val_loss: 1.4302 - val_acc: 0.3769\n",
      "Epoch 18/2000\n",
      "33s - loss: 1.2945 - acc: 0.4393 - val_loss: 1.4467 - val_acc: 0.3701\n",
      "Epoch 19/2000\n",
      "32s - loss: 1.2873 - acc: 0.4412 - val_loss: 1.4485 - val_acc: 0.3665\n",
      "Epoch 20/2000\n",
      "32s - loss: 1.2816 - acc: 0.4440 - val_loss: 1.4405 - val_acc: 0.3814\n",
      "Epoch 21/2000\n",
      "32s - loss: 1.2714 - acc: 0.4497 - val_loss: 1.4121 - val_acc: 0.3941\n",
      "Epoch 22/2000\n",
      "32s - loss: 1.2577 - acc: 0.4526 - val_loss: 1.4032 - val_acc: 0.3977\n",
      "Epoch 23/2000\n",
      "32s - loss: 1.2571 - acc: 0.4546 - val_loss: 1.4487 - val_acc: 0.3869\n",
      "Epoch 24/2000\n",
      "32s - loss: 1.2433 - acc: 0.4605 - val_loss: 1.5846 - val_acc: 0.3548\n",
      "Epoch 25/2000\n",
      "32s - loss: 1.2399 - acc: 0.4604 - val_loss: 1.5742 - val_acc: 0.3566\n",
      "Epoch 26/2000\n",
      "32s - loss: 1.2387 - acc: 0.4610 - val_loss: 1.4372 - val_acc: 0.3950\n",
      "Epoch 27/2000\n",
      "32s - loss: 1.2172 - acc: 0.4731 - val_loss: 1.4128 - val_acc: 0.4041\n",
      "Epoch 28/2000\n",
      "32s - loss: 1.2115 - acc: 0.4712 - val_loss: 1.4008 - val_acc: 0.4036\n",
      "Epoch 29/2000\n",
      "32s - loss: 1.2086 - acc: 0.4709 - val_loss: 1.3890 - val_acc: 0.3995\n",
      "Epoch 30/2000\n",
      "32s - loss: 1.1871 - acc: 0.4847 - val_loss: 1.4512 - val_acc: 0.3873\n",
      "Epoch 31/2000\n",
      "32s - loss: 1.1882 - acc: 0.4797 - val_loss: 1.3820 - val_acc: 0.4086\n",
      "Epoch 32/2000\n",
      "32s - loss: 1.1820 - acc: 0.4830 - val_loss: 1.4281 - val_acc: 0.4014\n",
      "Epoch 33/2000\n",
      "32s - loss: 1.1774 - acc: 0.4790 - val_loss: 1.5703 - val_acc: 0.3706\n",
      "Epoch 34/2000\n",
      "32s - loss: 1.1582 - acc: 0.4984 - val_loss: 1.4362 - val_acc: 0.4005\n",
      "Epoch 35/2000\n",
      "32s - loss: 1.1602 - acc: 0.4922 - val_loss: 1.4020 - val_acc: 0.4032\n",
      "Epoch 36/2000\n",
      "32s - loss: 1.1452 - acc: 0.4982 - val_loss: 1.4671 - val_acc: 0.3900\n",
      "Epoch 37/2000\n",
      "32s - loss: 1.1404 - acc: 0.5005 - val_loss: 1.4099 - val_acc: 0.3955\n",
      "Epoch 38/2000\n",
      "32s - loss: 1.1255 - acc: 0.5102 - val_loss: 1.3958 - val_acc: 0.4086\n",
      "Epoch 39/2000\n",
      "32s - loss: 1.1200 - acc: 0.5094 - val_loss: 1.4801 - val_acc: 0.3860\n",
      "Epoch 40/2000\n",
      "32s - loss: 1.1190 - acc: 0.5081 - val_loss: 1.4414 - val_acc: 0.4014\n",
      "Epoch 41/2000\n",
      "32s - loss: 1.1042 - acc: 0.5207 - val_loss: 1.4676 - val_acc: 0.3900\n",
      "Epoch 42/2000\n",
      "32s - loss: 1.1172 - acc: 0.5106 - val_loss: 1.4199 - val_acc: 0.3973\n",
      "Epoch 43/2000\n",
      "32s - loss: 1.0941 - acc: 0.5220 - val_loss: 1.5302 - val_acc: 0.3937\n",
      "Epoch 44/2000\n",
      "32s - loss: 1.0888 - acc: 0.5217 - val_loss: 1.4275 - val_acc: 0.4054\n",
      "Epoch 45/2000\n",
      "32s - loss: 1.0743 - acc: 0.5326 - val_loss: 1.4757 - val_acc: 0.4086\n",
      "Epoch 46/2000\n",
      "32s - loss: 1.0671 - acc: 0.5304 - val_loss: 1.4608 - val_acc: 0.3977\n",
      "Epoch 47/2000\n",
      "32s - loss: 1.0605 - acc: 0.5354 - val_loss: 1.6525 - val_acc: 0.3796\n",
      "Epoch 48/2000\n",
      "32s - loss: 1.0514 - acc: 0.5401 - val_loss: 1.4531 - val_acc: 0.3891\n",
      "Epoch 49/2000\n",
      "32s - loss: 1.0466 - acc: 0.5385 - val_loss: 1.4351 - val_acc: 0.4122\n",
      "Epoch 50/2000\n",
      "32s - loss: 1.0426 - acc: 0.5416 - val_loss: 1.4544 - val_acc: 0.3873\n",
      "Epoch 51/2000\n",
      "32s - loss: 1.0439 - acc: 0.5416 - val_loss: 1.5356 - val_acc: 0.3950\n",
      "Epoch 52/2000\n",
      "32s - loss: 1.0223 - acc: 0.5513 - val_loss: 1.4599 - val_acc: 0.4063\n",
      "Epoch 53/2000\n",
      "32s - loss: 1.0273 - acc: 0.5531 - val_loss: 1.5009 - val_acc: 0.4077\n",
      "Epoch 54/2000\n",
      "32s - loss: 1.0154 - acc: 0.5493 - val_loss: 1.5929 - val_acc: 0.3769\n",
      "Epoch 55/2000\n",
      "32s - loss: 1.0097 - acc: 0.5568 - val_loss: 1.5088 - val_acc: 0.3819\n",
      "Epoch 56/2000\n",
      "32s - loss: 1.0013 - acc: 0.5596 - val_loss: 1.5071 - val_acc: 0.3964\n",
      "Epoch 57/2000\n",
      "32s - loss: 1.0026 - acc: 0.5555 - val_loss: 1.7755 - val_acc: 0.3751\n",
      "Epoch 58/2000\n",
      "32s - loss: 0.9801 - acc: 0.5631 - val_loss: 1.6931 - val_acc: 0.3819\n",
      "Epoch 59/2000\n",
      "32s - loss: 0.9875 - acc: 0.5663 - val_loss: 1.7249 - val_acc: 0.3552\n",
      "Epoch 60/2000\n",
      "32s - loss: 0.9854 - acc: 0.5669 - val_loss: 1.5241 - val_acc: 0.4050\n",
      "Epoch 61/2000\n",
      "32s - loss: 0.9744 - acc: 0.5692 - val_loss: 1.6785 - val_acc: 0.3864\n",
      "Epoch 62/2000\n",
      "32s - loss: 0.9564 - acc: 0.5726 - val_loss: 1.5580 - val_acc: 0.3855\n",
      "Epoch 63/2000\n",
      "32s - loss: 0.9588 - acc: 0.5784 - val_loss: 1.5225 - val_acc: 0.4045\n",
      "Epoch 64/2000\n",
      "32s - loss: 0.9592 - acc: 0.5791 - val_loss: 1.5522 - val_acc: 0.4172\n",
      "Epoch 65/2000\n",
      "32s - loss: 0.9485 - acc: 0.5862 - val_loss: 1.8058 - val_acc: 0.3502\n",
      "Epoch 66/2000\n",
      "32s - loss: 0.9385 - acc: 0.5879 - val_loss: 1.9767 - val_acc: 0.3480\n",
      "Epoch 67/2000\n",
      "32s - loss: 0.9351 - acc: 0.5911 - val_loss: 1.7092 - val_acc: 0.3828\n",
      "Epoch 68/2000\n",
      "32s - loss: 0.9352 - acc: 0.5920 - val_loss: 1.6648 - val_acc: 0.3937\n",
      "Epoch 69/2000\n",
      "32s - loss: 0.9356 - acc: 0.5943 - val_loss: 1.5501 - val_acc: 0.4122\n",
      "Epoch 70/2000\n",
      "32s - loss: 0.9179 - acc: 0.5998 - val_loss: 1.7138 - val_acc: 0.3814\n",
      "Epoch 71/2000\n",
      "32s - loss: 0.9087 - acc: 0.6051 - val_loss: 1.6084 - val_acc: 0.4109\n",
      "Epoch 72/2000\n",
      "32s - loss: 0.9191 - acc: 0.5995 - val_loss: 1.6570 - val_acc: 0.3719\n",
      "Epoch 73/2000\n",
      "32s - loss: 0.9316 - acc: 0.5923 - val_loss: 1.6637 - val_acc: 0.3887\n",
      "Epoch 74/2000\n",
      "32s - loss: 0.8896 - acc: 0.6094 - val_loss: 1.6002 - val_acc: 0.3792\n",
      "Epoch 75/2000\n",
      "32s - loss: 0.8929 - acc: 0.6125 - val_loss: 1.6210 - val_acc: 0.3801\n",
      "Epoch 76/2000\n",
      "32s - loss: 0.8910 - acc: 0.6164 - val_loss: 1.7074 - val_acc: 0.3968\n",
      "Epoch 77/2000\n",
      "32s - loss: 0.8815 - acc: 0.6134 - val_loss: 1.6310 - val_acc: 0.3977\n",
      "Epoch 78/2000\n",
      "32s - loss: 0.8694 - acc: 0.6229 - val_loss: 1.6782 - val_acc: 0.4036\n",
      "Epoch 79/2000\n",
      "32s - loss: 0.8721 - acc: 0.6218 - val_loss: 1.7945 - val_acc: 0.3805\n",
      "Epoch 80/2000\n",
      "32s - loss: 0.8561 - acc: 0.6308 - val_loss: 1.6575 - val_acc: 0.3914\n",
      "Epoch 81/2000\n",
      "32s - loss: 0.8564 - acc: 0.6212 - val_loss: 1.8069 - val_acc: 0.3756\n",
      "Epoch 82/2000\n",
      "32s - loss: 0.8499 - acc: 0.6294 - val_loss: 1.6478 - val_acc: 0.3982\n",
      "Epoch 83/2000\n",
      "32s - loss: 0.8356 - acc: 0.6440 - val_loss: 1.6785 - val_acc: 0.3955\n",
      "Epoch 84/2000\n",
      "32s - loss: 0.8335 - acc: 0.6377 - val_loss: 1.6911 - val_acc: 0.4018\n",
      "Epoch 85/2000\n",
      "32s - loss: 0.8246 - acc: 0.6448 - val_loss: 1.7867 - val_acc: 0.3919\n",
      "Epoch 86/2000\n",
      "32s - loss: 0.8385 - acc: 0.6368 - val_loss: 1.8078 - val_acc: 0.3683\n",
      "Epoch 87/2000\n",
      "32s - loss: 0.8177 - acc: 0.6469 - val_loss: 1.6880 - val_acc: 0.3968\n",
      "Epoch 88/2000\n",
      "31s - loss: 0.8358 - acc: 0.6404 - val_loss: 1.6737 - val_acc: 0.4077\n",
      "Epoch 89/2000\n",
      "32s - loss: 0.8171 - acc: 0.6476 - val_loss: 1.7307 - val_acc: 0.3846\n",
      "Epoch 90/2000\n",
      "32s - loss: 0.8000 - acc: 0.6580 - val_loss: 2.4150 - val_acc: 0.3520\n",
      "Epoch 91/2000\n",
      "32s - loss: 0.8085 - acc: 0.6557 - val_loss: 1.8099 - val_acc: 0.3941\n",
      "Epoch 92/2000\n",
      "32s - loss: 0.8039 - acc: 0.6535 - val_loss: 2.7104 - val_acc: 0.3335\n",
      "Epoch 93/2000\n",
      "32s - loss: 0.7829 - acc: 0.6659 - val_loss: 1.7930 - val_acc: 0.3846\n",
      "Epoch 94/2000\n",
      "32s - loss: 0.7890 - acc: 0.6594 - val_loss: 1.7659 - val_acc: 0.3950\n",
      "Epoch 95/2000\n",
      "32s - loss: 0.7842 - acc: 0.6612 - val_loss: 2.1618 - val_acc: 0.3710\n",
      "Epoch 96/2000\n",
      "32s - loss: 0.7661 - acc: 0.6711 - val_loss: 1.7562 - val_acc: 0.3855\n",
      "Epoch 97/2000\n",
      "32s - loss: 0.7644 - acc: 0.6736 - val_loss: 1.8484 - val_acc: 0.3738\n",
      "Epoch 98/2000\n",
      "32s - loss: 0.7609 - acc: 0.6763 - val_loss: 1.8813 - val_acc: 0.3842\n",
      "Epoch 99/2000\n",
      "32s - loss: 0.7570 - acc: 0.6772 - val_loss: 1.8004 - val_acc: 0.4118\n",
      "Epoch 100/2000\n",
      "32s - loss: 0.7511 - acc: 0.6830 - val_loss: 1.9228 - val_acc: 0.4045\n",
      "Epoch 101/2000\n",
      "32s - loss: 0.7684 - acc: 0.6709 - val_loss: 1.8535 - val_acc: 0.3579\n",
      "Epoch 102/2000\n",
      "32s - loss: 0.7431 - acc: 0.6830 - val_loss: 1.9061 - val_acc: 0.4027\n",
      "Epoch 103/2000\n",
      "32s - loss: 0.7370 - acc: 0.6873 - val_loss: 1.9164 - val_acc: 0.3783\n",
      "Epoch 104/2000\n",
      "32s - loss: 0.7491 - acc: 0.6853 - val_loss: 2.0134 - val_acc: 0.3643\n",
      "Epoch 105/2000\n",
      "32s - loss: 0.7364 - acc: 0.6885 - val_loss: 1.8899 - val_acc: 0.3910\n",
      "Epoch 106/2000\n",
      "32s - loss: 0.7296 - acc: 0.6934 - val_loss: 1.9357 - val_acc: 0.3937\n",
      "Epoch 107/2000\n",
      "32s - loss: 0.7279 - acc: 0.6936 - val_loss: 1.9355 - val_acc: 0.4045\n",
      "Epoch 108/2000\n",
      "32s - loss: 0.7000 - acc: 0.7053 - val_loss: 1.9206 - val_acc: 0.3928\n",
      "Epoch 109/2000\n",
      "32s - loss: 0.7286 - acc: 0.6906 - val_loss: 1.9595 - val_acc: 0.3733\n",
      "Epoch 110/2000\n",
      "32s - loss: 0.7029 - acc: 0.7013 - val_loss: 1.9290 - val_acc: 0.3787\n",
      "Epoch 111/2000\n",
      "32s - loss: 0.7006 - acc: 0.7052 - val_loss: 2.0575 - val_acc: 0.3778\n",
      "Epoch 112/2000\n",
      "32s - loss: 0.6914 - acc: 0.7081 - val_loss: 2.1423 - val_acc: 0.3928\n",
      "Epoch 113/2000\n",
      "32s - loss: 0.6951 - acc: 0.7072 - val_loss: 1.9548 - val_acc: 0.3873\n",
      "Epoch 114/2000\n",
      "32s - loss: 0.6761 - acc: 0.7181 - val_loss: 2.0191 - val_acc: 0.3887\n",
      "Epoch 115/2000\n",
      "32s - loss: 0.6824 - acc: 0.7053 - val_loss: 2.0320 - val_acc: 0.3905\n",
      "Epoch 116/2000\n",
      "32s - loss: 0.6749 - acc: 0.7212 - val_loss: 1.9619 - val_acc: 0.3765\n",
      "Epoch 117/2000\n",
      "32s - loss: 0.6810 - acc: 0.7129 - val_loss: 2.2984 - val_acc: 0.3923\n",
      "Epoch 118/2000\n",
      "32s - loss: 0.6669 - acc: 0.7168 - val_loss: 2.2792 - val_acc: 0.3502\n",
      "Epoch 119/2000\n",
      "32s - loss: 0.6577 - acc: 0.7249 - val_loss: 2.4012 - val_acc: 0.3548\n",
      "Epoch 120/2000\n",
      "32s - loss: 0.6679 - acc: 0.7190 - val_loss: 2.0949 - val_acc: 0.3792\n",
      "Epoch 121/2000\n",
      "32s - loss: 0.6530 - acc: 0.7300 - val_loss: 2.0745 - val_acc: 0.3991\n",
      "Epoch 122/2000\n",
      "32s - loss: 0.6485 - acc: 0.7322 - val_loss: 2.0339 - val_acc: 0.3882\n",
      "Epoch 123/2000\n",
      "32s - loss: 0.6384 - acc: 0.7333 - val_loss: 2.0258 - val_acc: 0.3968\n",
      "Epoch 124/2000\n",
      "32s - loss: 0.6365 - acc: 0.7370 - val_loss: 2.2750 - val_acc: 0.3724\n",
      "Epoch 125/2000\n",
      "32s - loss: 0.6392 - acc: 0.7408 - val_loss: 2.0277 - val_acc: 0.3905\n",
      "Epoch 126/2000\n",
      "32s - loss: 0.6237 - acc: 0.7396 - val_loss: 2.5328 - val_acc: 0.3480\n",
      "Epoch 127/2000\n",
      "32s - loss: 0.6113 - acc: 0.7516 - val_loss: 2.1656 - val_acc: 0.3810\n",
      "Epoch 128/2000\n",
      "32s - loss: 0.6132 - acc: 0.7484 - val_loss: 2.1531 - val_acc: 0.3910\n",
      "Epoch 129/2000\n",
      "31s - loss: 0.6077 - acc: 0.7499 - val_loss: 2.1998 - val_acc: 0.3882\n",
      "Epoch 130/2000\n",
      "32s - loss: 0.6010 - acc: 0.7522 - val_loss: 2.0911 - val_acc: 0.3860\n",
      "Epoch 131/2000\n",
      "32s - loss: 0.6056 - acc: 0.7496 - val_loss: 2.3655 - val_acc: 0.3701\n",
      "Epoch 132/2000\n",
      "32s - loss: 0.6001 - acc: 0.7516 - val_loss: 2.1825 - val_acc: 0.3783\n",
      "Epoch 133/2000\n",
      "32s - loss: 0.5842 - acc: 0.7664 - val_loss: 2.3108 - val_acc: 0.3805\n",
      "Epoch 134/2000\n",
      "32s - loss: 0.5867 - acc: 0.7611 - val_loss: 2.4899 - val_acc: 0.3584\n",
      "Epoch 135/2000\n",
      "32s - loss: 0.5989 - acc: 0.7538 - val_loss: 2.1380 - val_acc: 0.3900\n",
      "Epoch 136/2000\n",
      "31s - loss: 0.5615 - acc: 0.7743 - val_loss: 2.2662 - val_acc: 0.3878\n",
      "Epoch 137/2000\n",
      "31s - loss: 0.5596 - acc: 0.7718 - val_loss: 2.1985 - val_acc: 0.3833\n",
      "Epoch 138/2000\n",
      "32s - loss: 0.5830 - acc: 0.7664 - val_loss: 2.3516 - val_acc: 0.3611\n",
      "Epoch 139/2000\n",
      "31s - loss: 0.5626 - acc: 0.7725 - val_loss: 2.9331 - val_acc: 0.3267\n",
      "Epoch 140/2000\n",
      "32s - loss: 0.5279 - acc: 0.7852 - val_loss: 4.6544 - val_acc: 0.2937\n",
      "Epoch 141/2000\n",
      "32s - loss: 0.5618 - acc: 0.7726 - val_loss: 2.1923 - val_acc: 0.3873\n",
      "Epoch 142/2000\n",
      "34s - loss: 0.5547 - acc: 0.7745 - val_loss: 2.2384 - val_acc: 0.3878\n",
      "Epoch 143/2000\n",
      "32s - loss: 0.5350 - acc: 0.7843 - val_loss: 2.2283 - val_acc: 0.3742\n",
      "Epoch 144/2000\n",
      "32s - loss: 0.5289 - acc: 0.7892 - val_loss: 2.2826 - val_acc: 0.3756\n",
      "Epoch 145/2000\n",
      "32s - loss: 0.5091 - acc: 0.8008 - val_loss: 2.3836 - val_acc: 0.3792\n",
      "Epoch 146/2000\n",
      "32s - loss: 0.5427 - acc: 0.7874 - val_loss: 2.3061 - val_acc: 0.3814\n",
      "Epoch 147/2000\n",
      "32s - loss: 0.4985 - acc: 0.8066 - val_loss: 2.3585 - val_acc: 0.3778\n",
      "Epoch 148/2000\n",
      "32s - loss: 0.5177 - acc: 0.7997 - val_loss: 2.2517 - val_acc: 0.3738\n",
      "Epoch 149/2000\n",
      "32s - loss: 0.5191 - acc: 0.7941 - val_loss: 2.7994 - val_acc: 0.3584\n",
      "Epoch 150/2000\n",
      "32s - loss: 0.4795 - acc: 0.8121 - val_loss: 2.2860 - val_acc: 0.3860\n",
      "Epoch 151/2000\n",
      "32s - loss: 0.4921 - acc: 0.8077 - val_loss: 2.3340 - val_acc: 0.3647\n",
      "Epoch 152/2000\n",
      "31s - loss: 0.4922 - acc: 0.8089 - val_loss: 2.3418 - val_acc: 0.3765\n",
      "Epoch 153/2000\n",
      "32s - loss: 0.4835 - acc: 0.8114 - val_loss: 2.4063 - val_acc: 0.3738\n",
      "Epoch 154/2000\n",
      "31s - loss: 0.4836 - acc: 0.8119 - val_loss: 2.6169 - val_acc: 0.3814\n",
      "Epoch 155/2000\n",
      "32s - loss: 0.4841 - acc: 0.8085 - val_loss: 2.5414 - val_acc: 0.3643\n",
      "Epoch 156/2000\n",
      "32s - loss: 0.4824 - acc: 0.8141 - val_loss: 2.4547 - val_acc: 0.3796\n",
      "Epoch 157/2000\n",
      "32s - loss: 0.4503 - acc: 0.8222 - val_loss: 2.4046 - val_acc: 0.3928\n",
      "Epoch 158/2000\n",
      "32s - loss: 0.4747 - acc: 0.8175 - val_loss: 2.3812 - val_acc: 0.3828\n",
      "Epoch 159/2000\n",
      "32s - loss: 0.4421 - acc: 0.8317 - val_loss: 2.5828 - val_acc: 0.3887\n",
      "Epoch 160/2000\n",
      "32s - loss: 0.4431 - acc: 0.8303 - val_loss: 2.4777 - val_acc: 0.3900\n",
      "Epoch 161/2000\n",
      "32s - loss: 0.4642 - acc: 0.8258 - val_loss: 2.4238 - val_acc: 0.3828\n",
      "Epoch 162/2000\n",
      "32s - loss: 0.4356 - acc: 0.8335 - val_loss: 2.5850 - val_acc: 0.3747\n",
      "Epoch 163/2000\n",
      "32s - loss: 0.4203 - acc: 0.8416 - val_loss: 3.1556 - val_acc: 0.3719\n",
      "Epoch 164/2000\n",
      "32s - loss: 0.4317 - acc: 0.8360 - val_loss: 2.5103 - val_acc: 0.3765\n",
      "Epoch 165/2000\n",
      "32s - loss: 0.4419 - acc: 0.8353 - val_loss: 2.5799 - val_acc: 0.3701\n",
      "Epoch 166/2000\n",
      "32s - loss: 0.4253 - acc: 0.8366 - val_loss: 2.6006 - val_acc: 0.3891\n",
      "Epoch 167/2000\n",
      "32s - loss: 0.4277 - acc: 0.8384 - val_loss: 2.5333 - val_acc: 0.3828\n",
      "Epoch 168/2000\n",
      "32s - loss: 0.4207 - acc: 0.8418 - val_loss: 2.5370 - val_acc: 0.3792\n",
      "Epoch 169/2000\n",
      "32s - loss: 0.3796 - acc: 0.8619 - val_loss: 3.3661 - val_acc: 0.3570\n",
      "Epoch 170/2000\n",
      "32s - loss: 0.3960 - acc: 0.8554 - val_loss: 2.7913 - val_acc: 0.3534\n",
      "Epoch 171/2000\n",
      "32s - loss: 0.3859 - acc: 0.8554 - val_loss: 2.7402 - val_acc: 0.3855\n",
      "Epoch 172/2000\n",
      "32s - loss: 0.3967 - acc: 0.8523 - val_loss: 2.6333 - val_acc: 0.3670\n",
      "Epoch 173/2000\n",
      "32s - loss: 0.3740 - acc: 0.8583 - val_loss: 2.9840 - val_acc: 0.3584\n",
      "Epoch 174/2000\n",
      "32s - loss: 0.3759 - acc: 0.8646 - val_loss: 2.6862 - val_acc: 0.3787\n",
      "Epoch 175/2000\n",
      "32s - loss: 0.3981 - acc: 0.8545 - val_loss: 2.6553 - val_acc: 0.3810\n",
      "Epoch 176/2000\n",
      "32s - loss: 0.3665 - acc: 0.8632 - val_loss: 2.8075 - val_acc: 0.3837\n",
      "Epoch 177/2000\n",
      "32s - loss: 0.3670 - acc: 0.8657 - val_loss: 2.7832 - val_acc: 0.3724\n",
      "Epoch 178/2000\n",
      "32s - loss: 0.3448 - acc: 0.8762 - val_loss: 2.7483 - val_acc: 0.3507\n",
      "Epoch 179/2000\n",
      "32s - loss: 0.3763 - acc: 0.8610 - val_loss: 2.9469 - val_acc: 0.3620\n",
      "Epoch 180/2000\n",
      "32s - loss: 0.3634 - acc: 0.8677 - val_loss: 2.7059 - val_acc: 0.3778\n",
      "Epoch 181/2000\n",
      "32s - loss: 0.3119 - acc: 0.8879 - val_loss: 2.7803 - val_acc: 0.3647\n",
      "Epoch 182/2000\n",
      "32s - loss: 0.3582 - acc: 0.8683 - val_loss: 3.6854 - val_acc: 0.3253\n",
      "Epoch 183/2000\n",
      "32s - loss: 0.3365 - acc: 0.8769 - val_loss: 2.8656 - val_acc: 0.3742\n",
      "Epoch 184/2000\n",
      "32s - loss: 0.3195 - acc: 0.8820 - val_loss: 2.7172 - val_acc: 0.3833\n",
      "Epoch 185/2000\n",
      "32s - loss: 0.3146 - acc: 0.8907 - val_loss: 3.1382 - val_acc: 0.3701\n",
      "Epoch 186/2000\n",
      "32s - loss: 0.3051 - acc: 0.8948 - val_loss: 2.8217 - val_acc: 0.3774\n",
      "Epoch 187/2000\n",
      "32s - loss: 0.3299 - acc: 0.8839 - val_loss: 2.8752 - val_acc: 0.3538\n",
      "Epoch 188/2000\n",
      "32s - loss: 0.3331 - acc: 0.8798 - val_loss: 2.7736 - val_acc: 0.3502\n",
      "Epoch 189/2000\n",
      "32s - loss: 0.3305 - acc: 0.8854 - val_loss: 2.9204 - val_acc: 0.3851\n",
      "Epoch 190/2000\n",
      "32s - loss: 0.3272 - acc: 0.8813 - val_loss: 2.8073 - val_acc: 0.3796\n",
      "Epoch 191/2000\n",
      "32s - loss: 0.3203 - acc: 0.8838 - val_loss: 3.1210 - val_acc: 0.3810\n",
      "Epoch 192/2000\n",
      "32s - loss: 0.2799 - acc: 0.9024 - val_loss: 2.9165 - val_acc: 0.3593\n",
      "Epoch 193/2000\n",
      "32s - loss: 0.2832 - acc: 0.9039 - val_loss: 2.9496 - val_acc: 0.3810\n",
      "Epoch 194/2000\n",
      "32s - loss: 0.2848 - acc: 0.9026 - val_loss: 3.2607 - val_acc: 0.3543\n",
      "Epoch 195/2000\n",
      "32s - loss: 0.2773 - acc: 0.9054 - val_loss: 3.0011 - val_acc: 0.3719\n",
      "Epoch 196/2000\n",
      "32s - loss: 0.3023 - acc: 0.8945 - val_loss: 3.0382 - val_acc: 0.3778\n",
      "Epoch 197/2000\n",
      "32s - loss: 0.2920 - acc: 0.8961 - val_loss: 2.9105 - val_acc: 0.3919\n",
      "Epoch 198/2000\n",
      "32s - loss: 0.2644 - acc: 0.9091 - val_loss: 2.9014 - val_acc: 0.3715\n",
      "Epoch 199/2000\n",
      "32s - loss: 0.2627 - acc: 0.9098 - val_loss: 3.0203 - val_acc: 0.3810\n",
      "Epoch 200/2000\n",
      "32s - loss: 0.2660 - acc: 0.9086 - val_loss: 3.0444 - val_acc: 0.3851\n",
      "Epoch 201/2000\n",
      "32s - loss: 0.2505 - acc: 0.9133 - val_loss: 3.4001 - val_acc: 0.3670\n",
      "Epoch 202/2000\n",
      "32s - loss: 0.2747 - acc: 0.9041 - val_loss: 2.9213 - val_acc: 0.3692\n",
      "Epoch 203/2000\n",
      "32s - loss: 0.2490 - acc: 0.9159 - val_loss: 2.9690 - val_acc: 0.3769\n",
      "Epoch 204/2000\n",
      "32s - loss: 0.2487 - acc: 0.9149 - val_loss: 3.0520 - val_acc: 0.3787\n",
      "Epoch 205/2000\n",
      "32s - loss: 0.2403 - acc: 0.9160 - val_loss: 3.1185 - val_acc: 0.3760\n",
      "Epoch 206/2000\n",
      "32s - loss: 0.2498 - acc: 0.9168 - val_loss: 3.0755 - val_acc: 0.3860\n",
      "Epoch 207/2000\n",
      "32s - loss: 0.2170 - acc: 0.9237 - val_loss: 3.2098 - val_acc: 0.3398\n",
      "Epoch 208/2000\n",
      "32s - loss: 0.2305 - acc: 0.9222 - val_loss: 3.1661 - val_acc: 0.3824\n",
      "Epoch 209/2000\n",
      "32s - loss: 0.2389 - acc: 0.9179 - val_loss: 3.3810 - val_acc: 0.3683\n",
      "Epoch 210/2000\n",
      "32s - loss: 0.2321 - acc: 0.9196 - val_loss: 3.1625 - val_acc: 0.3837\n",
      "Epoch 211/2000\n",
      "32s - loss: 0.2222 - acc: 0.9213 - val_loss: 3.3815 - val_acc: 0.3751\n",
      "Epoch 212/2000\n",
      "32s - loss: 0.2330 - acc: 0.9228 - val_loss: 3.3929 - val_acc: 0.3801\n",
      "Epoch 213/2000\n",
      "32s - loss: 0.2362 - acc: 0.9191 - val_loss: 3.1942 - val_acc: 0.3774\n",
      "Epoch 214/2000\n",
      "32s - loss: 0.2021 - acc: 0.9340 - val_loss: 3.2533 - val_acc: 0.3742\n",
      "Epoch 215/2000\n",
      "32s - loss: 0.2340 - acc: 0.9217 - val_loss: 3.2190 - val_acc: 0.3747\n",
      "Epoch 216/2000\n",
      "32s - loss: 0.1993 - acc: 0.9333 - val_loss: 3.1626 - val_acc: 0.3665\n",
      "Epoch 217/2000\n",
      "32s - loss: 0.2055 - acc: 0.9312 - val_loss: 3.1981 - val_acc: 0.3769\n",
      "Epoch 218/2000\n",
      "32s - loss: 0.1825 - acc: 0.9399 - val_loss: 3.3705 - val_acc: 0.3679\n",
      "Epoch 219/2000\n",
      "32s - loss: 0.1933 - acc: 0.9359 - val_loss: 3.2083 - val_acc: 0.3923\n",
      "Epoch 220/2000\n",
      "32s - loss: 0.2026 - acc: 0.9317 - val_loss: 3.3004 - val_acc: 0.3747\n",
      "Epoch 221/2000\n",
      "32s - loss: 0.1911 - acc: 0.9375 - val_loss: 3.4267 - val_acc: 0.3706\n",
      "Epoch 222/2000\n",
      "32s - loss: 0.1861 - acc: 0.9377 - val_loss: 3.3143 - val_acc: 0.3796\n",
      "Epoch 223/2000\n",
      "32s - loss: 0.1938 - acc: 0.9378 - val_loss: 3.4106 - val_acc: 0.3783\n",
      "Epoch 224/2000\n",
      "32s - loss: 0.2176 - acc: 0.9313 - val_loss: 3.3740 - val_acc: 0.3566\n",
      "Epoch 225/2000\n",
      "32s - loss: 0.1903 - acc: 0.9371 - val_loss: 3.2144 - val_acc: 0.3538\n",
      "Epoch 226/2000\n",
      "32s - loss: 0.1919 - acc: 0.9364 - val_loss: 3.2776 - val_acc: 0.3873\n",
      "Epoch 227/2000\n",
      "32s - loss: 0.1721 - acc: 0.9445 - val_loss: 3.5613 - val_acc: 0.3715\n",
      "Epoch 228/2000\n",
      "32s - loss: 0.1793 - acc: 0.9386 - val_loss: 3.4403 - val_acc: 0.3679\n",
      "Epoch 229/2000\n",
      "32s - loss: 0.1580 - acc: 0.9481 - val_loss: 3.5314 - val_acc: 0.3710\n",
      "Epoch 230/2000\n",
      "32s - loss: 0.1694 - acc: 0.9435 - val_loss: 3.4182 - val_acc: 0.3873\n",
      "Epoch 231/2000\n",
      "32s - loss: 0.1588 - acc: 0.9476 - val_loss: 3.4833 - val_acc: 0.3792\n",
      "Epoch 232/2000\n",
      "32s - loss: 0.1849 - acc: 0.9398 - val_loss: 3.5194 - val_acc: 0.3787\n",
      "Epoch 233/2000\n",
      "32s - loss: 0.1492 - acc: 0.9536 - val_loss: 3.5351 - val_acc: 0.3534\n",
      "Epoch 234/2000\n",
      "32s - loss: 0.1675 - acc: 0.9460 - val_loss: 3.5450 - val_acc: 0.3701\n",
      "Epoch 235/2000\n",
      "32s - loss: 0.1480 - acc: 0.9524 - val_loss: 3.5392 - val_acc: 0.3597\n",
      "Epoch 236/2000\n",
      "32s - loss: 0.1621 - acc: 0.9461 - val_loss: 3.5469 - val_acc: 0.3719\n",
      "Epoch 237/2000\n",
      "31s - loss: 0.1410 - acc: 0.9565 - val_loss: 3.6354 - val_acc: 0.3710\n",
      "Epoch 238/2000\n",
      "36232s - loss: 0.1473 - acc: 0.9498 - val_loss: 3.6665 - val_acc: 0.3729\n",
      "Epoch 239/2000\n",
      "40s - loss: 0.1331 - acc: 0.9571 - val_loss: 3.5639 - val_acc: 0.3566\n",
      "Epoch 240/2000\n",
      "35s - loss: 0.1776 - acc: 0.9466 - val_loss: 3.6243 - val_acc: 0.3683\n",
      "Epoch 241/2000\n",
      "33s - loss: 0.1571 - acc: 0.9484 - val_loss: 3.5736 - val_acc: 0.3801\n",
      "Epoch 242/2000\n",
      "36s - loss: 0.1488 - acc: 0.9515 - val_loss: 3.6564 - val_acc: 0.3719\n",
      "Epoch 243/2000\n",
      "43s - loss: 0.1407 - acc: 0.9541 - val_loss: 3.7180 - val_acc: 0.3647\n",
      "Epoch 244/2000\n",
      "34s - loss: 0.1349 - acc: 0.9565 - val_loss: 3.6284 - val_acc: 0.3670\n",
      "Epoch 245/2000\n",
      "32s - loss: 0.1355 - acc: 0.9576 - val_loss: 3.6099 - val_acc: 0.3652\n",
      "Epoch 246/2000\n",
      "33s - loss: 0.1530 - acc: 0.9503 - val_loss: 3.6430 - val_acc: 0.3692\n",
      "Epoch 247/2000\n",
      "32s - loss: 0.1608 - acc: 0.9468 - val_loss: 3.8118 - val_acc: 0.3724\n",
      "Epoch 248/2000\n",
      "32s - loss: 0.1320 - acc: 0.9568 - val_loss: 3.6779 - val_acc: 0.3756\n",
      "Epoch 249/2000\n",
      "32s - loss: 0.1203 - acc: 0.9604 - val_loss: 3.6383 - val_acc: 0.3629\n",
      "Epoch 250/2000\n",
      "31s - loss: 0.1175 - acc: 0.9616 - val_loss: 3.6993 - val_acc: 0.3747\n",
      "Epoch 251/2000\n",
      "32s - loss: 0.1149 - acc: 0.9630 - val_loss: 3.8812 - val_acc: 0.3570\n",
      "Epoch 252/2000\n",
      "32s - loss: 0.1415 - acc: 0.9553 - val_loss: 3.9360 - val_acc: 0.3579\n",
      "Epoch 253/2000\n",
      "32s - loss: 0.1336 - acc: 0.9600 - val_loss: 3.7673 - val_acc: 0.3593\n",
      "Epoch 254/2000\n",
      "32s - loss: 0.1302 - acc: 0.9565 - val_loss: 3.8480 - val_acc: 0.3805\n",
      "Epoch 255/2000\n",
      "32s - loss: 0.1246 - acc: 0.9599 - val_loss: 3.7244 - val_acc: 0.3729\n",
      "Epoch 256/2000\n",
      "32s - loss: 0.1107 - acc: 0.9649 - val_loss: 3.8439 - val_acc: 0.3620\n",
      "Epoch 257/2000\n",
      "32s - loss: 0.1446 - acc: 0.9548 - val_loss: 4.0451 - val_acc: 0.3724\n",
      "Epoch 258/2000\n",
      "32s - loss: 0.1155 - acc: 0.9628 - val_loss: 3.7981 - val_acc: 0.3724\n",
      "Epoch 259/2000\n",
      "32s - loss: 0.0938 - acc: 0.9703 - val_loss: 3.9466 - val_acc: 0.3679\n",
      "Epoch 260/2000\n",
      "32s - loss: 0.1238 - acc: 0.9601 - val_loss: 4.0176 - val_acc: 0.3624\n",
      "Epoch 261/2000\n",
      "32s - loss: 0.1246 - acc: 0.9624 - val_loss: 3.9780 - val_acc: 0.3620\n",
      "Epoch 262/2000\n",
      "32s - loss: 0.1052 - acc: 0.9681 - val_loss: 3.8861 - val_acc: 0.3656\n",
      "Epoch 263/2000\n",
      "32s - loss: 0.1443 - acc: 0.9575 - val_loss: 3.7543 - val_acc: 0.3643\n",
      "Epoch 264/2000\n",
      "32s - loss: 0.0917 - acc: 0.9710 - val_loss: 3.8861 - val_acc: 0.3552\n",
      "Epoch 265/2000\n",
      "32s - loss: 0.1116 - acc: 0.9631 - val_loss: 4.0060 - val_acc: 0.3570\n",
      "Epoch 266/2000\n",
      "32s - loss: 0.0879 - acc: 0.9733 - val_loss: 4.0112 - val_acc: 0.3692\n",
      "Epoch 267/2000\n",
      "32s - loss: 0.0781 - acc: 0.9750 - val_loss: 4.1969 - val_acc: 0.3733\n",
      "Epoch 268/2000\n",
      "32s - loss: 0.1026 - acc: 0.9647 - val_loss: 4.0945 - val_acc: 0.3747\n",
      "Epoch 269/2000\n",
      "32s - loss: 0.1000 - acc: 0.9692 - val_loss: 4.0382 - val_acc: 0.3661\n",
      "Epoch 270/2000\n",
      "32s - loss: 0.1017 - acc: 0.9671 - val_loss: 4.1392 - val_acc: 0.3710\n",
      "Epoch 271/2000\n",
      "32s - loss: 0.1074 - acc: 0.9641 - val_loss: 4.0684 - val_acc: 0.3769\n",
      "Epoch 272/2000\n",
      "32s - loss: 0.0858 - acc: 0.9730 - val_loss: 4.0355 - val_acc: 0.3643\n",
      "Epoch 273/2000\n",
      "32s - loss: 0.0912 - acc: 0.9712 - val_loss: 4.2343 - val_acc: 0.3643\n",
      "Epoch 274/2000\n",
      "32s - loss: 0.0834 - acc: 0.9740 - val_loss: 4.1117 - val_acc: 0.3710\n",
      "Epoch 275/2000\n",
      "32s - loss: 0.0812 - acc: 0.9759 - val_loss: 4.0852 - val_acc: 0.3796\n",
      "Epoch 276/2000\n",
      "32s - loss: 0.0809 - acc: 0.9739 - val_loss: 4.1365 - val_acc: 0.3719\n",
      "Epoch 277/2000\n",
      "32s - loss: 0.0799 - acc: 0.9764 - val_loss: 4.0435 - val_acc: 0.3643\n",
      "Epoch 278/2000\n",
      "32s - loss: 0.0984 - acc: 0.9717 - val_loss: 4.3319 - val_acc: 0.3647\n",
      "Epoch 279/2000\n",
      "32s - loss: 0.0812 - acc: 0.9755 - val_loss: 4.0561 - val_acc: 0.3656\n",
      "Epoch 280/2000\n",
      "32s - loss: 0.0822 - acc: 0.9749 - val_loss: 4.0759 - val_acc: 0.3765\n",
      "Epoch 281/2000\n",
      "32s - loss: 0.0795 - acc: 0.9755 - val_loss: 4.2551 - val_acc: 0.3692\n",
      "Epoch 282/2000\n",
      "32s - loss: 0.0965 - acc: 0.9711 - val_loss: 4.0749 - val_acc: 0.3706\n",
      "Epoch 283/2000\n",
      "32s - loss: 0.0727 - acc: 0.9783 - val_loss: 4.2359 - val_acc: 0.3579\n",
      "Epoch 284/2000\n",
      "32s - loss: 0.0901 - acc: 0.9747 - val_loss: 3.9621 - val_acc: 0.3566\n",
      "Epoch 285/2000\n",
      "32s - loss: 0.0981 - acc: 0.9683 - val_loss: 4.2383 - val_acc: 0.3670\n",
      "Epoch 286/2000\n",
      "32s - loss: 0.0708 - acc: 0.9781 - val_loss: 4.2706 - val_acc: 0.3579\n",
      "Epoch 287/2000\n",
      "32s - loss: 0.0683 - acc: 0.9781 - val_loss: 4.2336 - val_acc: 0.3643\n",
      "Epoch 288/2000\n",
      "32s - loss: 0.0664 - acc: 0.9805 - val_loss: 4.2220 - val_acc: 0.3633\n",
      "Epoch 289/2000\n",
      "32s - loss: 0.0621 - acc: 0.9829 - val_loss: 4.4606 - val_acc: 0.3597\n",
      "Epoch 290/2000\n",
      "32s - loss: 0.0887 - acc: 0.9731 - val_loss: 4.2623 - val_acc: 0.3643\n",
      "Epoch 291/2000\n",
      "32s - loss: 0.0814 - acc: 0.9738 - val_loss: 4.2056 - val_acc: 0.3593\n",
      "Epoch 292/2000\n",
      "32s - loss: 0.0784 - acc: 0.9776 - val_loss: 4.3093 - val_acc: 0.3692\n",
      "Epoch 293/2000\n",
      "32s - loss: 0.0737 - acc: 0.9768 - val_loss: 4.2766 - val_acc: 0.3624\n",
      "Epoch 294/2000\n",
      "32s - loss: 0.0765 - acc: 0.9769 - val_loss: 4.2380 - val_acc: 0.3706\n",
      "Epoch 295/2000\n",
      "32s - loss: 0.0777 - acc: 0.9759 - val_loss: 4.3106 - val_acc: 0.3633\n",
      "Epoch 296/2000\n",
      "32s - loss: 0.0630 - acc: 0.9809 - val_loss: 4.3583 - val_acc: 0.3570\n",
      "Epoch 297/2000\n",
      "32s - loss: 0.0898 - acc: 0.9738 - val_loss: 4.2174 - val_acc: 0.3656\n",
      "Epoch 298/2000\n",
      "32s - loss: 0.0679 - acc: 0.9790 - val_loss: 4.3582 - val_acc: 0.3692\n",
      "Epoch 299/2000\n",
      "32s - loss: 0.0614 - acc: 0.9806 - val_loss: 4.4006 - val_acc: 0.3588\n",
      "Epoch 300/2000\n",
      "32s - loss: 0.0594 - acc: 0.9813 - val_loss: 4.4160 - val_acc: 0.3538\n",
      "Epoch 301/2000\n",
      "32s - loss: 0.0664 - acc: 0.9797 - val_loss: 4.4791 - val_acc: 0.3633\n",
      "Epoch 302/2000\n",
      "32s - loss: 0.0699 - acc: 0.9797 - val_loss: 4.2535 - val_acc: 0.3588\n",
      "Epoch 303/2000\n",
      "32s - loss: 0.0693 - acc: 0.9812 - val_loss: 4.4247 - val_acc: 0.3575\n",
      "Epoch 304/2000\n",
      "32s - loss: 0.0734 - acc: 0.9774 - val_loss: 4.2579 - val_acc: 0.3760\n",
      "Epoch 305/2000\n",
      "32s - loss: 0.0706 - acc: 0.9780 - val_loss: 4.3401 - val_acc: 0.3525\n",
      "Epoch 306/2000\n",
      "32s - loss: 0.0652 - acc: 0.9805 - val_loss: 4.3721 - val_acc: 0.3674\n",
      "Epoch 307/2000\n",
      "32s - loss: 0.0713 - acc: 0.9763 - val_loss: 4.4307 - val_acc: 0.3652\n",
      "Epoch 308/2000\n",
      "32s - loss: 0.0707 - acc: 0.9798 - val_loss: 4.3281 - val_acc: 0.3624\n",
      "Epoch 309/2000\n",
      "32s - loss: 0.0649 - acc: 0.9804 - val_loss: 4.3139 - val_acc: 0.3633\n",
      "Epoch 310/2000\n",
      "32s - loss: 0.0584 - acc: 0.9822 - val_loss: 4.4701 - val_acc: 0.3516\n",
      "Epoch 311/2000\n",
      "32s - loss: 0.0532 - acc: 0.9836 - val_loss: 4.4515 - val_acc: 0.3566\n",
      "Epoch 312/2000\n",
      "32s - loss: 0.0568 - acc: 0.9831 - val_loss: 6.1729 - val_acc: 0.2995\n",
      "Epoch 313/2000\n",
      "32s - loss: 0.0775 - acc: 0.9792 - val_loss: 4.6201 - val_acc: 0.3606\n",
      "Epoch 314/2000\n",
      "32s - loss: 0.0805 - acc: 0.9758 - val_loss: 4.5019 - val_acc: 0.3692\n",
      "Epoch 315/2000\n",
      "32s - loss: 0.0553 - acc: 0.9844 - val_loss: 4.3574 - val_acc: 0.3692\n",
      "Epoch 316/2000\n",
      "32s - loss: 0.0492 - acc: 0.9867 - val_loss: 4.3844 - val_acc: 0.3697\n",
      "Epoch 317/2000\n",
      "32s - loss: 0.0790 - acc: 0.9777 - val_loss: 4.3035 - val_acc: 0.3629\n",
      "Epoch 318/2000\n",
      "32s - loss: 0.0606 - acc: 0.9813 - val_loss: 4.4707 - val_acc: 0.3665\n",
      "Epoch 319/2000\n",
      "32s - loss: 0.0587 - acc: 0.9814 - val_loss: 4.4719 - val_acc: 0.3520\n",
      "Epoch 320/2000\n",
      "32s - loss: 0.0588 - acc: 0.9823 - val_loss: 4.5184 - val_acc: 0.3516\n",
      "Epoch 321/2000\n",
      "32s - loss: 0.0508 - acc: 0.9855 - val_loss: 4.5268 - val_acc: 0.3335\n",
      "Epoch 322/2000\n",
      "32s - loss: 0.0415 - acc: 0.9882 - val_loss: 4.4275 - val_acc: 0.3543\n",
      "Epoch 323/2000\n",
      "32s - loss: 0.0438 - acc: 0.9863 - val_loss: 4.5999 - val_acc: 0.3557\n",
      "Epoch 324/2000\n",
      "32s - loss: 0.0760 - acc: 0.9785 - val_loss: 4.5176 - val_acc: 0.3665\n",
      "Epoch 325/2000\n",
      "32s - loss: 0.0430 - acc: 0.9877 - val_loss: 4.5345 - val_acc: 0.3620\n",
      "Epoch 326/2000\n",
      "32s - loss: 0.0435 - acc: 0.9875 - val_loss: 4.5506 - val_acc: 0.3525\n",
      "Epoch 327/2000\n",
      "32s - loss: 0.0454 - acc: 0.9860 - val_loss: 4.5640 - val_acc: 0.3434\n",
      "Epoch 328/2000\n",
      "32s - loss: 0.0512 - acc: 0.9838 - val_loss: 4.6461 - val_acc: 0.3670\n",
      "Epoch 329/2000\n",
      "32s - loss: 0.1283 - acc: 0.9665 - val_loss: 4.2535 - val_acc: 0.3670\n",
      "Epoch 330/2000\n",
      "32s - loss: 0.0473 - acc: 0.9860 - val_loss: 4.4740 - val_acc: 0.3615\n",
      "Epoch 331/2000\n",
      "32s - loss: 0.0424 - acc: 0.9887 - val_loss: 4.5640 - val_acc: 0.3697\n",
      "Epoch 332/2000\n",
      "32s - loss: 0.0442 - acc: 0.9881 - val_loss: 4.5813 - val_acc: 0.3670\n",
      "Epoch 333/2000\n",
      "33s - loss: 0.0436 - acc: 0.9883 - val_loss: 4.7297 - val_acc: 0.3624\n",
      "Epoch 334/2000\n",
      "34s - loss: 0.0535 - acc: 0.9841 - val_loss: 4.8142 - val_acc: 0.3552\n",
      "Epoch 335/2000\n",
      "32s - loss: 0.0357 - acc: 0.9908 - val_loss: 4.6383 - val_acc: 0.3629\n",
      "Epoch 336/2000\n",
      "32s - loss: 0.0779 - acc: 0.9781 - val_loss: 4.8168 - val_acc: 0.3520\n",
      "Epoch 337/2000\n",
      "32s - loss: 0.0458 - acc: 0.9868 - val_loss: 4.5652 - val_acc: 0.3489\n",
      "Epoch 338/2000\n",
      "32s - loss: 0.0411 - acc: 0.9880 - val_loss: 4.7698 - val_acc: 0.3575\n",
      "Epoch 339/2000\n",
      "32s - loss: 0.0863 - acc: 0.9765 - val_loss: 4.6254 - val_acc: 0.3579\n",
      "Epoch 340/2000\n",
      "32s - loss: 0.0644 - acc: 0.9798 - val_loss: 4.5654 - val_acc: 0.3525\n",
      "Epoch 341/2000\n",
      "32s - loss: 0.0421 - acc: 0.9876 - val_loss: 4.7076 - val_acc: 0.3548\n",
      "Epoch 342/2000\n",
      "32s - loss: 0.0418 - acc: 0.9881 - val_loss: 4.5488 - val_acc: 0.3679\n",
      "Epoch 343/2000\n",
      "32s - loss: 0.0337 - acc: 0.9914 - val_loss: 4.6115 - val_acc: 0.3543\n",
      "Epoch 344/2000\n",
      "32s - loss: 0.0389 - acc: 0.9883 - val_loss: 4.6551 - val_acc: 0.3557\n",
      "Epoch 345/2000\n",
      "32s - loss: 0.0394 - acc: 0.9894 - val_loss: 4.6741 - val_acc: 0.3579\n",
      "Epoch 346/2000\n",
      "32s - loss: 0.0455 - acc: 0.9861 - val_loss: 4.9576 - val_acc: 0.3443\n",
      "Epoch 347/2000\n",
      "32s - loss: 0.0400 - acc: 0.9875 - val_loss: 4.7267 - val_acc: 0.3620\n",
      "Epoch 348/2000\n",
      "32s - loss: 0.0391 - acc: 0.9891 - val_loss: 4.7986 - val_acc: 0.3588\n",
      "Epoch 349/2000\n",
      "32s - loss: 0.0371 - acc: 0.9899 - val_loss: 4.7434 - val_acc: 0.3611\n",
      "Epoch 350/2000\n",
      "32s - loss: 0.0294 - acc: 0.9926 - val_loss: 4.7088 - val_acc: 0.3566\n",
      "Epoch 351/2000\n",
      "32s - loss: 0.0472 - acc: 0.9856 - val_loss: 4.8023 - val_acc: 0.3602\n",
      "Epoch 352/2000\n",
      "32s - loss: 0.0335 - acc: 0.9905 - val_loss: 4.7409 - val_acc: 0.3611\n",
      "Epoch 353/2000\n",
      "32s - loss: 0.0326 - acc: 0.9896 - val_loss: 4.7055 - val_acc: 0.3638\n",
      "Epoch 354/2000\n",
      "32s - loss: 0.0440 - acc: 0.9880 - val_loss: 4.7384 - val_acc: 0.3611\n",
      "Epoch 355/2000\n",
      "32s - loss: 0.0319 - acc: 0.9913 - val_loss: 4.7677 - val_acc: 0.3602\n",
      "Epoch 356/2000\n",
      "32s - loss: 0.0314 - acc: 0.9916 - val_loss: 5.3280 - val_acc: 0.3398\n",
      "Epoch 357/2000\n",
      "32s - loss: 0.0749 - acc: 0.9768 - val_loss: 4.6347 - val_acc: 0.3602\n",
      "Epoch 358/2000\n",
      "32s - loss: 0.0354 - acc: 0.9904 - val_loss: 4.7768 - val_acc: 0.3602\n",
      "Epoch 359/2000\n",
      "32s - loss: 0.0317 - acc: 0.9911 - val_loss: 4.7872 - val_acc: 0.3538\n",
      "Epoch 360/2000\n",
      "32s - loss: 0.0403 - acc: 0.9886 - val_loss: 4.7816 - val_acc: 0.3692\n",
      "Epoch 361/2000\n",
      "32s - loss: 0.0724 - acc: 0.9796 - val_loss: 4.6882 - val_acc: 0.3597\n",
      "Epoch 362/2000\n",
      "32s - loss: 0.0308 - acc: 0.9919 - val_loss: 4.6791 - val_acc: 0.3710\n",
      "Epoch 363/2000\n",
      "32s - loss: 0.0430 - acc: 0.9859 - val_loss: 4.7701 - val_acc: 0.3670\n",
      "Epoch 364/2000\n",
      "32s - loss: 0.0334 - acc: 0.9897 - val_loss: 4.7748 - val_acc: 0.3760\n",
      "Epoch 365/2000\n",
      "32s - loss: 0.0290 - acc: 0.9919 - val_loss: 5.1302 - val_acc: 0.3548\n",
      "Epoch 366/2000\n",
      "32s - loss: 0.0348 - acc: 0.9900 - val_loss: 4.8936 - val_acc: 0.3633\n",
      "Epoch 367/2000\n",
      "32s - loss: 0.0279 - acc: 0.9928 - val_loss: 4.8427 - val_acc: 0.3593\n",
      "Epoch 368/2000\n",
      "32s - loss: 0.0302 - acc: 0.9911 - val_loss: 4.7591 - val_acc: 0.3756\n",
      "Epoch 369/2000\n",
      "32s - loss: 0.0339 - acc: 0.9899 - val_loss: 4.8518 - val_acc: 0.3557\n",
      "Epoch 370/2000\n",
      "32s - loss: 0.0275 - acc: 0.9924 - val_loss: 4.8905 - val_acc: 0.3575\n",
      "Epoch 371/2000\n",
      "32s - loss: 0.0251 - acc: 0.9939 - val_loss: 4.9291 - val_acc: 0.3647\n",
      "Epoch 372/2000\n",
      "32s - loss: 0.0321 - acc: 0.9913 - val_loss: 4.8083 - val_acc: 0.3611\n",
      "Epoch 373/2000\n",
      "32s - loss: 0.0437 - acc: 0.9869 - val_loss: 5.0088 - val_acc: 0.3611\n",
      "Epoch 374/2000\n",
      "32s - loss: 0.0316 - acc: 0.9916 - val_loss: 4.9368 - val_acc: 0.3670\n",
      "Epoch 375/2000\n",
      "32s - loss: 0.0269 - acc: 0.9919 - val_loss: 4.9114 - val_acc: 0.3647\n",
      "Epoch 376/2000\n",
      "32s - loss: 0.0285 - acc: 0.9924 - val_loss: 4.9427 - val_acc: 0.3665\n",
      "Epoch 377/2000\n",
      "32s - loss: 0.0326 - acc: 0.9910 - val_loss: 4.9197 - val_acc: 0.3647\n",
      "Epoch 378/2000\n",
      "32s - loss: 0.0239 - acc: 0.9939 - val_loss: 4.9509 - val_acc: 0.3615\n",
      "Epoch 379/2000\n",
      "32s - loss: 0.0314 - acc: 0.9904 - val_loss: 5.1986 - val_acc: 0.3330\n",
      "Epoch 380/2000\n",
      "32s - loss: 0.0313 - acc: 0.9909 - val_loss: 4.9907 - val_acc: 0.3624\n",
      "Epoch 381/2000\n",
      "32s - loss: 0.0204 - acc: 0.9946 - val_loss: 5.0126 - val_acc: 0.3498\n",
      "Epoch 382/2000\n",
      "32s - loss: 0.0408 - acc: 0.9880 - val_loss: 5.0555 - val_acc: 0.3561\n",
      "Epoch 383/2000\n",
      "32s - loss: 0.0219 - acc: 0.9950 - val_loss: 5.0943 - val_acc: 0.3620\n",
      "Epoch 384/2000\n",
      "32s - loss: 0.0221 - acc: 0.9941 - val_loss: 4.9709 - val_acc: 0.3597\n",
      "Epoch 385/2000\n",
      "32s - loss: 0.0293 - acc: 0.9923 - val_loss: 4.9292 - val_acc: 0.3620\n",
      "Epoch 386/2000\n",
      "32s - loss: 0.0253 - acc: 0.9929 - val_loss: 5.0319 - val_acc: 0.3498\n",
      "Epoch 387/2000\n",
      "32s - loss: 0.0286 - acc: 0.9911 - val_loss: 4.9991 - val_acc: 0.3602\n",
      "Epoch 388/2000\n",
      "32s - loss: 0.0231 - acc: 0.9935 - val_loss: 5.1077 - val_acc: 0.3507\n",
      "Epoch 389/2000\n",
      "32s - loss: 0.0326 - acc: 0.9898 - val_loss: 5.0131 - val_acc: 0.3624\n",
      "Epoch 390/2000\n",
      "32s - loss: 0.0261 - acc: 0.9920 - val_loss: 5.1434 - val_acc: 0.3643\n",
      "Epoch 391/2000\n",
      "32s - loss: 0.0199 - acc: 0.9955 - val_loss: 5.0277 - val_acc: 0.3606\n",
      "Epoch 392/2000\n",
      "32s - loss: 0.0345 - acc: 0.9904 - val_loss: 5.0575 - val_acc: 0.3602\n",
      "Epoch 393/2000\n",
      "32s - loss: 0.0253 - acc: 0.9924 - val_loss: 5.1383 - val_acc: 0.3593\n",
      "Epoch 394/2000\n",
      "32s - loss: 0.0520 - acc: 0.9870 - val_loss: 5.1304 - val_acc: 0.3466\n",
      "Epoch 395/2000\n",
      "32s - loss: 0.0398 - acc: 0.9876 - val_loss: 5.0963 - val_acc: 0.3638\n",
      "Epoch 396/2000\n",
      "32s - loss: 0.0347 - acc: 0.9907 - val_loss: 5.0560 - val_acc: 0.3688\n",
      "Epoch 397/2000\n",
      "32s - loss: 0.0290 - acc: 0.9910 - val_loss: 5.0618 - val_acc: 0.3611\n",
      "Epoch 398/2000\n",
      "32s - loss: 0.0306 - acc: 0.9916 - val_loss: 5.0461 - val_acc: 0.3584\n",
      "Epoch 399/2000\n",
      "32s - loss: 0.0400 - acc: 0.9889 - val_loss: 5.1926 - val_acc: 0.3588\n",
      "Epoch 400/2000\n",
      "32s - loss: 0.0297 - acc: 0.9910 - val_loss: 5.0443 - val_acc: 0.3593\n",
      "Epoch 401/2000\n",
      "32s - loss: 0.0262 - acc: 0.9924 - val_loss: 5.0329 - val_acc: 0.3498\n",
      "Epoch 402/2000\n",
      "32s - loss: 0.0238 - acc: 0.9928 - val_loss: 5.1115 - val_acc: 0.3611\n",
      "Epoch 403/2000\n",
      "32s - loss: 0.0460 - acc: 0.9886 - val_loss: 5.0076 - val_acc: 0.3647\n",
      "Epoch 404/2000\n",
      "32s - loss: 0.0330 - acc: 0.9912 - val_loss: 5.0041 - val_acc: 0.3620\n",
      "Epoch 405/2000\n",
      "32s - loss: 0.0203 - acc: 0.9943 - val_loss: 5.1519 - val_acc: 0.3575\n",
      "Epoch 406/2000\n",
      "32s - loss: 0.0224 - acc: 0.9944 - val_loss: 5.2507 - val_acc: 0.3557\n",
      "Epoch 407/2000\n",
      "32s - loss: 0.0273 - acc: 0.9918 - val_loss: 5.0495 - val_acc: 0.3701\n",
      "Epoch 408/2000\n",
      "32s - loss: 0.0190 - acc: 0.9950 - val_loss: 5.2744 - val_acc: 0.3656\n",
      "Epoch 409/2000\n",
      "32s - loss: 0.0252 - acc: 0.9934 - val_loss: 5.1660 - val_acc: 0.3624\n",
      "Epoch 410/2000\n",
      "32s - loss: 0.0225 - acc: 0.9936 - val_loss: 5.1288 - val_acc: 0.3606\n",
      "Epoch 411/2000\n",
      "32s - loss: 0.0187 - acc: 0.9953 - val_loss: 5.1274 - val_acc: 0.3570\n",
      "Epoch 412/2000\n",
      "32s - loss: 0.0197 - acc: 0.9942 - val_loss: 5.1695 - val_acc: 0.3697\n",
      "Epoch 413/2000\n",
      "32s - loss: 0.0222 - acc: 0.9931 - val_loss: 5.0150 - val_acc: 0.3647\n",
      "Epoch 414/2000\n",
      "32s - loss: 0.0183 - acc: 0.9951 - val_loss: 5.1156 - val_acc: 0.3602\n",
      "Epoch 415/2000\n",
      "32s - loss: 0.0166 - acc: 0.9960 - val_loss: 5.1611 - val_acc: 0.3529\n",
      "Epoch 416/2000\n",
      "32s - loss: 0.0175 - acc: 0.9957 - val_loss: 5.2744 - val_acc: 0.3511\n",
      "Epoch 417/2000\n",
      "32s - loss: 0.0274 - acc: 0.9916 - val_loss: 5.0910 - val_acc: 0.3697\n",
      "Epoch 418/2000\n",
      "32s - loss: 0.0181 - acc: 0.9951 - val_loss: 5.1923 - val_acc: 0.3620\n",
      "Epoch 419/2000\n",
      "32s - loss: 0.0208 - acc: 0.9946 - val_loss: 5.1582 - val_acc: 0.3557\n",
      "Epoch 420/2000\n",
      "32s - loss: 0.0272 - acc: 0.9916 - val_loss: 5.4070 - val_acc: 0.3661\n",
      "Epoch 421/2000\n",
      "32s - loss: 0.0299 - acc: 0.9920 - val_loss: 5.1922 - val_acc: 0.3606\n",
      "Epoch 422/2000\n",
      "32s - loss: 0.0184 - acc: 0.9955 - val_loss: 5.1931 - val_acc: 0.3602\n",
      "Epoch 423/2000\n",
      "32s - loss: 0.0144 - acc: 0.9969 - val_loss: 5.2199 - val_acc: 0.3579\n",
      "Epoch 424/2000\n",
      "32s - loss: 0.0169 - acc: 0.9952 - val_loss: 5.3439 - val_acc: 0.3579\n",
      "Epoch 425/2000\n",
      "31s - loss: 0.0169 - acc: 0.9953 - val_loss: 5.2831 - val_acc: 0.3570\n",
      "Epoch 426/2000\n",
      "32s - loss: 0.0390 - acc: 0.9878 - val_loss: 5.1592 - val_acc: 0.3643\n",
      "Epoch 427/2000\n",
      "32s - loss: 0.0268 - acc: 0.9913 - val_loss: 5.1605 - val_acc: 0.3579\n",
      "Epoch 428/2000\n",
      "32s - loss: 0.0327 - acc: 0.9919 - val_loss: 5.0108 - val_acc: 0.3538\n",
      "Epoch 429/2000\n",
      "32s - loss: 0.0347 - acc: 0.9899 - val_loss: 5.1109 - val_acc: 0.3602\n",
      "Epoch 430/2000\n",
      "32s - loss: 0.0202 - acc: 0.9948 - val_loss: 5.2088 - val_acc: 0.3620\n",
      "Epoch 431/2000\n",
      "32s - loss: 0.0152 - acc: 0.9961 - val_loss: 5.1019 - val_acc: 0.3674\n",
      "Epoch 432/2000\n",
      "32s - loss: 0.0142 - acc: 0.9965 - val_loss: 5.1725 - val_acc: 0.3579\n",
      "Epoch 433/2000\n",
      "32s - loss: 0.0247 - acc: 0.9927 - val_loss: 5.2481 - val_acc: 0.3502\n",
      "Epoch 434/2000\n",
      "32s - loss: 0.0370 - acc: 0.9907 - val_loss: 5.1974 - val_acc: 0.3615\n",
      "Epoch 435/2000\n",
      "32s - loss: 0.0203 - acc: 0.9948 - val_loss: 5.1239 - val_acc: 0.3656\n",
      "Epoch 436/2000\n",
      "32s - loss: 0.0233 - acc: 0.9932 - val_loss: 5.1632 - val_acc: 0.3502\n",
      "Epoch 437/2000\n",
      "32s - loss: 0.0218 - acc: 0.9935 - val_loss: 5.2952 - val_acc: 0.3529\n",
      "Epoch 438/2000\n",
      "32s - loss: 0.0239 - acc: 0.9937 - val_loss: 5.1615 - val_acc: 0.3584\n",
      "Epoch 439/2000\n",
      "32s - loss: 0.0277 - acc: 0.9922 - val_loss: 5.1567 - val_acc: 0.3638\n",
      "Epoch 440/2000\n",
      "32s - loss: 0.0225 - acc: 0.9945 - val_loss: 5.2564 - val_acc: 0.3665\n",
      "Epoch 441/2000\n",
      "32s - loss: 0.0200 - acc: 0.9948 - val_loss: 5.3236 - val_acc: 0.3511\n",
      "Epoch 442/2000\n",
      "32s - loss: 0.0203 - acc: 0.9940 - val_loss: 5.1472 - val_acc: 0.3584\n",
      "Epoch 443/2000\n",
      "32s - loss: 0.0175 - acc: 0.9954 - val_loss: 5.2063 - val_acc: 0.3638\n",
      "Epoch 444/2000\n",
      "32s - loss: 0.0176 - acc: 0.9948 - val_loss: 5.2971 - val_acc: 0.3575\n",
      "Epoch 445/2000\n",
      "32s - loss: 0.0150 - acc: 0.9972 - val_loss: 5.2587 - val_acc: 0.3588\n",
      "Epoch 446/2000\n",
      "32s - loss: 0.0160 - acc: 0.9969 - val_loss: 5.2719 - val_acc: 0.3597\n",
      "Epoch 447/2000\n",
      "32s - loss: 0.0127 - acc: 0.9971 - val_loss: 5.3366 - val_acc: 0.3620\n",
      "Epoch 448/2000\n",
      "32s - loss: 0.0282 - acc: 0.9920 - val_loss: 5.2595 - val_acc: 0.3615\n",
      "Epoch 449/2000\n",
      "32s - loss: 0.0163 - acc: 0.9962 - val_loss: 5.2769 - val_acc: 0.3579\n",
      "Epoch 450/2000\n",
      "32s - loss: 0.0268 - acc: 0.9917 - val_loss: 5.1818 - val_acc: 0.3710\n",
      "Epoch 451/2000\n",
      "32s - loss: 0.0178 - acc: 0.9951 - val_loss: 5.3073 - val_acc: 0.3643\n",
      "Epoch 452/2000\n",
      "32s - loss: 0.0191 - acc: 0.9952 - val_loss: 5.2249 - val_acc: 0.3538\n",
      "Epoch 453/2000\n",
      "32s - loss: 0.0237 - acc: 0.9928 - val_loss: 5.2394 - val_acc: 0.3593\n",
      "Epoch 454/2000\n",
      "32s - loss: 0.0181 - acc: 0.9959 - val_loss: 5.2070 - val_acc: 0.3652\n",
      "Epoch 455/2000\n",
      "32s - loss: 0.0136 - acc: 0.9970 - val_loss: 5.2674 - val_acc: 0.3611\n",
      "Epoch 456/2000\n",
      "32s - loss: 0.0112 - acc: 0.9979 - val_loss: 5.4310 - val_acc: 0.3661\n",
      "Epoch 457/2000\n",
      "32s - loss: 0.0285 - acc: 0.9922 - val_loss: 5.2069 - val_acc: 0.3579\n",
      "Epoch 458/2000\n",
      "32s - loss: 0.0149 - acc: 0.9964 - val_loss: 5.3529 - val_acc: 0.3588\n",
      "Epoch 459/2000\n",
      "32s - loss: 0.0151 - acc: 0.9960 - val_loss: 5.3339 - val_acc: 0.3575\n",
      "Epoch 460/2000\n",
      "32s - loss: 0.0126 - acc: 0.9969 - val_loss: 5.3044 - val_acc: 0.3543\n",
      "Epoch 461/2000\n",
      "32s - loss: 0.0184 - acc: 0.9961 - val_loss: 5.7026 - val_acc: 0.3385\n",
      "Epoch 462/2000\n",
      "32s - loss: 0.0408 - acc: 0.9883 - val_loss: 5.1850 - val_acc: 0.3611\n",
      "Epoch 463/2000\n",
      "33s - loss: 0.0103 - acc: 0.9982 - val_loss: 5.4031 - val_acc: 0.3570\n",
      "Epoch 464/2000\n",
      "32s - loss: 0.0150 - acc: 0.9960 - val_loss: 5.3559 - val_acc: 0.3548\n",
      "Epoch 465/2000\n",
      "32s - loss: 0.0182 - acc: 0.9942 - val_loss: 5.2941 - val_acc: 0.3538\n",
      "Epoch 466/2000\n",
      "32s - loss: 0.0125 - acc: 0.9971 - val_loss: 5.3350 - val_acc: 0.3566\n",
      "Epoch 467/2000\n",
      "32s - loss: 0.0525 - acc: 0.9876 - val_loss: 5.1019 - val_acc: 0.3443\n",
      "Epoch 468/2000\n",
      "32s - loss: 0.0370 - acc: 0.9897 - val_loss: 5.2414 - val_acc: 0.3647\n",
      "Epoch 469/2000\n",
      "32s - loss: 0.0227 - acc: 0.9951 - val_loss: 5.2461 - val_acc: 0.3520\n",
      "Epoch 470/2000\n",
      "32s - loss: 0.0159 - acc: 0.9970 - val_loss: 5.2968 - val_acc: 0.3656\n",
      "Epoch 471/2000\n",
      "32s - loss: 0.0142 - acc: 0.9968 - val_loss: 5.4101 - val_acc: 0.3525\n",
      "Epoch 472/2000\n",
      "32s - loss: 0.0118 - acc: 0.9974 - val_loss: 6.2118 - val_acc: 0.3299\n",
      "Epoch 473/2000\n",
      "32s - loss: 0.0344 - acc: 0.9892 - val_loss: 5.4009 - val_acc: 0.3452\n",
      "Epoch 474/2000\n",
      "32s - loss: 0.0158 - acc: 0.9952 - val_loss: 5.3969 - val_acc: 0.3489\n",
      "Epoch 475/2000\n",
      "32s - loss: 0.0160 - acc: 0.9950 - val_loss: 5.3064 - val_acc: 0.3552\n",
      "Epoch 476/2000\n",
      "32s - loss: 0.0104 - acc: 0.9976 - val_loss: 5.3391 - val_acc: 0.3561\n",
      "Epoch 477/2000\n",
      "32s - loss: 0.0094 - acc: 0.9979 - val_loss: 5.3997 - val_acc: 0.3534\n",
      "Epoch 478/2000\n",
      "32s - loss: 0.0535 - acc: 0.9883 - val_loss: 5.4084 - val_acc: 0.3588\n",
      "Epoch 479/2000\n",
      "32s - loss: 0.0206 - acc: 0.9942 - val_loss: 5.2549 - val_acc: 0.3552\n",
      "Epoch 480/2000\n",
      "32s - loss: 0.0152 - acc: 0.9956 - val_loss: 5.2999 - val_acc: 0.3769\n",
      "Epoch 481/2000\n",
      "32s - loss: 0.0172 - acc: 0.9947 - val_loss: 5.3182 - val_acc: 0.3602\n",
      "Epoch 482/2000\n",
      "32s - loss: 0.0140 - acc: 0.9966 - val_loss: 5.3384 - val_acc: 0.3561\n",
      "Epoch 483/2000\n",
      "32s - loss: 0.0118 - acc: 0.9969 - val_loss: 5.4044 - val_acc: 0.3597\n",
      "Epoch 484/2000\n",
      "32s - loss: 0.0094 - acc: 0.9977 - val_loss: 5.4381 - val_acc: 0.3575\n",
      "Epoch 485/2000\n",
      "32s - loss: 0.0109 - acc: 0.9969 - val_loss: 5.3388 - val_acc: 0.3557\n",
      "Epoch 486/2000\n",
      "32s - loss: 0.0147 - acc: 0.9962 - val_loss: 5.3935 - val_acc: 0.3548\n",
      "Epoch 487/2000\n",
      "32s - loss: 0.0121 - acc: 0.9973 - val_loss: 5.5539 - val_acc: 0.3620\n",
      "Epoch 488/2000\n",
      "32s - loss: 0.0124 - acc: 0.9973 - val_loss: 5.3265 - val_acc: 0.3665\n",
      "Epoch 489/2000\n",
      "32s - loss: 0.0168 - acc: 0.9948 - val_loss: 5.3914 - val_acc: 0.3602\n",
      "Epoch 490/2000\n",
      "32s - loss: 0.0130 - acc: 0.9966 - val_loss: 5.4260 - val_acc: 0.3606\n",
      "Epoch 491/2000\n",
      "32s - loss: 0.0141 - acc: 0.9952 - val_loss: 5.4680 - val_acc: 0.3561\n",
      "Epoch 492/2000\n",
      "32s - loss: 0.0262 - acc: 0.9914 - val_loss: 5.3487 - val_acc: 0.3511\n",
      "Epoch 493/2000\n",
      "32s - loss: 0.0745 - acc: 0.9836 - val_loss: 5.4050 - val_acc: 0.3466\n",
      "Epoch 494/2000\n",
      "32s - loss: 0.0262 - acc: 0.9924 - val_loss: 5.3038 - val_acc: 0.3452\n",
      "Epoch 495/2000\n",
      "32s - loss: 0.0239 - acc: 0.9929 - val_loss: 5.4692 - val_acc: 0.3434\n",
      "Epoch 496/2000\n",
      "32s - loss: 0.0136 - acc: 0.9966 - val_loss: 5.4934 - val_acc: 0.3548\n",
      "Epoch 497/2000\n",
      "32s - loss: 0.0124 - acc: 0.9966 - val_loss: 5.4623 - val_acc: 0.3548\n",
      "Epoch 498/2000\n",
      "32s - loss: 0.0118 - acc: 0.9968 - val_loss: 5.5217 - val_acc: 0.3507\n",
      "Epoch 499/2000\n",
      "32s - loss: 0.0106 - acc: 0.9980 - val_loss: 5.4393 - val_acc: 0.3597\n",
      "Epoch 500/2000\n",
      "32s - loss: 0.0116 - acc: 0.9973 - val_loss: 5.4986 - val_acc: 0.3611\n",
      "Epoch 501/2000\n",
      "32s - loss: 0.0262 - acc: 0.9924 - val_loss: 5.4412 - val_acc: 0.3403\n",
      "Epoch 502/2000\n",
      "32s - loss: 0.0182 - acc: 0.9953 - val_loss: 5.3133 - val_acc: 0.3579\n",
      "Epoch 503/2000\n",
      "32s - loss: 0.0353 - acc: 0.9910 - val_loss: 5.2080 - val_acc: 0.3493\n",
      "Epoch 504/2000\n",
      "32s - loss: 0.1246 - acc: 0.9702 - val_loss: 5.1451 - val_acc: 0.3597\n",
      "Epoch 505/2000\n",
      "32s - loss: 0.0201 - acc: 0.9951 - val_loss: 5.2596 - val_acc: 0.3611\n",
      "Epoch 506/2000\n",
      "32s - loss: 0.0344 - acc: 0.9902 - val_loss: 5.4548 - val_acc: 0.3457\n",
      "Epoch 507/2000\n",
      "32s - loss: 0.0207 - acc: 0.9938 - val_loss: 5.2472 - val_acc: 0.3561\n",
      "Epoch 508/2000\n",
      "32s - loss: 0.0127 - acc: 0.9964 - val_loss: 5.2896 - val_acc: 0.3502\n",
      "Epoch 509/2000\n",
      "32s - loss: 0.0149 - acc: 0.9961 - val_loss: 5.3571 - val_acc: 0.3538\n",
      "Epoch 510/2000\n",
      "32s - loss: 0.0156 - acc: 0.9950 - val_loss: 5.4171 - val_acc: 0.3498\n",
      "Epoch 511/2000\n",
      "32s - loss: 0.0188 - acc: 0.9946 - val_loss: 5.4004 - val_acc: 0.3525\n",
      "Epoch 512/2000\n",
      "32s - loss: 0.0130 - acc: 0.9965 - val_loss: 5.3365 - val_acc: 0.3434\n",
      "Epoch 513/2000\n",
      "32s - loss: 0.0180 - acc: 0.9947 - val_loss: 5.2646 - val_acc: 0.3602\n",
      "Epoch 514/2000\n",
      "32s - loss: 0.0181 - acc: 0.9953 - val_loss: 5.2850 - val_acc: 0.3584\n",
      "Epoch 515/2000\n",
      "32s - loss: 0.0118 - acc: 0.9968 - val_loss: 5.3845 - val_acc: 0.3597\n",
      "Epoch 516/2000\n",
      "32s - loss: 0.0159 - acc: 0.9966 - val_loss: 5.4331 - val_acc: 0.3615\n",
      "Epoch 517/2000\n",
      "32s - loss: 0.0141 - acc: 0.9969 - val_loss: 5.3231 - val_acc: 0.3633\n",
      "Epoch 518/2000\n",
      "32s - loss: 0.0116 - acc: 0.9971 - val_loss: 5.4432 - val_acc: 0.3543\n",
      "Epoch 519/2000\n",
      "32s - loss: 0.0153 - acc: 0.9959 - val_loss: 5.4026 - val_acc: 0.3570\n",
      "Epoch 520/2000\n",
      "32s - loss: 0.0108 - acc: 0.9973 - val_loss: 5.3869 - val_acc: 0.3575\n",
      "Epoch 521/2000\n",
      "32s - loss: 0.0114 - acc: 0.9979 - val_loss: 5.4656 - val_acc: 0.3593\n",
      "Epoch 522/2000\n",
      "32s - loss: 0.0080 - acc: 0.9985 - val_loss: 5.4868 - val_acc: 0.3588\n",
      "Epoch 523/2000\n",
      "32s - loss: 0.0142 - acc: 0.9962 - val_loss: 5.3964 - val_acc: 0.3529\n",
      "Epoch 524/2000\n",
      "32s - loss: 0.0091 - acc: 0.9981 - val_loss: 5.3901 - val_acc: 0.3511\n",
      "Epoch 525/2000\n",
      "32s - loss: 0.0075 - acc: 0.9990 - val_loss: 5.4708 - val_acc: 0.3633\n",
      "Epoch 526/2000\n",
      "32s - loss: 0.0072 - acc: 0.9989 - val_loss: 5.4614 - val_acc: 0.3593\n",
      "Epoch 527/2000\n",
      "32s - loss: 0.0089 - acc: 0.9978 - val_loss: 5.5235 - val_acc: 0.3511\n",
      "Epoch 528/2000\n",
      "32s - loss: 0.0073 - acc: 0.9989 - val_loss: 5.4307 - val_acc: 0.3484\n",
      "Epoch 529/2000\n",
      "32s - loss: 0.0168 - acc: 0.9947 - val_loss: 5.4153 - val_acc: 0.3579\n",
      "Epoch 530/2000\n",
      "32s - loss: 0.0103 - acc: 0.9974 - val_loss: 5.5879 - val_acc: 0.3629\n",
      "Epoch 531/2000\n",
      "32s - loss: 0.0083 - acc: 0.9983 - val_loss: 5.5583 - val_acc: 0.3489\n",
      "Epoch 532/2000\n",
      "32s - loss: 0.0060 - acc: 0.9990 - val_loss: 5.5576 - val_acc: 0.3606\n",
      "Epoch 533/2000\n",
      "32s - loss: 0.0068 - acc: 0.9991 - val_loss: 5.6967 - val_acc: 0.3552\n",
      "Epoch 534/2000\n",
      "32s - loss: 0.0100 - acc: 0.9980 - val_loss: 5.5617 - val_acc: 0.3570\n",
      "Epoch 535/2000\n",
      "32s - loss: 0.0086 - acc: 0.9980 - val_loss: 5.4128 - val_acc: 0.3656\n",
      "Epoch 536/2000\n",
      "32s - loss: 0.0083 - acc: 0.9980 - val_loss: 5.6546 - val_acc: 0.3701\n",
      "Epoch 537/2000\n",
      "32s - loss: 0.0203 - acc: 0.9941 - val_loss: 5.5164 - val_acc: 0.3615\n",
      "Epoch 538/2000\n",
      "32s - loss: 0.0192 - acc: 0.9942 - val_loss: 5.4848 - val_acc: 0.3606\n",
      "Epoch 539/2000\n",
      "32s - loss: 0.0120 - acc: 0.9972 - val_loss: 5.6292 - val_acc: 0.3448\n",
      "Epoch 540/2000\n",
      "32s - loss: 0.0100 - acc: 0.9979 - val_loss: 5.6559 - val_acc: 0.3566\n",
      "Epoch 541/2000\n",
      "32s - loss: 0.0096 - acc: 0.9978 - val_loss: 5.5042 - val_acc: 0.3697\n",
      "Epoch 542/2000\n",
      "32s - loss: 0.0190 - acc: 0.9953 - val_loss: 5.5179 - val_acc: 0.3611\n",
      "Epoch 543/2000\n",
      "32s - loss: 0.0112 - acc: 0.9977 - val_loss: 5.5561 - val_acc: 0.3575\n",
      "Epoch 544/2000\n",
      "32s - loss: 0.0123 - acc: 0.9969 - val_loss: 6.0509 - val_acc: 0.3335\n",
      "Epoch 545/2000\n",
      "32s - loss: 0.0564 - acc: 0.9861 - val_loss: 5.5501 - val_acc: 0.3679\n",
      "Epoch 546/2000\n",
      "32s - loss: 0.0150 - acc: 0.9963 - val_loss: 5.5160 - val_acc: 0.3629\n",
      "Epoch 547/2000\n",
      "32s - loss: 0.0123 - acc: 0.9968 - val_loss: 5.5919 - val_acc: 0.3593\n",
      "Epoch 548/2000\n",
      "32s - loss: 0.0091 - acc: 0.9981 - val_loss: 5.3655 - val_acc: 0.3661\n",
      "Epoch 549/2000\n",
      "32s - loss: 0.0211 - acc: 0.9944 - val_loss: 5.4564 - val_acc: 0.3561\n",
      "Epoch 550/2000\n",
      "32s - loss: 0.0085 - acc: 0.9984 - val_loss: 5.5170 - val_acc: 0.3584\n",
      "Epoch 551/2000\n",
      "32s - loss: 0.0093 - acc: 0.9979 - val_loss: 5.6208 - val_acc: 0.3566\n",
      "Epoch 552/2000\n",
      "32s - loss: 0.0071 - acc: 0.9985 - val_loss: 5.5499 - val_acc: 0.3602\n",
      "Epoch 553/2000\n",
      "32s - loss: 0.0249 - acc: 0.9931 - val_loss: 5.4931 - val_acc: 0.3633\n",
      "Epoch 554/2000\n",
      "32s - loss: 0.0146 - acc: 0.9959 - val_loss: 5.5307 - val_acc: 0.3480\n",
      "Epoch 555/2000\n",
      "32s - loss: 0.0097 - acc: 0.9974 - val_loss: 5.5267 - val_acc: 0.3611\n",
      "Epoch 556/2000\n",
      "32s - loss: 0.0124 - acc: 0.9973 - val_loss: 5.5468 - val_acc: 0.3629\n",
      "Epoch 557/2000\n",
      "32s - loss: 0.0101 - acc: 0.9972 - val_loss: 5.5400 - val_acc: 0.3615\n",
      "Epoch 558/2000\n",
      "32s - loss: 0.0062 - acc: 0.9993 - val_loss: 5.5702 - val_acc: 0.3575\n",
      "Epoch 559/2000\n",
      "32s - loss: 0.0059 - acc: 0.9992 - val_loss: 5.5813 - val_acc: 0.3593\n",
      "Epoch 560/2000\n",
      "32s - loss: 0.0077 - acc: 0.9982 - val_loss: 5.6091 - val_acc: 0.3543\n",
      "Epoch 561/2000\n",
      "32s - loss: 0.0071 - acc: 0.9984 - val_loss: 5.5819 - val_acc: 0.3597\n",
      "Epoch 562/2000\n",
      "32s - loss: 0.0059 - acc: 0.9992 - val_loss: 5.6158 - val_acc: 0.3629\n",
      "Epoch 563/2000\n",
      "32s - loss: 0.0060 - acc: 0.9991 - val_loss: 5.5808 - val_acc: 0.3643\n",
      "Epoch 564/2000\n",
      "32s - loss: 0.0060 - acc: 0.9989 - val_loss: 5.6306 - val_acc: 0.3557\n",
      "Epoch 565/2000\n",
      "32s - loss: 0.0054 - acc: 0.9992 - val_loss: 5.6765 - val_acc: 0.3597\n",
      "Epoch 566/2000\n",
      "32s - loss: 0.0054 - acc: 0.9989 - val_loss: 5.6082 - val_acc: 0.3575\n",
      "Epoch 567/2000\n",
      "32s - loss: 0.0058 - acc: 0.9990 - val_loss: 5.5730 - val_acc: 0.3588\n",
      "Epoch 568/2000\n",
      "32s - loss: 0.0054 - acc: 0.9992 - val_loss: 5.6682 - val_acc: 0.3566\n",
      "Epoch 569/2000\n",
      "32s - loss: 0.0082 - acc: 0.9980 - val_loss: 5.6086 - val_acc: 0.3561\n",
      "Epoch 570/2000\n",
      "32s - loss: 0.0074 - acc: 0.9985 - val_loss: 5.6800 - val_acc: 0.3584\n",
      "Epoch 571/2000\n",
      "32s - loss: 0.0067 - acc: 0.9990 - val_loss: 5.6631 - val_acc: 0.3566\n",
      "Epoch 572/2000\n",
      "32s - loss: 0.0072 - acc: 0.9988 - val_loss: 5.6502 - val_acc: 0.3579\n",
      "Epoch 573/2000\n",
      "32s - loss: 0.0083 - acc: 0.9983 - val_loss: 5.6775 - val_acc: 0.3466\n",
      "Epoch 574/2000\n",
      "32s - loss: 0.0145 - acc: 0.9967 - val_loss: 5.6565 - val_acc: 0.3670\n",
      "Epoch 575/2000\n",
      "32s - loss: 0.0106 - acc: 0.9969 - val_loss: 5.6084 - val_acc: 0.3665\n",
      "Epoch 576/2000\n",
      "32s - loss: 0.0165 - acc: 0.9965 - val_loss: 5.6150 - val_acc: 0.3525\n",
      "Epoch 577/2000\n",
      "32s - loss: 0.0121 - acc: 0.9970 - val_loss: 5.5574 - val_acc: 0.3679\n",
      "Epoch 578/2000\n",
      "32s - loss: 0.0076 - acc: 0.9985 - val_loss: 5.6525 - val_acc: 0.3597\n",
      "Epoch 579/2000\n",
      "32s - loss: 0.0069 - acc: 0.9989 - val_loss: 5.6183 - val_acc: 0.3552\n",
      "Epoch 580/2000\n",
      "32s - loss: 0.0070 - acc: 0.9985 - val_loss: 5.6649 - val_acc: 0.3579\n",
      "Epoch 581/2000\n",
      "32s - loss: 0.0075 - acc: 0.9982 - val_loss: 5.6221 - val_acc: 0.3629\n",
      "Epoch 582/2000\n",
      "32s - loss: 0.0058 - acc: 0.9988 - val_loss: 5.6697 - val_acc: 0.3701\n",
      "Epoch 583/2000\n",
      "32s - loss: 0.0104 - acc: 0.9972 - val_loss: 5.8146 - val_acc: 0.3484\n",
      "Epoch 584/2000\n",
      "32s - loss: 0.0129 - acc: 0.9966 - val_loss: 5.7226 - val_acc: 0.3602\n",
      "Epoch 585/2000\n",
      "32s - loss: 0.0075 - acc: 0.9985 - val_loss: 5.6308 - val_acc: 0.3593\n",
      "Epoch 586/2000\n",
      "32s - loss: 0.0215 - acc: 0.9934 - val_loss: 5.6047 - val_acc: 0.3529\n",
      "Epoch 587/2000\n",
      "32s - loss: 0.0110 - acc: 0.9966 - val_loss: 5.6374 - val_acc: 0.3575\n",
      "Epoch 588/2000\n",
      "32s - loss: 0.0078 - acc: 0.9981 - val_loss: 5.6069 - val_acc: 0.3561\n",
      "Epoch 589/2000\n",
      "32s - loss: 0.0129 - acc: 0.9964 - val_loss: 5.6873 - val_acc: 0.3620\n",
      "Epoch 590/2000\n",
      "32s - loss: 0.0095 - acc: 0.9975 - val_loss: 5.6749 - val_acc: 0.3529\n",
      "Epoch 591/2000\n",
      "32s - loss: 0.0150 - acc: 0.9956 - val_loss: 5.6759 - val_acc: 0.3534\n",
      "Epoch 592/2000\n",
      "32s - loss: 0.0086 - acc: 0.9982 - val_loss: 5.7272 - val_acc: 0.3525\n",
      "Epoch 593/2000\n",
      "32s - loss: 0.0140 - acc: 0.9965 - val_loss: 5.6256 - val_acc: 0.3620\n",
      "Epoch 594/2000\n",
      "32s - loss: 0.0100 - acc: 0.9977 - val_loss: 5.6256 - val_acc: 0.3529\n",
      "Epoch 595/2000\n",
      "32s - loss: 0.0079 - acc: 0.9982 - val_loss: 5.6335 - val_acc: 0.3584\n",
      "Epoch 596/2000\n",
      "32s - loss: 0.0133 - acc: 0.9963 - val_loss: 5.6322 - val_acc: 0.3588\n",
      "Epoch 597/2000\n",
      "32s - loss: 0.0084 - acc: 0.9982 - val_loss: 5.7048 - val_acc: 0.3602\n",
      "Epoch 598/2000\n",
      "32s - loss: 0.0092 - acc: 0.9980 - val_loss: 5.7046 - val_acc: 0.3511\n",
      "Epoch 599/2000\n",
      "32s - loss: 0.0064 - acc: 0.9985 - val_loss: 5.7417 - val_acc: 0.3507\n",
      "Epoch 600/2000\n",
      "32s - loss: 0.0058 - acc: 0.9985 - val_loss: 5.7086 - val_acc: 0.3579\n",
      "Epoch 601/2000\n",
      "32s - loss: 0.0080 - acc: 0.9982 - val_loss: 5.7072 - val_acc: 0.3493\n",
      "Epoch 602/2000\n",
      "32s - loss: 0.0184 - acc: 0.9946 - val_loss: 5.5989 - val_acc: 0.3538\n",
      "Epoch 603/2000\n",
      "32s - loss: 0.0075 - acc: 0.9984 - val_loss: 5.7885 - val_acc: 0.3561\n",
      "Epoch 604/2000\n",
      "32s - loss: 0.0086 - acc: 0.9978 - val_loss: 5.7106 - val_acc: 0.3584\n",
      "Epoch 605/2000\n",
      "32s - loss: 0.0086 - acc: 0.9980 - val_loss: 5.7302 - val_acc: 0.3548\n",
      "Epoch 606/2000\n",
      "32s - loss: 0.0076 - acc: 0.9984 - val_loss: 5.6942 - val_acc: 0.3543\n",
      "Epoch 607/2000\n",
      "32s - loss: 0.0072 - acc: 0.9985 - val_loss: 5.6781 - val_acc: 0.3647\n",
      "Epoch 608/2000\n",
      "32s - loss: 0.0072 - acc: 0.9987 - val_loss: 5.5959 - val_acc: 0.3525\n",
      "Epoch 609/2000\n",
      "32s - loss: 0.0217 - acc: 0.9934 - val_loss: 5.5897 - val_acc: 0.3606\n",
      "Epoch 610/2000\n",
      "32s - loss: 0.0087 - acc: 0.9979 - val_loss: 5.7346 - val_acc: 0.3489\n",
      "Epoch 611/2000\n",
      "32s - loss: 0.0054 - acc: 0.9991 - val_loss: 5.6982 - val_acc: 0.3593\n",
      "Epoch 612/2000\n",
      "32s - loss: 0.0076 - acc: 0.9979 - val_loss: 5.7139 - val_acc: 0.3498\n",
      "Epoch 613/2000\n",
      "32s - loss: 0.0068 - acc: 0.9984 - val_loss: 5.7131 - val_acc: 0.3570\n",
      "Epoch 614/2000\n",
      "32s - loss: 0.0082 - acc: 0.9978 - val_loss: 5.7240 - val_acc: 0.3502\n",
      "Epoch 615/2000\n",
      "32s - loss: 0.0281 - acc: 0.9934 - val_loss: 5.7926 - val_acc: 0.3475\n",
      "Epoch 616/2000\n",
      "32s - loss: 0.0094 - acc: 0.9975 - val_loss: 5.7560 - val_acc: 0.3520\n",
      "Epoch 617/2000\n",
      "32s - loss: 0.0156 - acc: 0.9956 - val_loss: 5.6830 - val_acc: 0.3520\n",
      "Epoch 618/2000\n",
      "32s - loss: 0.0090 - acc: 0.9977 - val_loss: 5.7182 - val_acc: 0.3579\n",
      "Epoch 619/2000\n",
      "32s - loss: 0.0053 - acc: 0.9991 - val_loss: 5.7860 - val_acc: 0.3561\n",
      "Epoch 620/2000\n",
      "32s - loss: 0.0153 - acc: 0.9957 - val_loss: 5.8481 - val_acc: 0.3538\n",
      "Epoch 621/2000\n",
      "32s - loss: 0.0187 - acc: 0.9945 - val_loss: 5.6836 - val_acc: 0.3566\n",
      "Epoch 622/2000\n",
      "32s - loss: 0.0105 - acc: 0.9968 - val_loss: 5.7283 - val_acc: 0.3552\n",
      "Epoch 623/2000\n",
      "32s - loss: 0.0107 - acc: 0.9966 - val_loss: 5.8019 - val_acc: 0.3588\n",
      "Epoch 624/2000\n",
      "32s - loss: 0.0098 - acc: 0.9969 - val_loss: 5.7419 - val_acc: 0.3557\n",
      "Epoch 625/2000\n",
      "32s - loss: 0.0099 - acc: 0.9976 - val_loss: 5.7124 - val_acc: 0.3629\n",
      "Epoch 626/2000\n",
      "32s - loss: 0.0123 - acc: 0.9969 - val_loss: 5.6196 - val_acc: 0.3656\n",
      "Epoch 627/2000\n",
      "32s - loss: 0.0101 - acc: 0.9973 - val_loss: 5.6738 - val_acc: 0.3525\n",
      "Epoch 628/2000\n",
      "32s - loss: 0.0071 - acc: 0.9985 - val_loss: 5.7286 - val_acc: 0.3466\n",
      "Epoch 629/2000\n",
      "32s - loss: 0.0060 - acc: 0.9990 - val_loss: 5.7656 - val_acc: 0.3579\n",
      "Epoch 630/2000\n",
      "32s - loss: 0.0068 - acc: 0.9985 - val_loss: 5.7427 - val_acc: 0.3538\n",
      "Epoch 631/2000\n",
      "32s - loss: 0.0085 - acc: 0.9982 - val_loss: 5.8239 - val_acc: 0.3511\n",
      "Epoch 632/2000\n",
      "32s - loss: 0.0401 - acc: 0.9909 - val_loss: 6.0039 - val_acc: 0.3597\n",
      "Epoch 633/2000\n",
      "32s - loss: 0.0545 - acc: 0.9867 - val_loss: 5.5517 - val_acc: 0.3620\n",
      "Epoch 634/2000\n",
      "32s - loss: 0.0133 - acc: 0.9969 - val_loss: 5.6370 - val_acc: 0.3629\n",
      "Epoch 635/2000\n",
      "32s - loss: 0.0109 - acc: 0.9979 - val_loss: 5.6153 - val_acc: 0.3543\n",
      "Epoch 636/2000\n",
      "32s - loss: 0.0100 - acc: 0.9968 - val_loss: 5.6555 - val_acc: 0.3638\n",
      "Epoch 637/2000\n",
      "32s - loss: 0.0080 - acc: 0.9979 - val_loss: 5.6670 - val_acc: 0.3633\n",
      "Epoch 638/2000\n",
      "32s - loss: 0.0088 - acc: 0.9983 - val_loss: 5.7688 - val_acc: 0.3615\n",
      "Epoch 639/2000\n",
      "32s - loss: 0.0093 - acc: 0.9972 - val_loss: 5.7173 - val_acc: 0.3548\n",
      "Epoch 640/2000\n",
      "32s - loss: 0.0063 - acc: 0.9989 - val_loss: 5.7542 - val_acc: 0.3643\n",
      "Epoch 641/2000\n",
      "32s - loss: 0.0090 - acc: 0.9972 - val_loss: 5.7532 - val_acc: 0.3529\n",
      "Epoch 642/2000\n",
      "32s - loss: 0.0049 - acc: 0.9992 - val_loss: 5.7780 - val_acc: 0.3606\n",
      "Epoch 643/2000\n",
      "32s - loss: 0.0058 - acc: 0.9988 - val_loss: 5.7273 - val_acc: 0.3502\n",
      "Epoch 644/2000\n",
      "32s - loss: 0.0072 - acc: 0.9982 - val_loss: 5.6636 - val_acc: 0.3561\n",
      "Epoch 645/2000\n",
      "32s - loss: 0.0113 - acc: 0.9972 - val_loss: 5.7953 - val_acc: 0.3416\n",
      "Epoch 646/2000\n",
      "32s - loss: 0.0086 - acc: 0.9969 - val_loss: 5.7177 - val_acc: 0.3624\n",
      "Epoch 647/2000\n",
      "32s - loss: 0.0072 - acc: 0.9985 - val_loss: 5.6521 - val_acc: 0.3579\n",
      "Epoch 648/2000\n",
      "32s - loss: 0.0087 - acc: 0.9977 - val_loss: 5.7358 - val_acc: 0.3520\n",
      "Epoch 649/2000\n",
      "32s - loss: 0.0081 - acc: 0.9982 - val_loss: 5.7650 - val_acc: 0.3665\n",
      "Epoch 650/2000\n",
      "32s - loss: 0.0074 - acc: 0.9985 - val_loss: 5.8487 - val_acc: 0.3629\n",
      "Epoch 651/2000\n",
      "32s - loss: 0.0070 - acc: 0.9983 - val_loss: 5.7651 - val_acc: 0.3566\n",
      "Epoch 652/2000\n",
      "32s - loss: 0.0062 - acc: 0.9988 - val_loss: 5.7747 - val_acc: 0.3557\n",
      "Epoch 653/2000\n",
      "32s - loss: 0.0053 - acc: 0.9991 - val_loss: 5.7414 - val_acc: 0.3584\n",
      "Epoch 654/2000\n",
      "32s - loss: 0.0077 - acc: 0.9983 - val_loss: 5.8322 - val_acc: 0.3570\n",
      "Epoch 655/2000\n",
      "32s - loss: 0.0068 - acc: 0.9988 - val_loss: 5.6961 - val_acc: 0.3624\n",
      "Epoch 656/2000\n",
      "32s - loss: 0.0103 - acc: 0.9969 - val_loss: 5.7723 - val_acc: 0.3597\n",
      "Epoch 657/2000\n",
      "32s - loss: 0.0098 - acc: 0.9973 - val_loss: 5.7963 - val_acc: 0.3566\n",
      "Epoch 658/2000\n",
      "32s - loss: 0.0079 - acc: 0.9978 - val_loss: 5.7598 - val_acc: 0.3566\n",
      "Epoch 659/2000\n",
      "32s - loss: 0.0100 - acc: 0.9973 - val_loss: 5.6966 - val_acc: 0.3552\n",
      "Epoch 660/2000\n",
      "32s - loss: 0.0074 - acc: 0.9981 - val_loss: 5.7939 - val_acc: 0.3534\n",
      "Epoch 661/2000\n",
      "32s - loss: 0.0061 - acc: 0.9984 - val_loss: 5.7494 - val_acc: 0.3570\n",
      "Epoch 662/2000\n",
      "32s - loss: 0.0050 - acc: 0.9988 - val_loss: 5.7543 - val_acc: 0.3525\n",
      "Epoch 663/2000\n",
      "32s - loss: 0.0064 - acc: 0.9985 - val_loss: 5.8795 - val_acc: 0.3462\n",
      "Epoch 664/2000\n",
      "32s - loss: 0.0034 - acc: 0.9997 - val_loss: 5.7851 - val_acc: 0.3552\n",
      "Epoch 665/2000\n",
      "32s - loss: 0.0048 - acc: 0.9991 - val_loss: 5.8461 - val_acc: 0.3538\n",
      "Epoch 666/2000\n",
      "32s - loss: 0.0071 - acc: 0.9979 - val_loss: 5.7794 - val_acc: 0.3538\n",
      "Epoch 667/2000\n",
      "32s - loss: 0.0061 - acc: 0.9984 - val_loss: 5.7664 - val_acc: 0.3466\n",
      "Epoch 668/2000\n",
      "32s - loss: 0.0049 - acc: 0.9988 - val_loss: 5.7774 - val_acc: 0.3552\n",
      "Epoch 669/2000\n",
      "32s - loss: 0.0082 - acc: 0.9972 - val_loss: 5.9159 - val_acc: 0.3421\n",
      "Epoch 670/2000\n",
      "32s - loss: 0.0054 - acc: 0.9989 - val_loss: 5.8718 - val_acc: 0.3529\n",
      "Epoch 671/2000\n",
      "32s - loss: 0.0032 - acc: 0.9995 - val_loss: 5.9400 - val_acc: 0.3507\n",
      "Epoch 672/2000\n",
      "32s - loss: 0.0069 - acc: 0.9985 - val_loss: 5.7027 - val_acc: 0.3566\n",
      "Epoch 673/2000\n",
      "32s - loss: 0.0078 - acc: 0.9985 - val_loss: 5.8713 - val_acc: 0.3502\n",
      "Epoch 674/2000\n",
      "32s - loss: 0.0224 - acc: 0.9939 - val_loss: 5.9085 - val_acc: 0.3584\n",
      "Epoch 675/2000\n",
      "32s - loss: 0.0407 - acc: 0.9924 - val_loss: 5.7449 - val_acc: 0.3561\n",
      "Epoch 676/2000\n",
      "32s - loss: 0.0144 - acc: 0.9956 - val_loss: 5.6758 - val_acc: 0.3529\n",
      "Epoch 677/2000\n",
      "32s - loss: 0.0133 - acc: 0.9975 - val_loss: 5.7332 - val_acc: 0.3353\n",
      "Epoch 678/2000\n",
      "32s - loss: 0.0301 - acc: 0.9922 - val_loss: 5.7273 - val_acc: 0.3480\n",
      "Epoch 679/2000\n",
      "32s - loss: 0.0093 - acc: 0.9977 - val_loss: 5.7536 - val_acc: 0.3575\n",
      "Epoch 680/2000\n",
      "32s - loss: 0.0069 - acc: 0.9989 - val_loss: 5.7481 - val_acc: 0.3557\n",
      "Epoch 681/2000\n",
      "32s - loss: 0.0069 - acc: 0.9987 - val_loss: 5.8075 - val_acc: 0.3484\n",
      "Epoch 682/2000\n",
      "32s - loss: 0.0065 - acc: 0.9985 - val_loss: 5.8224 - val_acc: 0.3534\n",
      "Epoch 683/2000\n",
      "32s - loss: 0.0072 - acc: 0.9983 - val_loss: 5.7760 - val_acc: 0.3462\n",
      "Epoch 684/2000\n",
      "32s - loss: 0.0080 - acc: 0.9982 - val_loss: 5.8770 - val_acc: 0.3434\n",
      "Epoch 685/2000\n",
      "32s - loss: 0.0043 - acc: 0.9994 - val_loss: 5.8006 - val_acc: 0.3566\n",
      "Epoch 686/2000\n",
      "32s - loss: 0.0046 - acc: 0.9992 - val_loss: 5.7982 - val_acc: 0.3525\n",
      "Epoch 687/2000\n",
      "32s - loss: 0.0046 - acc: 0.9992 - val_loss: 5.8730 - val_acc: 0.3489\n",
      "Epoch 688/2000\n",
      "32s - loss: 0.0032 - acc: 0.9994 - val_loss: 5.8990 - val_acc: 0.3489\n",
      "Epoch 689/2000\n",
      "32s - loss: 0.0061 - acc: 0.9985 - val_loss: 5.8914 - val_acc: 0.3489\n",
      "Epoch 690/2000\n",
      "32s - loss: 0.0037 - acc: 0.9995 - val_loss: 5.9396 - val_acc: 0.3529\n",
      "Epoch 691/2000\n",
      "32s - loss: 0.0058 - acc: 0.9987 - val_loss: 5.8339 - val_acc: 0.3516\n",
      "Epoch 692/2000\n",
      "32s - loss: 0.0035 - acc: 0.9994 - val_loss: 5.8318 - val_acc: 0.3529\n",
      "Epoch 693/2000\n",
      "32s - loss: 0.0042 - acc: 0.9990 - val_loss: 5.9367 - val_acc: 0.3552\n",
      "Epoch 694/2000\n",
      "32s - loss: 0.0107 - acc: 0.9968 - val_loss: 5.8515 - val_acc: 0.3534\n",
      "Epoch 695/2000\n",
      "32s - loss: 0.0054 - acc: 0.9990 - val_loss: 5.8738 - val_acc: 0.3511\n",
      "Epoch 696/2000\n",
      "32s - loss: 0.0079 - acc: 0.9981 - val_loss: 6.0126 - val_acc: 0.3434\n",
      "Epoch 697/2000\n",
      "32s - loss: 0.0102 - acc: 0.9974 - val_loss: 5.8634 - val_acc: 0.3516\n",
      "Epoch 698/2000\n",
      "32s - loss: 0.0089 - acc: 0.9975 - val_loss: 5.8420 - val_acc: 0.3557\n",
      "Epoch 699/2000\n",
      "32s - loss: 0.0160 - acc: 0.9951 - val_loss: 5.8155 - val_acc: 0.3570\n",
      "Epoch 700/2000\n",
      "32s - loss: 0.0153 - acc: 0.9960 - val_loss: 5.8133 - val_acc: 0.3597\n",
      "Epoch 701/2000\n",
      "32s - loss: 0.0082 - acc: 0.9984 - val_loss: 5.8458 - val_acc: 0.3534\n",
      "Epoch 702/2000\n",
      "32s - loss: 0.0046 - acc: 0.9989 - val_loss: 5.8058 - val_acc: 0.3588\n",
      "Epoch 703/2000\n",
      "32s - loss: 0.0038 - acc: 0.9996 - val_loss: 5.9019 - val_acc: 0.3457\n",
      "Epoch 704/2000\n",
      "32s - loss: 0.0240 - acc: 0.9929 - val_loss: 5.7467 - val_acc: 0.3656\n",
      "Epoch 705/2000\n",
      "32s - loss: 0.0061 - acc: 0.9984 - val_loss: 5.8449 - val_acc: 0.3606\n",
      "Epoch 706/2000\n",
      "32s - loss: 0.0066 - acc: 0.9982 - val_loss: 5.8302 - val_acc: 0.3525\n",
      "Epoch 707/2000\n",
      "31s - loss: 0.0046 - acc: 0.9991 - val_loss: 5.8713 - val_acc: 0.3507\n",
      "Epoch 708/2000\n",
      "32s - loss: 0.0055 - acc: 0.9990 - val_loss: 5.8964 - val_acc: 0.3507\n",
      "Epoch 709/2000\n",
      "32s - loss: 0.0153 - acc: 0.9957 - val_loss: 5.7574 - val_acc: 0.3502\n",
      "Epoch 710/2000\n",
      "32s - loss: 0.0095 - acc: 0.9978 - val_loss: 5.9245 - val_acc: 0.3502\n",
      "Epoch 711/2000\n",
      "32s - loss: 0.0068 - acc: 0.9985 - val_loss: 5.8232 - val_acc: 0.3511\n",
      "Epoch 712/2000\n",
      "32s - loss: 0.0169 - acc: 0.9954 - val_loss: 5.8673 - val_acc: 0.3584\n",
      "Epoch 713/2000\n",
      "32s - loss: 0.0069 - acc: 0.9980 - val_loss: 5.8072 - val_acc: 0.3538\n",
      "Epoch 714/2000\n",
      "32s - loss: 0.0050 - acc: 0.9994 - val_loss: 5.9089 - val_acc: 0.3471\n",
      "Epoch 715/2000\n",
      "32s - loss: 0.0044 - acc: 0.9993 - val_loss: 5.8749 - val_acc: 0.3584\n",
      "Epoch 716/2000\n",
      "32s - loss: 0.0039 - acc: 0.9994 - val_loss: 5.8658 - val_acc: 0.3552\n",
      "Epoch 717/2000\n",
      "32s - loss: 0.0035 - acc: 0.9997 - val_loss: 5.9048 - val_acc: 0.3566\n",
      "Epoch 718/2000\n",
      "32s - loss: 0.0061 - acc: 0.9983 - val_loss: 5.9103 - val_acc: 0.3525\n",
      "Epoch 719/2000\n",
      "32s - loss: 0.0052 - acc: 0.9987 - val_loss: 5.8592 - val_acc: 0.3566\n",
      "Epoch 720/2000\n",
      "32s - loss: 0.0042 - acc: 0.9996 - val_loss: 5.8497 - val_acc: 0.3566\n",
      "Epoch 721/2000\n",
      "32s - loss: 0.0039 - acc: 0.9993 - val_loss: 5.9282 - val_acc: 0.3511\n",
      "Epoch 722/2000\n",
      "32s - loss: 0.0047 - acc: 0.9992 - val_loss: 5.9071 - val_acc: 0.3534\n",
      "Epoch 723/2000\n",
      "32s - loss: 0.0035 - acc: 0.9994 - val_loss: 5.9386 - val_acc: 0.3561\n",
      "Epoch 724/2000\n",
      "32s - loss: 0.0022 - acc: 1.0000 - val_loss: 5.9147 - val_acc: 0.3575\n",
      "Epoch 725/2000\n",
      "32s - loss: 0.0045 - acc: 0.9992 - val_loss: 5.8794 - val_acc: 0.3538\n",
      "Epoch 726/2000\n",
      "32s - loss: 0.0041 - acc: 0.9993 - val_loss: 6.0240 - val_acc: 0.3457\n",
      "Epoch 727/2000\n",
      "32s - loss: 0.0068 - acc: 0.9984 - val_loss: 5.9045 - val_acc: 0.3620\n",
      "Epoch 728/2000\n",
      "32s - loss: 0.0039 - acc: 0.9994 - val_loss: 5.9648 - val_acc: 0.3529\n",
      "Epoch 729/2000\n",
      "32s - loss: 0.0036 - acc: 0.9996 - val_loss: 5.9756 - val_acc: 0.3493\n",
      "Epoch 730/2000\n",
      "32s - loss: 0.0025 - acc: 0.9999 - val_loss: 5.9002 - val_acc: 0.3489\n",
      "Epoch 731/2000\n",
      "32s - loss: 0.0051 - acc: 0.9989 - val_loss: 5.8567 - val_acc: 0.3543\n",
      "Epoch 732/2000\n",
      "32s - loss: 0.0041 - acc: 0.9993 - val_loss: 5.9816 - val_acc: 0.3615\n",
      "Epoch 733/2000\n",
      "32s - loss: 0.0037 - acc: 0.9993 - val_loss: 5.9146 - val_acc: 0.3552\n",
      "Epoch 734/2000\n",
      "32s - loss: 0.0027 - acc: 0.9998 - val_loss: 5.9784 - val_acc: 0.3534\n",
      "Epoch 735/2000\n",
      "32s - loss: 0.0022 - acc: 0.9998 - val_loss: 6.0140 - val_acc: 0.3471\n",
      "Epoch 736/2000\n",
      "32s - loss: 0.0066 - acc: 0.9978 - val_loss: 5.9314 - val_acc: 0.3647\n",
      "Epoch 737/2000\n",
      "32s - loss: 0.0028 - acc: 0.9998 - val_loss: 5.9356 - val_acc: 0.3548\n",
      "Epoch 738/2000\n",
      "32s - loss: 0.0056 - acc: 0.9990 - val_loss: 7.0701 - val_acc: 0.3371\n",
      "Epoch 739/2000\n",
      "32s - loss: 0.0230 - acc: 0.9953 - val_loss: 5.9313 - val_acc: 0.3575\n",
      "Epoch 740/2000\n",
      "32s - loss: 0.0052 - acc: 0.9988 - val_loss: 5.8237 - val_acc: 0.3534\n",
      "Epoch 741/2000\n",
      "32s - loss: 0.0039 - acc: 0.9993 - val_loss: 5.9124 - val_acc: 0.3543\n",
      "Epoch 742/2000\n",
      "32s - loss: 0.0090 - acc: 0.9978 - val_loss: 5.8988 - val_acc: 0.3538\n",
      "Epoch 743/2000\n",
      "32s - loss: 0.0037 - acc: 0.9993 - val_loss: 5.9134 - val_acc: 0.3493\n",
      "Epoch 744/2000\n",
      "32s - loss: 0.0049 - acc: 0.9990 - val_loss: 5.9224 - val_acc: 0.3534\n",
      "Epoch 745/2000\n",
      "32s - loss: 0.0030 - acc: 0.9997 - val_loss: 5.9374 - val_acc: 0.3502\n",
      "Epoch 746/2000\n",
      "32s - loss: 0.0058 - acc: 0.9983 - val_loss: 6.0142 - val_acc: 0.3484\n",
      "Epoch 747/2000\n",
      "32s - loss: 0.0035 - acc: 0.9997 - val_loss: 5.9808 - val_acc: 0.3502\n",
      "Epoch 748/2000\n",
      "32s - loss: 0.0048 - acc: 0.9992 - val_loss: 5.9475 - val_acc: 0.3557\n",
      "Epoch 749/2000\n",
      "32s - loss: 0.0034 - acc: 0.9995 - val_loss: 6.0030 - val_acc: 0.3538\n",
      "Epoch 750/2000\n",
      "32s - loss: 0.0037 - acc: 0.9992 - val_loss: 6.0241 - val_acc: 0.3552\n",
      "Epoch 751/2000\n",
      "32s - loss: 0.0285 - acc: 0.9946 - val_loss: 5.8400 - val_acc: 0.3538\n",
      "Epoch 752/2000\n",
      "32s - loss: 0.0070 - acc: 0.9982 - val_loss: 5.9278 - val_acc: 0.3457\n",
      "Epoch 753/2000\n",
      "32s - loss: 0.0048 - acc: 0.9993 - val_loss: 5.9071 - val_acc: 0.3570\n",
      "Epoch 754/2000\n",
      "32s - loss: 0.0042 - acc: 0.9992 - val_loss: 5.8324 - val_acc: 0.3561\n",
      "Epoch 755/2000\n",
      "32s - loss: 0.0036 - acc: 0.9995 - val_loss: 5.9540 - val_acc: 0.3534\n",
      "Epoch 756/2000\n",
      "32s - loss: 0.0027 - acc: 0.9998 - val_loss: 5.9907 - val_acc: 0.3493\n",
      "Epoch 757/2000\n",
      "32s - loss: 0.0060 - acc: 0.9988 - val_loss: 5.9809 - val_acc: 0.3593\n",
      "Epoch 758/2000\n",
      "32s - loss: 0.0052 - acc: 0.9989 - val_loss: 5.9593 - val_acc: 0.3552\n",
      "Epoch 759/2000\n",
      "32s - loss: 0.0055 - acc: 0.9991 - val_loss: 5.9258 - val_acc: 0.3570\n",
      "Epoch 760/2000\n",
      "32s - loss: 0.0034 - acc: 0.9995 - val_loss: 5.9418 - val_acc: 0.3543\n",
      "Epoch 761/2000\n",
      "32s - loss: 0.0042 - acc: 0.9991 - val_loss: 6.0301 - val_acc: 0.3394\n",
      "Epoch 762/2000\n",
      "32s - loss: 0.0061 - acc: 0.9984 - val_loss: 5.9568 - val_acc: 0.3606\n",
      "Epoch 763/2000\n",
      "32s - loss: 0.0051 - acc: 0.9990 - val_loss: 5.8701 - val_acc: 0.3624\n",
      "Epoch 764/2000\n",
      "32s - loss: 0.0047 - acc: 0.9988 - val_loss: 5.9882 - val_acc: 0.3543\n",
      "Epoch 765/2000\n",
      "32s - loss: 0.0062 - acc: 0.9989 - val_loss: 5.9198 - val_acc: 0.3579\n",
      "Epoch 766/2000\n",
      "32s - loss: 0.0057 - acc: 0.9985 - val_loss: 5.9547 - val_acc: 0.3638\n",
      "Epoch 767/2000\n",
      "32s - loss: 0.0028 - acc: 0.9997 - val_loss: 5.9410 - val_acc: 0.3543\n",
      "Epoch 768/2000\n",
      "32s - loss: 0.0040 - acc: 0.9993 - val_loss: 5.9518 - val_acc: 0.3484\n",
      "Epoch 769/2000\n",
      "32s - loss: 0.0029 - acc: 0.9997 - val_loss: 5.9776 - val_acc: 0.3493\n",
      "Epoch 770/2000\n",
      "32s - loss: 0.0036 - acc: 0.9994 - val_loss: 5.9836 - val_acc: 0.3511\n",
      "Epoch 771/2000\n",
      "32s - loss: 0.0041 - acc: 0.9991 - val_loss: 6.0806 - val_acc: 0.3557\n",
      "Epoch 772/2000\n",
      "32s - loss: 0.0026 - acc: 0.9997 - val_loss: 6.0013 - val_acc: 0.3489\n",
      "Epoch 773/2000\n",
      "32s - loss: 0.0045 - acc: 0.9992 - val_loss: 5.8890 - val_acc: 0.3570\n",
      "Epoch 774/2000\n",
      "32s - loss: 0.0044 - acc: 0.9989 - val_loss: 5.9831 - val_acc: 0.3575\n",
      "Epoch 775/2000\n",
      "32s - loss: 0.0057 - acc: 0.9985 - val_loss: 6.0483 - val_acc: 0.3597\n",
      "Epoch 776/2000\n",
      "32s - loss: 0.0042 - acc: 0.9992 - val_loss: 6.1143 - val_acc: 0.3593\n",
      "Epoch 777/2000\n",
      "32s - loss: 0.0046 - acc: 0.9991 - val_loss: 5.9650 - val_acc: 0.3552\n",
      "Epoch 778/2000\n",
      "32s - loss: 0.0029 - acc: 0.9993 - val_loss: 5.9787 - val_acc: 0.3543\n",
      "Epoch 779/2000\n",
      "32s - loss: 0.0022 - acc: 0.9998 - val_loss: 5.9575 - val_acc: 0.3543\n",
      "Epoch 780/2000\n",
      "32s - loss: 0.0028 - acc: 0.9997 - val_loss: 6.0037 - val_acc: 0.3561\n",
      "Epoch 781/2000\n",
      "32s - loss: 0.0025 - acc: 0.9998 - val_loss: 5.9972 - val_acc: 0.3579\n",
      "Epoch 782/2000\n",
      "32s - loss: 0.0027 - acc: 0.9998 - val_loss: 6.0428 - val_acc: 0.3579\n",
      "Epoch 783/2000\n",
      "32s - loss: 0.0019 - acc: 1.0000 - val_loss: 6.0751 - val_acc: 0.3529\n",
      "Epoch 784/2000\n",
      "32s - loss: 0.0074 - acc: 0.9984 - val_loss: 6.0183 - val_acc: 0.3548\n",
      "Epoch 785/2000\n",
      "32s - loss: 0.0024 - acc: 0.9997 - val_loss: 6.0845 - val_acc: 0.3606\n",
      "Epoch 786/2000\n",
      "32s - loss: 0.0031 - acc: 0.9994 - val_loss: 6.0030 - val_acc: 0.3579\n",
      "Epoch 787/2000\n",
      "32s - loss: 0.0025 - acc: 0.9997 - val_loss: 6.0496 - val_acc: 0.3579\n",
      "Epoch 788/2000\n",
      "32s - loss: 0.0040 - acc: 0.9987 - val_loss: 6.0543 - val_acc: 0.3633\n",
      "Epoch 789/2000\n",
      "32s - loss: 0.0029 - acc: 0.9994 - val_loss: 6.0475 - val_acc: 0.3588\n",
      "Epoch 790/2000\n",
      "32s - loss: 0.0033 - acc: 0.9993 - val_loss: 6.0651 - val_acc: 0.3525\n",
      "Epoch 791/2000\n",
      "32s - loss: 0.0046 - acc: 0.9988 - val_loss: 6.0133 - val_acc: 0.3584\n",
      "Epoch 792/2000\n",
      "32s - loss: 0.0046 - acc: 0.9990 - val_loss: 6.0443 - val_acc: 0.3588\n",
      "Epoch 793/2000\n",
      "32s - loss: 0.0063 - acc: 0.9989 - val_loss: 6.0293 - val_acc: 0.3593\n",
      "Epoch 794/2000\n",
      "32s - loss: 0.0024 - acc: 0.9998 - val_loss: 6.0649 - val_acc: 0.3543\n",
      "Epoch 795/2000\n",
      "32s - loss: 0.0027 - acc: 0.9997 - val_loss: 6.0532 - val_acc: 0.3529\n",
      "Epoch 796/2000\n",
      "32s - loss: 0.0024 - acc: 0.9997 - val_loss: 6.0165 - val_acc: 0.3570\n",
      "Epoch 797/2000\n",
      "33s - loss: 0.0021 - acc: 0.9998 - val_loss: 6.0896 - val_acc: 0.3471\n",
      "Epoch 798/2000\n",
      "32s - loss: 0.0024 - acc: 0.9995 - val_loss: 6.0729 - val_acc: 0.3475\n",
      "Epoch 799/2000\n",
      "32s - loss: 0.0040 - acc: 0.9991 - val_loss: 6.0238 - val_acc: 0.3566\n",
      "Epoch 800/2000\n",
      "32s - loss: 0.0020 - acc: 0.9999 - val_loss: 6.1047 - val_acc: 0.3538\n",
      "Epoch 801/2000\n",
      "32s - loss: 0.0021 - acc: 0.9998 - val_loss: 6.0328 - val_acc: 0.3516\n",
      "Epoch 802/2000\n",
      "32s - loss: 0.0025 - acc: 0.9996 - val_loss: 6.1946 - val_acc: 0.3557\n",
      "Epoch 803/2000\n",
      "32s - loss: 0.0028 - acc: 0.9999 - val_loss: 6.0915 - val_acc: 0.3534\n",
      "Epoch 804/2000\n",
      "32s - loss: 0.0018 - acc: 0.9999 - val_loss: 6.1013 - val_acc: 0.3552\n",
      "Epoch 805/2000\n",
      "32s - loss: 0.0054 - acc: 0.9984 - val_loss: 6.1123 - val_acc: 0.3489\n",
      "Epoch 806/2000\n",
      "32s - loss: 0.0030 - acc: 0.9995 - val_loss: 6.0684 - val_acc: 0.3548\n",
      "Epoch 807/2000\n",
      "32s - loss: 0.0054 - acc: 0.9984 - val_loss: 6.0614 - val_acc: 0.3543\n",
      "Epoch 808/2000\n",
      "32s - loss: 0.0077 - acc: 0.9983 - val_loss: 6.0008 - val_acc: 0.3548\n",
      "Epoch 809/2000\n",
      "32s - loss: 0.0085 - acc: 0.9975 - val_loss: 6.1001 - val_acc: 0.3448\n",
      "Epoch 810/2000\n",
      "32s - loss: 0.0060 - acc: 0.9987 - val_loss: 6.0044 - val_acc: 0.3566\n",
      "Epoch 811/2000\n",
      "32s - loss: 0.0058 - acc: 0.9989 - val_loss: 6.0408 - val_acc: 0.3570\n",
      "Epoch 812/2000\n",
      "32s - loss: 0.0046 - acc: 0.9985 - val_loss: 6.0818 - val_acc: 0.3493\n",
      "Epoch 813/2000\n",
      "32s - loss: 0.0056 - acc: 0.9988 - val_loss: 6.0323 - val_acc: 0.3480\n",
      "Epoch 814/2000\n",
      "32s - loss: 0.0038 - acc: 0.9993 - val_loss: 6.1612 - val_acc: 0.3525\n",
      "Epoch 815/2000\n",
      "32s - loss: 0.0022 - acc: 0.9999 - val_loss: 6.1786 - val_acc: 0.3480\n",
      "Epoch 816/2000\n",
      "32s - loss: 0.0025 - acc: 0.9996 - val_loss: 6.0507 - val_acc: 0.3529\n",
      "Epoch 817/2000\n",
      "32s - loss: 0.0023 - acc: 0.9997 - val_loss: 6.1638 - val_acc: 0.3493\n",
      "Epoch 818/2000\n",
      "32s - loss: 0.0355 - acc: 0.9907 - val_loss: 6.0173 - val_acc: 0.3507\n",
      "Epoch 819/2000\n",
      "32s - loss: 0.0092 - acc: 0.9976 - val_loss: 6.0582 - val_acc: 0.3561\n",
      "Epoch 820/2000\n",
      "32s - loss: 0.0053 - acc: 0.9985 - val_loss: 6.0165 - val_acc: 0.3471\n",
      "Epoch 821/2000\n",
      "32s - loss: 0.0084 - acc: 0.9977 - val_loss: 6.0282 - val_acc: 0.3425\n",
      "Epoch 822/2000\n",
      "32s - loss: 0.0093 - acc: 0.9973 - val_loss: 6.0765 - val_acc: 0.3507\n",
      "Epoch 823/2000\n",
      "32s - loss: 0.0104 - acc: 0.9974 - val_loss: 6.2550 - val_acc: 0.3339\n",
      "Epoch 824/2000\n",
      "32s - loss: 0.0090 - acc: 0.9968 - val_loss: 5.9610 - val_acc: 0.3507\n",
      "Epoch 825/2000\n",
      "32s - loss: 0.0058 - acc: 0.9988 - val_loss: 6.0582 - val_acc: 0.3561\n",
      "Epoch 826/2000\n",
      "32s - loss: 0.0055 - acc: 0.9985 - val_loss: 5.9297 - val_acc: 0.3520\n",
      "Epoch 827/2000\n",
      "32s - loss: 0.0048 - acc: 0.9989 - val_loss: 6.0748 - val_acc: 0.3543\n",
      "Epoch 828/2000\n",
      "32s - loss: 0.0030 - acc: 0.9997 - val_loss: 6.0151 - val_acc: 0.3579\n",
      "Epoch 829/2000\n",
      "32s - loss: 0.0032 - acc: 0.9994 - val_loss: 6.0222 - val_acc: 0.3588\n",
      "Epoch 830/2000\n",
      "32s - loss: 0.0019 - acc: 0.9999 - val_loss: 6.0268 - val_acc: 0.3525\n",
      "Epoch 831/2000\n",
      "32s - loss: 0.0023 - acc: 0.9999 - val_loss: 6.0202 - val_acc: 0.3511\n",
      "Epoch 832/2000\n",
      "32s - loss: 0.0023 - acc: 0.9996 - val_loss: 6.0731 - val_acc: 0.3593\n",
      "Epoch 833/2000\n",
      "32s - loss: 0.0056 - acc: 0.9983 - val_loss: 6.0147 - val_acc: 0.3538\n",
      "Epoch 834/2000\n",
      "32s - loss: 0.0033 - acc: 0.9996 - val_loss: 6.0579 - val_acc: 0.3566\n",
      "Epoch 835/2000\n",
      "32s - loss: 0.0046 - acc: 0.9988 - val_loss: 6.0580 - val_acc: 0.3538\n",
      "Epoch 836/2000\n",
      "32s - loss: 0.0029 - acc: 0.9995 - val_loss: 6.0382 - val_acc: 0.3602\n",
      "Epoch 837/2000\n",
      "32s - loss: 0.0030 - acc: 0.9995 - val_loss: 5.9844 - val_acc: 0.3606\n",
      "Epoch 838/2000\n",
      "32s - loss: 0.0031 - acc: 0.9994 - val_loss: 6.0956 - val_acc: 0.3557\n",
      "Epoch 839/2000\n",
      "32s - loss: 0.0027 - acc: 0.9997 - val_loss: 6.1079 - val_acc: 0.3516\n",
      "Epoch 840/2000\n",
      "32s - loss: 0.0047 - acc: 0.9988 - val_loss: 6.1336 - val_acc: 0.3552\n",
      "Epoch 841/2000\n",
      "32s - loss: 0.0042 - acc: 0.9992 - val_loss: 6.0313 - val_acc: 0.3516\n",
      "Epoch 842/2000\n",
      "32s - loss: 0.0039 - acc: 0.9988 - val_loss: 6.1051 - val_acc: 0.3502\n",
      "Epoch 843/2000\n",
      "32s - loss: 0.0085 - acc: 0.9977 - val_loss: 6.1251 - val_acc: 0.3557\n",
      "Epoch 844/2000\n",
      "32s - loss: 0.0039 - acc: 0.9992 - val_loss: 6.0583 - val_acc: 0.3516\n",
      "Epoch 845/2000\n",
      "32s - loss: 0.0039 - acc: 0.9987 - val_loss: 6.1095 - val_acc: 0.3561\n",
      "Epoch 846/2000\n",
      "32s - loss: 0.0021 - acc: 0.9998 - val_loss: 6.0850 - val_acc: 0.3548\n",
      "Epoch 847/2000\n",
      "32s - loss: 0.0035 - acc: 0.9989 - val_loss: 6.0573 - val_acc: 0.3588\n",
      "Epoch 848/2000\n",
      "32s - loss: 0.0035 - acc: 0.9993 - val_loss: 6.1124 - val_acc: 0.3471\n",
      "Epoch 849/2000\n",
      "32s - loss: 0.0028 - acc: 0.9996 - val_loss: 6.0489 - val_acc: 0.3529\n",
      "Epoch 850/2000\n",
      "32s - loss: 0.0034 - acc: 0.9995 - val_loss: 6.1008 - val_acc: 0.3557\n",
      "Epoch 851/2000\n",
      "32s - loss: 0.0036 - acc: 0.9991 - val_loss: 6.0451 - val_acc: 0.3552\n",
      "Epoch 852/2000\n",
      "32s - loss: 0.0057 - acc: 0.9982 - val_loss: 6.1006 - val_acc: 0.3520\n",
      "Epoch 853/2000\n",
      "32s - loss: 0.0021 - acc: 0.9997 - val_loss: 6.1468 - val_acc: 0.3529\n",
      "Epoch 854/2000\n",
      "32s - loss: 0.0022 - acc: 0.9997 - val_loss: 6.1324 - val_acc: 0.3511\n",
      "Epoch 855/2000\n",
      "32s - loss: 0.0017 - acc: 0.9999 - val_loss: 6.1344 - val_acc: 0.3471\n",
      "Epoch 856/2000\n",
      "32s - loss: 0.0018 - acc: 0.9999 - val_loss: 6.1296 - val_acc: 0.3529\n",
      "Epoch 857/2000\n",
      "32s - loss: 0.0020 - acc: 0.9998 - val_loss: 6.1231 - val_acc: 0.3471\n",
      "Epoch 858/2000\n",
      "32s - loss: 0.0025 - acc: 0.9997 - val_loss: 6.1435 - val_acc: 0.3543\n",
      "Epoch 859/2000\n",
      "32s - loss: 0.0084 - acc: 0.9976 - val_loss: 6.1263 - val_acc: 0.3516\n",
      "Epoch 860/2000\n",
      "32s - loss: 0.0042 - acc: 0.9992 - val_loss: 6.1730 - val_acc: 0.3602\n",
      "Epoch 861/2000\n",
      "32s - loss: 0.0054 - acc: 0.9989 - val_loss: 6.1339 - val_acc: 0.3566\n",
      "Epoch 862/2000\n",
      "43s - loss: 0.0275 - acc: 0.9938 - val_loss: 6.1035 - val_acc: 0.3529\n",
      "Epoch 863/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-a91ee9c75e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           validation_data=(X_test, y_test), verbose=2)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m score, acc = model.evaluate(X_test, y_test,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "res = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2000,\n",
    "          validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "date = str(datetime.date.today() )\n",
    "time = str(datetime.datetime.now().time())[:-7]\n",
    "\n",
    "filename = './lstm1_' + '_' + date + '_' +time;\n",
    "with open( filename, 'wb') as output:\n",
    "    pickle.dump([res.model.get_config(), res.model.get_weights(), res.history], output, pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = [1.4223, 1.4112, 1.4105, 1.4039, 1.3943, 1.3897, 1.3852, 1.3769, 1.3618, 1.3530, 1.3558, 1.3452, 1.3363, 1.3299, 1.3241, 1.3087, 1.2970, 1.2945, 1.2873, 1.2816, 1.2714, 1.2577, 1.2571, 1.2433, 1.2399, 1.2387, 1.2172, 1.2115, 1.2086, 1.1871, 1.1882, 1.1820, 1.1774, 1.1582, 1.1602, 1.1452, 1.1404, 1.1255, 1.1200, 1.1190, 1.1042, 1.1172, 1.0941, 1.0888, 1.0743, 1.0671, 1.0605, 1.0514, 1.0466, 1.0426, 1.0439, 1.0223, 1.0273, 1.0154, 1.0097, 1.0013, 1.0026, 0.9801, 0.9875, 0.9854, 0.9744, 0.9564, 0.9588, 0.9592, 0.9485, 0.9385, 0.9351, 0.9352, 0.9356, 0.9179, 0.9087, 0.9191, 0.9316, 0.8896, 0.8929, 0.8910, 0.8815, 0.8694, 0.8721, 0.8561, 0.8564, 0.8499, 0.8356, 0.8335, 0.8246, 0.8385, 0.8177, 0.8358, 0.8171, 0.8000, 0.8085, 0.8039, 0.7829, 0.7890, 0.7842, 0.7661, 0.7644, 0.7609, 0.7570, 0.7511, 0.7684, 0.7431, 0.7370, 0.7491, 0.7364, 0.7296, 0.7279, 0.7000, 0.7286, 0.7029, 0.7006, 0.6914, 0.6951, 0.6761, 0.6824, 0.6749, 0.6810, 0.6669, 0.6577, 0.6679, 0.6530, 0.6485, 0.6384, 0.6365, 0.6392, 0.6237, 0.6113, 0.6132, 0.6077, 0.6010, 0.6056, 0.6001, 0.5842, 0.5867, 0.5989, 0.5615, 0.5596, 0.5830, 0.5626, 0.5279, 0.5618, 0.5547, 0.5350, 0.5289, 0.5091, 0.5427, 0.4985, 0.5177, 0.5191, 0.4795, 0.4921, 0.4922, 0.4835, 0.4836, 0.4841, 0.4824, 0.4503, 0.4747, 0.4421, 0.4431, 0.4642, 0.4356, 0.4203, 0.4317, 0.4419, 0.4253, 0.4277, 0.4207, 0.3796, 0.3960, 0.3859, 0.3967, 0.3740, 0.3759, 0.3981, 0.3665, 0.3670, 0.3448, 0.3763, 0.3634, 0.3119, 0.3582, 0.3365, 0.3195, 0.3146, 0.3051, 0.3299, 0.3331, 0.3305, 0.3272, 0.3203, 0.2799, 0.2832, 0.2848, 0.2773, 0.3023, 0.2920, 0.2644, 0.2627, 0.2660, 0.2505, 0.2747, 0.2490, 0.2487, 0.2403, 0.2498, 0.2170, 0.2305, 0.2389, 0.2321, 0.2222, 0.2330, 0.2362, 0.2021, 0.2340, 0.1993, 0.2055, 0.1825, 0.1933, 0.2026, 0.1911, 0.1861, 0.1938, 0.2176, 0.1903, 0.1919, 0.1721, 0.1793, 0.1580, 0.1694, 0.1588, 0.1849, 0.1492, 0.1675, 0.1480, 0.1621, 0.1410, 0.1473, 0.1331, 0.1776, 0.1571, 0.1488, 0.1407, 0.1349, 0.1355, 0.1530, 0.1608, 0.1320, 0.1203, 0.1175, 0.1149, 0.1415, 0.1336, 0.1302, 0.1246, 0.1107, 0.1446, 0.1155, 0.0938, 0.1238, 0.1246, 0.1052, 0.1443, 0.0917, 0.1116, 0.0879, 0.0781, 0.1026, 0.1000, 0.1017, 0.1074, 0.0858, 0.0912, 0.0834, 0.0812, 0.0809, 0.0799, 0.0984, 0.0812, 0.0822, 0.0795, 0.0965, 0.0727, 0.0901, 0.0981, 0.0708, 0.0683, 0.0664, 0.0621, 0.0887, 0.0814, 0.0784, 0.0737, 0.0765, 0.0777, 0.0630, 0.0898, 0.0679, 0.0614, 0.0594, 0.0664, 0.0699, 0.0693, 0.0734, 0.0706, 0.0652, 0.0713, 0.0707, 0.0649, 0.0584, 0.0532, 0.0568, 0.0775, 0.0805, 0.0553, 0.0492, 0.0790, 0.0606, 0.0587, 0.0588, 0.0508, 0.0415, 0.0438, 0.0760, 0.0430, 0.0435, 0.0454, 0.0512, 0.1283, 0.0473, 0.0424, 0.0442, 0.0436, 0.0535, 0.0357, 0.0779, 0.0458, 0.0411, 0.0863, 0.0644, 0.0421, 0.0418, 0.0337, 0.0389, 0.0394, 0.0455, 0.0400, 0.0391, 0.0371, 0.0294, 0.0472, 0.0335, 0.0326, 0.0440, 0.0319, 0.0314, 0.0749, 0.0354, 0.0317, 0.0403, 0.0724, 0.0308, 0.0430, 0.0334, 0.0290, 0.0348, 0.0279, 0.0302, 0.0339, 0.0275, 0.0251, 0.0321, 0.0437, 0.0316, 0.0269, 0.0285, 0.0326, 0.0239, 0.0314, 0.0313, 0.0204, 0.0408, 0.0219, 0.0221, 0.0293, 0.0253, 0.0286, 0.0231, 0.0326, 0.0261, 0.0199, 0.0345, 0.0253, 0.0520, 0.0398, 0.0347, 0.0290, 0.0306, 0.0400, 0.0297, 0.0262, 0.0238, 0.0460, 0.0330, 0.0203, 0.0224, 0.0273, 0.0190, 0.0252, 0.0225, 0.0187, 0.0197, 0.0222, 0.0183, 0.0166, 0.0175, 0.0274, 0.0181, 0.0208, 0.0272, 0.0299, 0.0184, 0.0144, 0.0169, 0.0169, 0.0390, 0.0268, 0.0327, 0.0347, 0.0202, 0.0152, 0.0142, 0.0247, 0.0370, 0.0203, 0.0233, 0.0218, 0.0239, 0.0277, 0.0225, 0.0200, 0.0203, 0.0175, 0.0176, 0.0150, 0.0160, 0.0127, 0.0282, 0.0163, 0.0268, 0.0178, 0.0191, 0.0237, 0.0181, 0.0136, 0.0112, 0.0285, 0.0149, 0.0151, 0.0126, 0.0184, 0.0408, 0.0103, 0.0150, 0.0182, 0.0125, 0.0525, 0.0370, 0.0227, 0.0159, 0.0142, 0.0118, 0.0344, 0.0158, 0.0160, 0.0104, 0.0094, 0.0535, 0.0206, 0.0152, 0.0172, 0.0140, 0.0118, 0.0094, 0.0109, 0.0147, 0.0121, 0.0124, 0.0168, 0.0130, 0.0141, 0.0262, 0.0745, 0.0262, 0.0239, 0.0136, 0.0124, 0.0118, 0.0106, 0.0116, 0.0262, 0.0182, 0.0353, 0.1246, 0.0201, 0.0344, 0.0207, 0.0127, 0.0149, 0.0156, 0.0188, 0.0130, 0.0180, 0.0181, 0.0118, 0.0159, 0.0141, 0.0116, 0.0153, 0.0108, 0.0114, 0.0080, 0.0142, 0.0091, 0.0075, 0.0072, 0.0089, 0.0073, 0.0168, 0.0103, 0.0083, 0.0060, 0.0068, 0.0100, 0.0086, 0.0083, 0.0203, 0.0192, 0.0120, 0.0100, 0.0096, 0.0190, 0.0112, 0.0123, 0.0564, 0.0150, 0.0123, 0.0091, 0.0211, 0.0085, 0.0093, 0.0071, 0.0249, 0.0146, 0.0097, 0.0124, 0.0101, 0.0062, 0.0059, 0.0077, 0.0071, 0.0059, 0.0060, 0.0060, 0.0054, 0.0054, 0.0058, 0.0054, 0.0082, 0.0074, 0.0067, 0.0072, 0.0083, 0.0145, 0.0106, 0.0165, 0.0121, 0.0076, 0.0069, 0.0070, 0.0075, 0.0058, 0.0104, 0.0129, 0.0075, 0.0215, 0.0110, 0.0078, 0.0129, 0.0095, 0.0150, 0.0086, 0.0140, 0.0100, 0.0079, 0.0133, 0.0084, 0.0092, 0.0064, 0.0058, 0.0080, 0.0184, 0.0075, 0.0086, 0.0086, 0.0076, 0.0072, 0.0072, 0.0217, 0.0087, 0.0054, 0.0076, 0.0068, 0.0082, 0.0281, 0.0094, 0.0156, 0.0090, 0.0053, 0.0153, 0.0187, 0.0105, 0.0107, 0.0098, 0.0099, 0.0123, 0.0101, 0.0071, 0.0060, 0.0068, 0.0085, 0.0401, 0.0545, 0.0133, 0.0109, 0.0100, 0.0080, 0.0088, 0.0093, 0.0063, 0.0090, 0.0049, 0.0058, 0.0072, 0.0113, 0.0086, 0.0072, 0.0087, 0.0081, 0.0074, 0.0070, 0.0062, 0.0053, 0.0077, 0.0068, 0.0103, 0.0098, 0.0079, 0.0100, 0.0074, 0.0061, 0.0050, 0.0064, 0.0034, 0.0048, 0.0071, 0.0061, 0.0049, 0.0082, 0.0054, 0.0032, 0.0069, 0.0078, 0.0224, 0.0407, 0.0144, 0.0133, 0.0301, 0.0093, 0.0069, 0.0069, 0.0065, 0.0072, 0.0080, 0.0043, 0.0046, 0.0046, 0.0032, 0.0061, 0.0037, 0.0058, 0.0035, 0.0042, 0.0107, 0.0054, 0.0079, 0.0102, 0.0089, 0.0160, 0.0153, 0.0082, 0.0046, 0.0038, 0.0240, 0.0061, 0.0066, 0.0046, 0.0055, 0.0153, 0.0095, 0.0068, 0.0169, 0.0069, 0.0050, 0.0044, 0.0039, 0.0035, 0.0061, 0.0052, 0.0042, 0.0039, 0.0047, 0.0035, 0.0022, 0.0045, 0.0041, 0.0068, 0.0039, 0.0036, 0.0025, 0.0051, 0.0041, 0.0037, 0.0027, 0.0022, 0.0066, 0.0028, 0.0056, 0.0230, 0.0052, 0.0039, 0.0090, 0.0037, 0.0049, 0.0030, 0.0058, 0.0035, 0.0048, 0.0034, 0.0037, 0.0285, 0.0070, 0.0048, 0.0042, 0.0036, 0.0027, 0.0060, 0.0052, 0.0055, 0.0034, 0.0042, 0.0061, 0.0051, 0.0047, 0.0062, 0.0057, 0.0028, 0.0040, 0.0029, 0.0036, 0.0041, 0.0026, 0.0045, 0.0044, 0.0057, 0.0042, 0.0046, 0.0029, 0.0022, 0.0028, 0.0025, 0.0027, 0.0019, 0.0074, 0.0024, 0.0031, 0.0025, 0.0040, 0.0029, 0.0033, 0.0046, 0.0046, 0.0063, 0.0024, 0.0027, 0.0024, 0.0021, 0.0024, 0.0040, 0.0020, 0.0021, 0.0025, 0.0028, 0.0018, 0.0054, 0.0030, 0.0054, 0.0077, 0.0085, 0.0060, 0.0058, 0.0046, 0.0056, 0.0038, 0.0022, 0.0025, 0.0023, 0.0355, 0.0092, 0.0053, 0.0084, 0.0093, 0.0104, 0.0090, 0.0058, 0.0055, 0.0048, 0.0030, 0.0032, 0.0019, 0.0023, 0.0023, 0.0056, 0.0033, 0.0046, 0.0029, 0.0030, 0.0031, 0.0027, 0.0047, 0.0042, 0.0039, 0.0085, 0.0039, 0.0039, 0.0021, 0.0035, 0.0035, 0.0028, 0.0034, 0.0036, 0.0057, 0.0021, 0.0022, 0.0017, 0.0018, 0.0020, 0.0025, 0.0084, 0.0042, 0.0054, 0.0275]\n",
    "train_acc = [0.3806, 0.3896, 0.3968, 0.3976, 0.3982, 0.4012, 0.4023, 0.4104, 0.4131, 0.4188, 0.4139, 0.4187, 0.4217, 0.4321, 0.4299, 0.4335, 0.4412, 0.4393, 0.4412, 0.4440, 0.4497, 0.4526, 0.4546, 0.4605, 0.4604, 0.4610, 0.4731, 0.4712, 0.4709, 0.4847, 0.4797, 0.4830, 0.4790, 0.4984, 0.4922, 0.4982, 0.5005, 0.5102, 0.5094, 0.5081, 0.5207, 0.5106, 0.5220, 0.5217, 0.5326, 0.5304, 0.5354, 0.5401, 0.5385, 0.5416, 0.5416, 0.5513, 0.5531, 0.5493, 0.5568, 0.5596, 0.5555, 0.5631, 0.5663, 0.5669, 0.5692, 0.5726, 0.5784, 0.5791, 0.5862, 0.5879, 0.5911, 0.5920, 0.5943, 0.5998, 0.6051, 0.5995, 0.5923, 0.6094, 0.6125, 0.6164, 0.6134, 0.6229, 0.6218, 0.6308, 0.6212, 0.6294, 0.6440, 0.6377, 0.6448, 0.6368, 0.6469, 0.6404, 0.6476, 0.6580, 0.6557, 0.6535, 0.6659, 0.6594, 0.6612, 0.6711, 0.6736, 0.6763, 0.6772, 0.6830, 0.6709, 0.6830, 0.6873, 0.6853, 0.6885, 0.6934, 0.6936, 0.7053, 0.6906, 0.7013, 0.7052, 0.7081, 0.7072, 0.7181, 0.7053, 0.7212, 0.7129, 0.7168, 0.7249, 0.7190, 0.7300, 0.7322, 0.7333, 0.7370, 0.7408, 0.7396, 0.7516, 0.7484, 0.7499, 0.7522, 0.7496, 0.7516, 0.7664, 0.7611, 0.7538, 0.7743, 0.7718, 0.7664, 0.7725, 0.7852, 0.7726, 0.7745, 0.7843, 0.7892, 0.8008, 0.7874, 0.8066, 0.7997, 0.7941, 0.8121, 0.8077, 0.8089, 0.8114, 0.8119, 0.8085, 0.8141, 0.8222, 0.8175, 0.8317, 0.8303, 0.8258, 0.8335, 0.8416, 0.8360, 0.8353, 0.8366, 0.8384, 0.8418, 0.8619, 0.8554, 0.8554, 0.8523, 0.8583, 0.8646, 0.8545, 0.8632, 0.8657, 0.8762, 0.8610, 0.8677, 0.8879, 0.8683, 0.8769, 0.8820, 0.8907, 0.8948, 0.8839, 0.8798, 0.8854, 0.8813, 0.8838, 0.9024, 0.9039, 0.9026, 0.9054, 0.8945, 0.8961, 0.9091, 0.9098, 0.9086, 0.9133, 0.9041, 0.9159, 0.9149, 0.9160, 0.9168, 0.9237, 0.9222, 0.9179, 0.9196, 0.9213, 0.9228, 0.9191, 0.9340, 0.9217, 0.9333, 0.9312, 0.9399, 0.9359, 0.9317, 0.9375, 0.9377, 0.9378, 0.9313, 0.9371, 0.9364, 0.9445, 0.9386, 0.9481, 0.9435, 0.9476, 0.9398, 0.9536, 0.9460, 0.9524, 0.9461, 0.9565, 0.9498, 0.9571, 0.9466, 0.9484, 0.9515, 0.9541, 0.9565, 0.9576, 0.9503, 0.9468, 0.9568, 0.9604, 0.9616, 0.9630, 0.9553, 0.9600, 0.9565, 0.9599, 0.9649, 0.9548, 0.9628, 0.9703, 0.9601, 0.9624, 0.9681, 0.9575, 0.9710, 0.9631, 0.9733, 0.9750, 0.9647, 0.9692, 0.9671, 0.9641, 0.9730, 0.9712, 0.9740, 0.9759, 0.9739, 0.9764, 0.9717, 0.9755, 0.9749, 0.9755, 0.9711, 0.9783, 0.9747, 0.9683, 0.9781, 0.9781, 0.9805, 0.9829, 0.9731, 0.9738, 0.9776, 0.9768, 0.9769, 0.9759, 0.9809, 0.9738, 0.9790, 0.9806, 0.9813, 0.9797, 0.9797, 0.9812, 0.9774, 0.9780, 0.9805, 0.9763, 0.9798, 0.9804, 0.9822, 0.9836, 0.9831, 0.9792, 0.9758, 0.9844, 0.9867, 0.9777, 0.9813, 0.9814, 0.9823, 0.9855, 0.9882, 0.9863, 0.9785, 0.9877, 0.9875, 0.9860, 0.9838, 0.9665, 0.9860, 0.9887, 0.9881, 0.9883, 0.9841, 0.9908, 0.9781, 0.9868, 0.9880, 0.9765, 0.9798, 0.9876, 0.9881, 0.9914, 0.9883, 0.9894, 0.9861, 0.9875, 0.9891, 0.9899, 0.9926, 0.9856, 0.9905, 0.9896, 0.9880, 0.9913, 0.9916, 0.9768, 0.9904, 0.9911, 0.9886, 0.9796, 0.9919, 0.9859, 0.9897, 0.9919, 0.9900, 0.9928, 0.9911, 0.9899, 0.9924, 0.9939, 0.9913, 0.9869, 0.9916, 0.9919, 0.9924, 0.9910, 0.9939, 0.9904, 0.9909, 0.9946, 0.9880, 0.9950, 0.9941, 0.9923, 0.9929, 0.9911, 0.9935, 0.9898, 0.9920, 0.9955, 0.9904, 0.9924, 0.9870, 0.9876, 0.9907, 0.9910, 0.9916, 0.9889, 0.9910, 0.9924, 0.9928, 0.9886, 0.9912, 0.9943, 0.9944, 0.9918, 0.9950, 0.9934, 0.9936, 0.9953, 0.9942, 0.9931, 0.9951, 0.9960, 0.9957, 0.9916, 0.9951, 0.9946, 0.9916, 0.9920, 0.9955, 0.9969, 0.9952, 0.9953, 0.9878, 0.9913, 0.9919, 0.9899, 0.9948, 0.9961, 0.9965, 0.9927, 0.9907, 0.9948, 0.9932, 0.9935, 0.9937, 0.9922, 0.9945, 0.9948, 0.9940, 0.9954, 0.9948, 0.9972, 0.9969, 0.9971, 0.9920, 0.9962, 0.9917, 0.9951, 0.9952, 0.9928, 0.9959, 0.9970, 0.9979, 0.9922, 0.9964, 0.9960, 0.9969, 0.9961, 0.9883, 0.9982, 0.9960, 0.9942, 0.9971, 0.9876, 0.9897, 0.9951, 0.9970, 0.9968, 0.9974, 0.9892, 0.9952, 0.9950, 0.9976, 0.9979, 0.9883, 0.9942, 0.9956, 0.9947, 0.9966, 0.9969, 0.9977, 0.9969, 0.9962, 0.9973, 0.9973, 0.9948, 0.9966, 0.9952, 0.9914, 0.9836, 0.9924, 0.9929, 0.9966, 0.9966, 0.9968, 0.9980, 0.9973, 0.9924, 0.9953, 0.9910, 0.9702, 0.9951, 0.9902, 0.9938, 0.9964, 0.9961, 0.9950, 0.9946, 0.9965, 0.9947, 0.9953, 0.9968, 0.9966, 0.9969, 0.9971, 0.9959, 0.9973, 0.9979, 0.9985, 0.9962, 0.9981, 0.9990, 0.9989, 0.9978, 0.9989, 0.9947, 0.9974, 0.9983, 0.9990, 0.9991, 0.9980, 0.9980, 0.9980, 0.9941, 0.9942, 0.9972, 0.9979, 0.9978, 0.9953, 0.9977, 0.9969, 0.9861, 0.9963, 0.9968, 0.9981, 0.9944, 0.9984, 0.9979, 0.9985, 0.9931, 0.9959, 0.9974, 0.9973, 0.9972, 0.9993, 0.9992, 0.9982, 0.9984, 0.9992, 0.9991, 0.9989, 0.9992, 0.9989, 0.9990, 0.9992, 0.9980, 0.9985, 0.9990, 0.9988, 0.9983, 0.9967, 0.9969, 0.9965, 0.9970, 0.9985, 0.9989, 0.9985, 0.9982, 0.9988, 0.9972, 0.9966, 0.9985, 0.9934, 0.9966, 0.9981, 0.9964, 0.9975, 0.9956, 0.9982, 0.9965, 0.9977, 0.9982, 0.9963, 0.9982, 0.9980, 0.9985, 0.9985, 0.9982, 0.9946, 0.9984, 0.9978, 0.9980, 0.9984, 0.9985, 0.9987, 0.9934, 0.9979, 0.9991, 0.9979, 0.9984, 0.9978, 0.9934, 0.9975, 0.9956, 0.9977, 0.9991, 0.9957, 0.9945, 0.9968, 0.9966, 0.9969, 0.9976, 0.9969, 0.9973, 0.9985, 0.9990, 0.9985, 0.9982, 0.9909, 0.9867, 0.9969, 0.9979, 0.9968, 0.9979, 0.9983, 0.9972, 0.9989, 0.9972, 0.9992, 0.9988, 0.9982, 0.9972, 0.9969, 0.9985, 0.9977, 0.9982, 0.9985, 0.9983, 0.9988, 0.9991, 0.9983, 0.9988, 0.9969, 0.9973, 0.9978, 0.9973, 0.9981, 0.9984, 0.9988, 0.9985, 0.9997, 0.9991, 0.9979, 0.9984, 0.9988, 0.9972, 0.9989, 0.9995, 0.9985, 0.9985, 0.9939, 0.9924, 0.9956, 0.9975, 0.9922, 0.9977, 0.9989, 0.9987, 0.9985, 0.9983, 0.9982, 0.9994, 0.9992, 0.9992, 0.9994, 0.9985, 0.9995, 0.9987, 0.9994, 0.9990, 0.9968, 0.9990, 0.9981, 0.9974, 0.9975, 0.9951, 0.9960, 0.9984, 0.9989, 0.9996, 0.9929, 0.9984, 0.9982, 0.9991, 0.9990, 0.9957, 0.9978, 0.9985, 0.9954, 0.9980, 0.9994, 0.9993, 0.9994, 0.9997, 0.9983, 0.9987, 0.9996, 0.9993, 0.9992, 0.9994, 1.0000, 0.9992, 0.9993, 0.9984, 0.9994, 0.9996, 0.9999, 0.9989, 0.9993, 0.9993, 0.9998, 0.9998, 0.9978, 0.9998, 0.9990, 0.9953, 0.9988, 0.9993, 0.9978, 0.9993, 0.9990, 0.9997, 0.9983, 0.9997, 0.9992, 0.9995, 0.9992, 0.9946, 0.9982, 0.9993, 0.9992, 0.9995, 0.9998, 0.9988, 0.9989, 0.9991, 0.9995, 0.9991, 0.9984, 0.9990, 0.9988, 0.9989, 0.9985, 0.9997, 0.9993, 0.9997, 0.9994, 0.9991, 0.9997, 0.9992, 0.9989, 0.9985, 0.9992, 0.9991, 0.9993, 0.9998, 0.9997, 0.9998, 0.9998, 1.0000, 0.9984, 0.9997, 0.9994, 0.9997, 0.9987, 0.9994, 0.9993, 0.9988, 0.9990, 0.9989, 0.9998, 0.9997, 0.9997, 0.9998, 0.9995, 0.9991, 0.9999, 0.9998, 0.9996, 0.9999, 0.9999, 0.9984, 0.9995, 0.9984, 0.9983, 0.9975, 0.9987, 0.9989, 0.9985, 0.9988, 0.9993, 0.9999, 0.9996, 0.9997, 0.9907, 0.9976, 0.9985, 0.9977, 0.9973, 0.9974, 0.9968, 0.9988, 0.9985, 0.9989, 0.9997, 0.9994, 0.9999, 0.9999, 0.9996, 0.9983, 0.9996, 0.9988, 0.9995, 0.9995, 0.9994, 0.9997, 0.9988, 0.9992, 0.9988, 0.9977, 0.9992, 0.9987, 0.9998, 0.9989, 0.9993, 0.9996, 0.9995, 0.9991, 0.9982, 0.9997, 0.9997, 0.9999, 0.9999, 0.9998, 0.9997, 0.9976, 0.9992, 0.9989, 0.9938]\n",
    "val_loss = [1.4880, 1.4540, 1.4822, 1.4394, 1.4993, 1.4640, 1.4325, 1.4407, 1.4598, 1.4777, 1.4338, 1.4466, 1.5117, 1.4710, 1.4828, 1.4102, 1.4302, 1.4467, 1.4485, 1.4405, 1.4121, 1.4032, 1.4487, 1.5846, 1.5742, 1.4372, 1.4128, 1.4008, 1.3890, 1.4512, 1.3820, 1.4281, 1.5703, 1.4362, 1.4020, 1.4671, 1.4099, 1.3958, 1.4801, 1.4414, 1.4676, 1.4199, 1.5302, 1.4275, 1.4757, 1.4608, 1.6525, 1.4531, 1.4351, 1.4544, 1.5356, 1.4599, 1.5009, 1.5929, 1.5088, 1.5071, 1.7755, 1.6931, 1.7249, 1.5241, 1.6785, 1.5580, 1.5225, 1.5522, 1.8058, 1.9767, 1.7092, 1.6648, 1.5501, 1.7138, 1.6084, 1.6570, 1.6637, 1.6002, 1.6210, 1.7074, 1.6310, 1.6782, 1.7945, 1.6575, 1.8069, 1.6478, 1.6785, 1.6911, 1.7867, 1.8078, 1.6880, 1.6737, 1.7307, 2.4150, 1.8099, 2.7104, 1.7930, 1.7659, 2.1618, 1.7562, 1.8484, 1.8813, 1.8004, 1.9228, 1.8535, 1.9061, 1.9164, 2.0134, 1.8899, 1.9357, 1.9355, 1.9206, 1.9595, 1.9290, 2.0575, 2.1423, 1.9548, 2.0191, 2.0320, 1.9619, 2.2984, 2.2792, 2.4012, 2.0949, 2.0745, 2.0339, 2.0258, 2.2750, 2.0277, 2.5328, 2.1656, 2.1531, 2.1998, 2.0911, 2.3655, 2.1825, 2.3108, 2.4899, 2.1380, 2.2662, 2.1985, 2.3516, 2.9331, 4.6544, 2.1923, 2.2384, 2.2283, 2.2826, 2.3836, 2.3061, 2.3585, 2.2517, 2.7994, 2.2860, 2.3340, 2.3418, 2.4063, 2.6169, 2.5414, 2.4547, 2.4046, 2.3812, 2.5828, 2.4777, 2.4238, 2.5850, 3.1556, 2.5103, 2.5799, 2.6006, 2.5333, 2.5370, 3.3661, 2.7913, 2.7402, 2.6333, 2.9840, 2.6862, 2.6553, 2.8075, 2.7832, 2.7483, 2.9469, 2.7059, 2.7803, 3.6854, 2.8656, 2.7172, 3.1382, 2.8217, 2.8752, 2.7736, 2.9204, 2.8073, 3.1210, 2.9165, 2.9496, 3.2607, 3.0011, 3.0382, 2.9105, 2.9014, 3.0203, 3.0444, 3.4001, 2.9213, 2.9690, 3.0520, 3.1185, 3.0755, 3.2098, 3.1661, 3.3810, 3.1625, 3.3815, 3.3929, 3.1942, 3.2533, 3.2190, 3.1626, 3.1981, 3.3705, 3.2083, 3.3004, 3.4267, 3.3143, 3.4106, 3.3740, 3.2144, 3.2776, 3.5613, 3.4403, 3.5314, 3.4182, 3.4833, 3.5194, 3.5351, 3.5450, 3.5392, 3.5469, 3.6354, 3.6665, 3.5639, 3.6243, 3.5736, 3.6564, 3.7180, 3.6284, 3.6099, 3.6430, 3.8118, 3.6779, 3.6383, 3.6993, 3.8812, 3.9360, 3.7673, 3.8480, 3.7244, 3.8439, 4.0451, 3.7981, 3.9466, 4.0176, 3.9780, 3.8861, 3.7543, 3.8861, 4.0060, 4.0112, 4.1969, 4.0945, 4.0382, 4.1392, 4.0684, 4.0355, 4.2343, 4.1117, 4.0852, 4.1365, 4.0435, 4.3319, 4.0561, 4.0759, 4.2551, 4.0749, 4.2359, 3.9621, 4.2383, 4.2706, 4.2336, 4.2220, 4.4606, 4.2623, 4.2056, 4.3093, 4.2766, 4.2380, 4.3106, 4.3583, 4.2174, 4.3582, 4.4006, 4.4160, 4.4791, 4.2535, 4.4247, 4.2579, 4.3401, 4.3721, 4.4307, 4.3281, 4.3139, 4.4701, 4.4515, 6.1729, 4.6201, 4.5019, 4.3574, 4.3844, 4.3035, 4.4707, 4.4719, 4.5184, 4.5268, 4.4275, 4.5999, 4.5176, 4.5345, 4.5506, 4.5640, 4.6461, 4.2535, 4.4740, 4.5640, 4.5813, 4.7297, 4.8142, 4.6383, 4.8168, 4.5652, 4.7698, 4.6254, 4.5654, 4.7076, 4.5488, 4.6115, 4.6551, 4.6741, 4.9576, 4.7267, 4.7986, 4.7434, 4.7088, 4.8023, 4.7409, 4.7055, 4.7384, 4.7677, 5.3280, 4.6347, 4.7768, 4.7872, 4.7816, 4.6882, 4.6791, 4.7701, 4.7748, 5.1302, 4.8936, 4.8427, 4.7591, 4.8518, 4.8905, 4.9291, 4.8083, 5.0088, 4.9368, 4.9114, 4.9427, 4.9197, 4.9509, 5.1986, 4.9907, 5.0126, 5.0555, 5.0943, 4.9709, 4.9292, 5.0319, 4.9991, 5.1077, 5.0131, 5.1434, 5.0277, 5.0575, 5.1383, 5.1304, 5.0963, 5.0560, 5.0618, 5.0461, 5.1926, 5.0443, 5.0329, 5.1115, 5.0076, 5.0041, 5.1519, 5.2507, 5.0495, 5.2744, 5.1660, 5.1288, 5.1274, 5.1695, 5.0150, 5.1156, 5.1611, 5.2744, 5.0910, 5.1923, 5.1582, 5.4070, 5.1922, 5.1931, 5.2199, 5.3439, 5.2831, 5.1592, 5.1605, 5.0108, 5.1109, 5.2088, 5.1019, 5.1725, 5.2481, 5.1974, 5.1239, 5.1632, 5.2952, 5.1615, 5.1567, 5.2564, 5.3236, 5.1472, 5.2063, 5.2971, 5.2587, 5.2719, 5.3366, 5.2595, 5.2769, 5.1818, 5.3073, 5.2249, 5.2394, 5.2070, 5.2674, 5.4310, 5.2069, 5.3529, 5.3339, 5.3044, 5.7026, 5.1850, 5.4031, 5.3559, 5.2941, 5.3350, 5.1019, 5.2414, 5.2461, 5.2968, 5.4101, 6.2118, 5.4009, 5.3969, 5.3064, 5.3391, 5.3997, 5.4084, 5.2549, 5.2999, 5.3182, 5.3384, 5.4044, 5.4381, 5.3388, 5.3935, 5.5539, 5.3265, 5.3914, 5.4260, 5.4680, 5.3487, 5.4050, 5.3038, 5.4692, 5.4934, 5.4623, 5.5217, 5.4393, 5.4986, 5.4412, 5.3133, 5.2080, 5.1451, 5.2596, 5.4548, 5.2472, 5.2896, 5.3571, 5.4171, 5.4004, 5.3365, 5.2646, 5.2850, 5.3845, 5.4331, 5.3231, 5.4432, 5.4026, 5.3869, 5.4656, 5.4868, 5.3964, 5.3901, 5.4708, 5.4614, 5.5235, 5.4307, 5.4153, 5.5879, 5.5583, 5.5576, 5.6967, 5.5617, 5.4128, 5.6546, 5.5164, 5.4848, 5.6292, 5.6559, 5.5042, 5.5179, 5.5561, 6.0509, 5.5501, 5.5160, 5.5919, 5.3655, 5.4564, 5.5170, 5.6208, 5.5499, 5.4931, 5.5307, 5.5267, 5.5468, 5.5400, 5.5702, 5.5813, 5.6091, 5.5819, 5.6158, 5.5808, 5.6306, 5.6765, 5.6082, 5.5730, 5.6682, 5.6086, 5.6800, 5.6631, 5.6502, 5.6775, 5.6565, 5.6084, 5.6150, 5.5574, 5.6525, 5.6183, 5.6649, 5.6221, 5.6697, 5.8146, 5.7226, 5.6308, 5.6047, 5.6374, 5.6069, 5.6873, 5.6749, 5.6759, 5.7272, 5.6256, 5.6256, 5.6335, 5.6322, 5.7048, 5.7046, 5.7417, 5.7086, 5.7072, 5.5989, 5.7885, 5.7106, 5.7302, 5.6942, 5.6781, 5.5959, 5.5897, 5.7346, 5.6982, 5.7139, 5.7131, 5.7240, 5.7926, 5.7560, 5.6830, 5.7182, 5.7860, 5.8481, 5.6836, 5.7283, 5.8019, 5.7419, 5.7124, 5.6196, 5.6738, 5.7286, 5.7656, 5.7427, 5.8239, 6.0039, 5.5517, 5.6370, 5.6153, 5.6555, 5.6670, 5.7688, 5.7173, 5.7542, 5.7532, 5.7780, 5.7273, 5.6636, 5.7953, 5.7177, 5.6521, 5.7358, 5.7650, 5.8487, 5.7651, 5.7747, 5.7414, 5.8322, 5.6961, 5.7723, 5.7963, 5.7598, 5.6966, 5.7939, 5.7494, 5.7543, 5.8795, 5.7851, 5.8461, 5.7794, 5.7664, 5.7774, 5.9159, 5.8718, 5.9400, 5.7027, 5.8713, 5.9085, 5.7449, 5.6758, 5.7332, 5.7273, 5.7536, 5.7481, 5.8075, 5.8224, 5.7760, 5.8770, 5.8006, 5.7982, 5.8730, 5.8990, 5.8914, 5.9396, 5.8339, 5.8318, 5.9367, 5.8515, 5.8738, 6.0126, 5.8634, 5.8420, 5.8155, 5.8133, 5.8458, 5.8058, 5.9019, 5.7467, 5.8449, 5.8302, 5.8713, 5.8964, 5.7574, 5.9245, 5.8232, 5.8673, 5.8072, 5.9089, 5.8749, 5.8658, 5.9048, 5.9103, 5.8592, 5.8497, 5.9282, 5.9071, 5.9386, 5.9147, 5.8794, 6.0240, 5.9045, 5.9648, 5.9756, 5.9002, 5.8567, 5.9816, 5.9146, 5.9784, 6.0140, 5.9314, 5.9356, 7.0701, 5.9313, 5.8237, 5.9124, 5.8988, 5.9134, 5.9224, 5.9374, 6.0142, 5.9808, 5.9475, 6.0030, 6.0241, 5.8400, 5.9278, 5.9071, 5.8324, 5.9540, 5.9907, 5.9809, 5.9593, 5.9258, 5.9418, 6.0301, 5.9568, 5.8701, 5.9882, 5.9198, 5.9547, 5.9410, 5.9518, 5.9776, 5.9836, 6.0806, 6.0013, 5.8890, 5.9831, 6.0483, 6.1143, 5.9650, 5.9787, 5.9575, 6.0037, 5.9972, 6.0428, 6.0751, 6.0183, 6.0845, 6.0030, 6.0496, 6.0543, 6.0475, 6.0651, 6.0133, 6.0443, 6.0293, 6.0649, 6.0532, 6.0165, 6.0896, 6.0729, 6.0238, 6.1047, 6.0328, 6.1946, 6.0915, 6.1013, 6.1123, 6.0684, 6.0614, 6.0008, 6.1001, 6.0044, 6.0408, 6.0818, 6.0323, 6.1612, 6.1786, 6.0507, 6.1638, 6.0173, 6.0582, 6.0165, 6.0282, 6.0765, 6.2550, 5.9610, 6.0582, 5.9297, 6.0748, 6.0151, 6.0222, 6.0268, 6.0202, 6.0731, 6.0147, 6.0579, 6.0580, 6.0382, 5.9844, 6.0956, 6.1079, 6.1336, 6.0313, 6.1051, 6.1251, 6.0583, 6.1095, 6.0850, 6.0573, 6.1124, 6.0489, 6.1008, 6.0451, 6.1006, 6.1468, 6.1324, 6.1344, 6.1296, 6.1231, 6.1435, 6.1263, 6.1730, 6.1339, 6.1035]\n",
    "val_acc = [0.3525,0.3588,0.3416,0.3738,0.3538,0.3493,0.3747,0.3733,0.3561,0.3611,0.3683,0.3579,0.3624,0.3462,0.3484,0.3932,0.3769,0.3701,0.3665,0.3814,0.3941,0.3977,0.3869,0.3548,0.3566,0.3950,0.4041,0.4036,0.3995,0.3873,0.4086,0.4014,0.3706,0.4005,0.4032,0.3900,0.3955,0.4086,0.3860,0.4014,0.3900,0.3973,0.3937,0.4054,0.4086,0.3977,0.3796,0.3891,0.4122,0.3873,0.3950,0.4063,0.4077,0.3769,0.3819,0.3964,0.3751,0.3819,0.3552,0.4050,0.3864,0.3855,0.4045,0.4172,0.3502,0.3480,0.3828,0.3937,0.4122,0.3814,0.4109,0.3719,0.3887,0.3792,0.3801,0.3968,0.3977,0.4036,0.3805,0.3914,0.3756,0.3982,0.3955,0.4018,0.3919,0.3683,0.3968,0.4077,0.3846,0.3520,0.3941,0.3335,0.3846,0.3950,0.3710,0.3855,0.3738,0.3842,0.4118,0.4045,0.3579,0.4027,0.3783,0.3643,0.3910,0.3937,0.4045,0.3928,0.3733,0.3787,0.3778,0.3928,0.3873,0.3887,0.3905,0.3765,0.3923,0.3502,0.3548,0.3792,0.3991,0.3882,0.3968,0.3724,0.3905,0.3480,0.3810,0.3910,0.3882,0.3860,0.3701,0.3783,0.3805,0.3584,0.3900,0.3878,0.3833,0.3611,0.3267,0.2937,0.3873,0.3878,0.3742,0.3756,0.3792,0.3814,0.3778,0.3738,0.3584,0.3860,0.3647,0.3765,0.3738,0.3814,0.3643,0.3796,0.3928,0.3828,0.3887,0.3900,0.3828,0.3747,0.3719,0.3765,0.3701,0.3891,0.3828,0.3792,0.3570,0.3534,0.3855,0.3670,0.3584,0.3787,0.3810,0.3837,0.3724,0.3507,0.3620,0.3778,0.3647,0.3253,0.3742,0.3833,0.3701,0.3774,0.3538,0.3502,0.3851,0.3796,0.3810,0.3593,0.3810,0.3543,0.3719,0.3778,0.3919,0.3715,0.3810,0.3851,0.3670,0.3692,0.3769,0.3787,0.3760,0.3860,0.3398,0.3824,0.3683,0.3837,0.3751,0.3801,0.3774,0.3742,0.3747,0.3665,0.3769,0.3679,0.3923,0.3747,0.3706,0.3796,0.3783,0.3566,0.3538,0.3873,0.3715,0.3679,0.3710,0.3873,0.3792,0.3787,0.3534,0.3701,0.3597,0.3719,0.3710,0.3729,0.3566,0.3683,0.3801,0.3719,0.3647,0.3670,0.3652,0.3692,0.3724,0.3756,0.3629,0.3747,0.3570,0.3579,0.3593,0.3805,0.3729,0.3620,0.3724,0.3724,0.3679,0.3624,0.3620,0.3656,0.3643,0.3552,0.3570,0.3692,0.3733,0.3747,0.3661,0.3710,0.3769,0.3643,0.3643,0.3710,0.3796,0.3719,0.3643,0.3647,0.3656,0.3765,0.3692,0.3706,0.3579,0.3566,0.3670,0.3579,0.3643,0.3633,0.3597,0.3643,0.3593,0.3692,0.3624,0.3706,0.3633,0.3570,0.3656,0.3692,0.3588,0.3538,0.3633,0.3588,0.3575,0.3760,0.3525,0.3674,0.3652,0.3624,0.3633,0.3516,0.3566,0.2995,0.3606,0.3692,0.3692,0.3697,0.3629,0.3665,0.3520,0.3516,0.3335,0.3543,0.3557,0.3665,0.3620,0.3525,0.3434,0.3670,0.3670,0.3615,0.3697,0.3670,0.3624,0.3552,0.3629,0.3520,0.3489,0.3575,0.3579,0.3525,0.3548,0.3679,0.3543,0.3557,0.3579,0.3443,0.3620,0.3588,0.3611,0.3566,0.3602,0.3611,0.3638,0.3611,0.3602,0.3398,0.3602,0.3602,0.3538,0.3692,0.3597,0.3710,0.3670,0.3760,0.3548,0.3633,0.3593,0.3756,0.3557,0.3575,0.3647,0.3611,0.3611,0.3670,0.3647,0.3665,0.3647,0.3615,0.3330,0.3624,0.3498,0.3561,0.3620,0.3597,0.3620,0.3498,0.3602,0.3507,0.3624,0.3643,0.3606,0.3602,0.3593,0.3466,0.3638,0.3688,0.3611,0.3584,0.3588,0.3593,0.3498,0.3611,0.3647,0.3620,0.3575,0.3557,0.3701,0.3656,0.3624,0.3606,0.3570,0.3697,0.3647,0.3602,0.3529,0.3511,0.3697,0.3620,0.3557,0.3661,0.3606,0.3602,0.3579,0.3579,0.3570,0.3643,0.3579,0.3538,0.3602,0.3620,0.3674,0.3579,0.3502,0.3615,0.3656,0.3502,0.3529,0.3584,0.3638,0.3665,0.3511,0.3584,0.3638,0.3575,0.3588,0.3597,0.3620,0.3615,0.3579,0.3710,0.3643,0.3538,0.3593,0.3652,0.3611,0.3661,0.3579,0.3588,0.3575,0.3543,0.3385,0.3611,0.3570,0.3548,0.3538,0.3566,0.3443,0.3647,0.3520,0.3656,0.3525,0.3299,0.3452,0.3489,0.3552,0.3561,0.3534,0.3588,0.3552,0.3769,0.3602,0.3561,0.3597,0.3575,0.3557,0.3548,0.3620,0.3665,0.3602,0.3606,0.3561,0.3511,0.3466,0.3452,0.3434,0.3548,0.3548,0.3507,0.3597,0.3611,0.3403,0.3579,0.3493,0.3597,0.3611,0.3457,0.3561,0.3502,0.3538,0.3498,0.3525,0.3434,0.3602,0.3584,0.3597,0.3615,0.3633,0.3543,0.3570,0.3575,0.3593,0.3588,0.3529,0.3511,0.3633,0.3593,0.3511,0.3484,0.3579,0.3629,0.3489,0.3606,0.3552,0.3570,0.3656,0.3701,0.3615,0.3606,0.3448,0.3566,0.3697,0.3611,0.3575,0.3335,0.3679,0.3629,0.3593,0.3661,0.3561,0.3584,0.3566,0.3602,0.3633,0.3480,0.3611,0.3629,0.3615,0.3575,0.3593,0.3543,0.3597,0.3629,0.3643,0.3557,0.3597,0.3575,0.3588,0.3566,0.3561,0.3584,0.3566,0.3579,0.3466,0.3670,0.3665,0.3525,0.3679,0.3597,0.3552,0.3579,0.3629,0.3701,0.3484,0.3602,0.3593,0.3529,0.3575,0.3561,0.3620,0.3529,0.3534,0.3525,0.3620,0.3529,0.3584,0.3588,0.3602,0.3511,0.3507,0.3579,0.3493,0.3538,0.3561,0.3584,0.3548,0.3543,0.3647,0.3525,0.3606,0.3489,0.3593,0.3498,0.3570,0.3502,0.3475,0.3520,0.3520,0.3579,0.3561,0.3538,0.3566,0.3552,0.3588,0.3557,0.3629,0.3656,0.3525,0.3466,0.3579,0.3538,0.3511,0.3597,0.3620,0.3629,0.3543,0.3638,0.3633,0.3615,0.3548,0.3643,0.3529,0.3606,0.3502,0.3561,0.3416,0.3624,0.3579,0.3520,0.3665,0.3629,0.3566,0.3557,0.3584,0.3570,0.3624,0.3597,0.3566,0.3566,0.3552,0.3534,0.3570,0.3525,0.3462,0.3552,0.3538,0.3538,0.3466,0.3552,0.3421,0.3529,0.3507,0.3566,0.3502,0.3584,0.3561,0.3529,0.3353,0.3480,0.3575,0.3557,0.3484,0.3534,0.3462,0.3434,0.3566,0.3525,0.3489,0.3489,0.3489,0.3529,0.3516,0.3529,0.3552,0.3534,0.3511,0.3434,0.3516,0.3557,0.3570,0.3597,0.3534,0.3588,0.3457,0.3656,0.3606,0.3525,0.3507,0.3507,0.3502,0.3502,0.3511,0.3584,0.3538,0.3471,0.3584,0.3552,0.3566,0.3525,0.3566,0.3566,0.3511,0.3534,0.3561,0.3575,0.3538,0.3457,0.3620,0.3529,0.3493,0.3489,0.3543,0.3615,0.3552,0.3534,0.3471,0.3647,0.3548,0.3371,0.3575,0.3534,0.3543,0.3538,0.3493,0.3534,0.3502,0.3484,0.3502,0.3557,0.3538,0.3552,0.3538,0.3457,0.3570,0.3561,0.3534,0.3493,0.3593,0.3552,0.3570,0.3543,0.3394,0.3606,0.3624,0.3543,0.3579,0.3638,0.3543,0.3484,0.3493,0.3511,0.3557,0.3489,0.3570,0.3575,0.3597,0.3593,0.3552,0.3543,0.3543,0.3561,0.3579,0.3579,0.3529,0.3548,0.3606,0.3579,0.3579,0.3633,0.3588,0.3525,0.3584,0.3588,0.3593,0.3543,0.3529,0.3570,0.3471,0.3475,0.3566,0.3538,0.3516,0.3557,0.3534,0.3552,0.3489,0.3548,0.3543,0.3548,0.3448,0.3566,0.3570,0.3493,0.3480,0.3525,0.3480,0.3529,0.3493,0.3507,0.3561,0.3471,0.3425,0.3507,0.3339,0.3507,0.3561,0.3520,0.3543,0.3579,0.3588,0.3525,0.3511,0.3593,0.3538,0.3566,0.3538,0.3602,0.3606,0.3557,0.3516,0.3552,0.3516,0.3502,0.3557,0.3516,0.3561,0.3548,0.3588,0.3471,0.3529,0.3557,0.3552,0.3520,0.3529,0.3511,0.3471,0.3529,0.3471,0.3543,0.3516,0.3602,0.3566,0.3529]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0448cd636b09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss/Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_type' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.plot(train_acc)\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_acc)\n",
    "# plt.plot(val_loss)\n",
    "plt.legend(['Train acc','Train loss','Valid acc', 'Valid loss'], loc=2)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.title('Using '+ model_type)\n",
    "date = str(datetime.date.today() )\n",
    "time = str(datetime.datetime.now().time())[:-7]\n",
    "imgName = 'Images/' + model_type + '_' + date + '_' + time + '.jpg'\n",
    "plt.savefig( imgName, dpi= 200, bbox_inches='tight', transparent=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
