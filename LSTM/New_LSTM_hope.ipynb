{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train docs...\n",
      "pre-processing test docs...\n",
      "dictionary size:  13189\n",
      "converting to token ids...\n",
      "fitting LSTM ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:65: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:66: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 156060 samples, validate on 2210 samples\n",
      "Epoch 1/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.9982 - acc: 0.5977 - val_loss: 1.3463 - val_acc: 0.4222\n",
      "Epoch 2/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.8184 - acc: 0.6637 - val_loss: 1.3727 - val_acc: 0.4154\n",
      "Epoch 3/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.7676 - acc: 0.6802 - val_loss: 1.3988 - val_acc: 0.4330\n",
      "Epoch 4/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.7282 - acc: 0.6942 - val_loss: 1.4285 - val_acc: 0.4262\n",
      "Epoch 5/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.6961 - acc: 0.7059 - val_loss: 1.4628 - val_acc: 0.4140\n",
      "Epoch 6/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.6698 - acc: 0.7148 - val_loss: 1.4902 - val_acc: 0.4281\n",
      "Epoch 7/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.6482 - acc: 0.7222 - val_loss: 1.5716 - val_acc: 0.4235\n",
      "Epoch 8/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.6294 - acc: 0.7289 - val_loss: 1.5988 - val_acc: 0.4199\n",
      "Epoch 9/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.6138 - acc: 0.7344 - val_loss: 1.6202 - val_acc: 0.4226\n",
      "Epoch 10/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.5992 - acc: 0.7395 - val_loss: 1.6566 - val_acc: 0.4172\n",
      "Epoch 11/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.5870 - acc: 0.7439 - val_loss: 1.7075 - val_acc: 0.4222\n",
      "Epoch 12/50\n",
      "156060/156060 [==============================] - 47s - loss: 0.5742 - acc: 0.7475 - val_loss: 1.7569 - val_acc: 0.4235\n",
      "Epoch 13/50\n",
      "156060/156060 [==============================] - 45s - loss: 0.5630 - acc: 0.7517 - val_loss: 1.7849 - val_acc: 0.4113\n",
      "Epoch 14/50\n",
      "156060/156060 [==============================] - 44s - loss: 0.5532 - acc: 0.7557 - val_loss: 1.8112 - val_acc: 0.4172\n",
      "Epoch 15/50\n",
      "156060/156060 [==============================] - 45s - loss: 0.5433 - acc: 0.7574 - val_loss: 1.8494 - val_acc: 0.3959\n",
      "Epoch 16/50\n",
      "156060/156060 [==============================] - 45s - loss: 0.5342 - acc: 0.7603 - val_loss: 1.9098 - val_acc: 0.4072\n",
      "Epoch 17/50\n",
      "156060/156060 [==============================] - 45s - loss: 0.5258 - acc: 0.7636 - val_loss: 1.9030 - val_acc: 0.4063\n",
      "Epoch 18/50\n",
      "156060/156060 [==============================] - 52s - loss: 0.5182 - acc: 0.7666 - val_loss: 1.9590 - val_acc: 0.4045\n",
      "Epoch 19/50\n",
      "156060/156060 [==============================] - 50s - loss: 0.5107 - acc: 0.7691 - val_loss: 1.9819 - val_acc: 0.4000\n",
      "Epoch 20/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.5031 - acc: 0.7714 - val_loss: 1.9882 - val_acc: 0.3891\n",
      "Epoch 21/50\n",
      "156060/156060 [==============================] - 51s - loss: 0.4969 - acc: 0.7737 - val_loss: 2.0488 - val_acc: 0.3941\n",
      "Epoch 22/50\n",
      "156060/156060 [==============================] - 48s - loss: 0.4898 - acc: 0.7759 - val_loss: 2.0730 - val_acc: 0.3995\n",
      "Epoch 23/50\n",
      "156060/156060 [==============================] - 48s - loss: 0.4836 - acc: 0.7787 - val_loss: 2.1459 - val_acc: 0.3937\n",
      "Epoch 24/50\n",
      "156060/156060 [==============================] - 49s - loss: 0.4779 - acc: 0.7795 - val_loss: 2.1615 - val_acc: 0.3964\n",
      "Epoch 25/50\n",
      "156060/156060 [==============================] - 51s - loss: 0.4727 - acc: 0.7824 - val_loss: 2.1597 - val_acc: 0.3955\n",
      "Epoch 26/50\n",
      "  3328/156060 [..............................] - ETA: 48s - loss: 0.4308 - acc: 0.8017"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a91baf974ba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m           verbose=1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('../Datasets/SST1_dataset/Processed_SST1.tsv', sep='\\t')\n",
    "train_df = pd.read_csv('/home/shikhar/Datasets/Kaggle_dataset/train.tsv', sep='\\t', header=0)\n",
    "test_df = pd.read_csv('/home/shikhar/Datasets/Kaggle_dataset/test.tsv', sep='\\t', header=0)\n",
    "\n",
    "raw_docs_train = train_df['Phrase'].values\n",
    "# raw_docs_test = test_df['Phrase'].values\n",
    "raw_docs_test = df[df.split_ind == 2]['Phrases'].values\n",
    "sentiment_train = train_df['Sentiment'].values\n",
    "sentiment_test = df[df.split_ind == 2]['Label'].values\n",
    "num_labels = len(np.unique(sentiment_train))\n",
    "\n",
    "#text pre-processing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "print (\"pre-processing train docs...\")\n",
    "processed_docs_train = []\n",
    "for doc in raw_docs_train:\n",
    "   tokens = word_tokenize(doc)\n",
    "   filtered = [word for word in tokens if word not in stop_words]\n",
    "   stemmed = [stemmer.stem(word) for word in filtered]\n",
    "   processed_docs_train.append(stemmed)\n",
    "\n",
    "print (\"pre-processing test docs...\")\n",
    "processed_docs_test = []\n",
    "for doc in raw_docs_test:\n",
    "   tokens = word_tokenize(doc)\n",
    "   filtered = [word for word in tokens if word not in stop_words]\n",
    "   stemmed = [stemmer.stem(word) for word in filtered]\n",
    "   processed_docs_test.append(stemmed)\n",
    "\n",
    "processed_docs_all = np.concatenate((processed_docs_train, processed_docs_test), axis=0)\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_docs_all)\n",
    "dictionary_size = len(dictionary.keys())\n",
    "print (\"dictionary size: \", dictionary_size )\n",
    "#dictionary.save('dictionary.dict')\n",
    "#corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print (\"converting to token ids...\")\n",
    "word_id_train, word_id_len = [], []\n",
    "for doc in processed_docs_train:\n",
    "    word_ids = [dictionary.token2id[word] for word in doc]\n",
    "    word_id_train.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))\n",
    "\n",
    "word_id_test, word_ids = [], []\n",
    "for doc in processed_docs_test:\n",
    "    word_ids = [dictionary.token2id[word] for word in doc]\n",
    "    word_id_test.append(word_ids)\n",
    "    word_id_len.append(len(word_ids))\n",
    "\n",
    "seq_len = np.round((np.mean(word_id_len) + 2*np.std(word_id_len))).astype(int)\n",
    "\n",
    "#pad sequences\n",
    "word_id_train = sequence.pad_sequences(np.array(word_id_train), maxlen=seq_len)\n",
    "word_id_test = sequence.pad_sequences(np.array(word_id_test), maxlen=seq_len)\n",
    "y_train_enc = np_utils.to_categorical(sentiment_train, num_labels)\n",
    "y_test_enc = np_utils.to_categorical(sentiment_test, num_labels)\n",
    "\n",
    "#LSTM\n",
    "print (\"fitting LSTM ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(dictionary_size, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(word_id_train, y_train_enc, epochs=50,\n",
    "          validation_data=(word_id_test, y_test_enc),\n",
    "          batch_size=256, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2208/2210 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_classes(word_id_test)\n",
    "\n",
    "#make a submission\n",
    "# df[]['Pred'] = test_pred.reshape(-1,1) \n",
    "# header = ['PhraseId', 'Sentiment']\n",
    "# df.to_csv('./lstm_sentiment.csv', columns=header, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
