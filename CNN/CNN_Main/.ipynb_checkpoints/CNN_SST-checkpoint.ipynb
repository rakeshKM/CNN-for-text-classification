{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232 14058\n"
     ]
    }
   ],
   "source": [
    "# Read dictionary\n",
    "word_to_ind = dict()\n",
    "\n",
    "with open('../../Datasets/SST1_dataset/dictionary.txt') as f:\n",
    "    for line in f:\n",
    "        entry = line.split('|')\n",
    "        word_to_ind[entry[0]] = int(entry[1])\n",
    "\n",
    "print(len(word_to_ind), word_to_ind['Good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239232 0.66667\n"
     ]
    }
   ],
   "source": [
    "# Loading Sentiment labels\n",
    "ind_to_senti = dict()\n",
    "\n",
    "with open('../../Datasets/SST1_dataset/sentiment_labels.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = line.split('|')\n",
    "        ind_to_senti[int(entry[0])] = float(entry[1])\n",
    "\n",
    "print(len(ind_to_senti), ind_to_senti[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n"
     ]
    }
   ],
   "source": [
    "# Loading data sentences\n",
    "sentence_list = []\n",
    "\n",
    "with open('../../Datasets/SST1_dataset/datasetSentences.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = line.split('\\t')\n",
    "        sentence_list.append(entry[1])\n",
    "        \n",
    "print(len(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11855\n"
     ]
    }
   ],
   "source": [
    "# Loading data split\n",
    "split_ind = []\n",
    "with open('../../Datasets/SST1_dataset/datasetSplit.txt') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        entry = line.split(',')\n",
    "        split_ind.append(int(entry[1]))\n",
    "\n",
    "print(len(split_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_su_sentiment_rotten_tomatoes(dirname, lowercase=True):\n",
    "    \"\"\"\n",
    "    Read and return documents from the Stanford Sentiment Treebank \n",
    "    corpus (Rotten Tomatoes reviews), from http://nlp.Stanford.edu/sentiment/\n",
    "    Initialize the corpus from a given directory, where\n",
    "    http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
    "    has been expanded. It's not too big, so compose entirely into memory.\n",
    "    \"\"\"\n",
    "    logging.info(\"loading corpus from %s\" % dirname)\n",
    "\n",
    "    # many mangled chars in sentences (datasetSentences.txt)\n",
    "    chars_sst_mangled = ['à', 'á', 'â', 'ã', 'æ', 'ç', 'è', 'é', 'í',\n",
    "                         'í', 'ï', 'ñ', 'ó', 'ô', 'ö', 'û', 'ü']\n",
    "    sentence_fixups = [(char.encode('utf-8').decode('latin1'), char) for char in chars_sst_mangled]\n",
    "    # more junk, and the replace necessary for sentence-phrase consistency\n",
    "    sentence_fixups.extend([\n",
    "        ('Â', ''),\n",
    "        ('\\xa0', ' '),\n",
    "        ('-LRB-', '('),\n",
    "        ('-RRB-', ')'),\n",
    "    ])\n",
    "    # only this junk in phrases (dictionary.txt)\n",
    "    phrase_fixups = [('\\xa0', ' ')]\n",
    "\n",
    "    # sentence_id and split are only positive for the full sentences\n",
    "\n",
    "    # read sentences to temp {sentence -> (id,split) dict, to correlate with dictionary.txt\n",
    "    info_by_sentence = {}\n",
    "    with open(os.path.join(dirname, 'datasetSentences.txt'), 'r') as sentences:\n",
    "        with open(os.path.join(dirname, 'datasetSplit.txt'), 'r') as splits:\n",
    "            next(sentences)  # legend\n",
    "            next(splits)     # legend\n",
    "            for sentence_line, split_line in izip(sentences, splits):\n",
    "                (id, text) = sentence_line.split('\\t')\n",
    "                id = int(id)\n",
    "                text = text.rstrip()\n",
    "                for junk, fix in sentence_fixups:\n",
    "                    text = text.replace(junk, fix)\n",
    "                (id2, split_i) = split_line.split(',')\n",
    "                assert id == int(id2)\n",
    "                if text not in info_by_sentence:    # discard duplicates\n",
    "                    info_by_sentence[text] = (id, int(split_i))\n",
    "\n",
    "    # read all phrase text\n",
    "    phrases = [None] * 239232  # known size of phrases\n",
    "    with open(os.path.join(dirname, 'dictionary.txt'), 'r') as phrase_lines:\n",
    "        for line in phrase_lines:\n",
    "            (text, id) = line.split('|')\n",
    "            for junk, fix in phrase_fixups:\n",
    "                text = text.replace(junk, fix)\n",
    "            phrases[int(id)] = text.rstrip()  # for 1st pass just string\n",
    "\n",
    "    SentimentPhrase = namedtuple('SentimentPhrase', SentimentDocument._fields + ('sentence_id',))\n",
    "    # add sentiment labels, correlate with sentences\n",
    "    with open(os.path.join(dirname, 'sentiment_labels.txt'), 'r') as sentiments:\n",
    "        next(sentiments)  # legend\n",
    "        for line in sentiments:\n",
    "            (id, sentiment) = line.split('|')\n",
    "            id = int(id)\n",
    "            sentiment = float(sentiment)\n",
    "            text = phrases[id]\n",
    "            words = text.split()\n",
    "            if lowercase:\n",
    "                words = [word.lower() for word in words]\n",
    "            (sentence_id, split_i) = info_by_sentence.get(text, (None, 0))\n",
    "            split = [None,'train','test','dev'][split_i]\n",
    "            phrases[id] = SentimentPhrase(words, [id], split, sentiment, sentence_id)\n",
    "\n",
    "    assert len([phrase for phrase in phrases if phrase.sentence_id is not None]) == len(info_by_sentence)  # all\n",
    "    # counts don't match 8544, 2210, 1101 because 13 TRAIN and 1 DEV sentences are duplicates\n",
    "    assert len([phrase for phrase in phrases if phrase.split == 'train']) == 8531  # 'train'\n",
    "    assert len([phrase for phrase in phrases if phrase.split == 'test']) == 2210  # 'test'\n",
    "    assert len([phrase for phrase in phrases if phrase.split == 'dev']) == 1100  # 'dev'\n",
    "\n",
    "    logging.info(\"loaded corpus with %i sentences and %i phrases from %s\"\n",
    "                % (len(info_by_sentence), len(phrases), dirname))\n",
    "\n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = read_su_sentiment_rotten_tomatoes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
