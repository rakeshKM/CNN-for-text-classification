{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Embedding, regularizers\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from w2v import train_word2vec \n",
    "from keras.utils import np_utils\n",
    "import pickle, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "import tensorflow as tf\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "from w2v import train_word2vec \n",
    "import pickle, datetime\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Datasets/SST1_dataset/Processed_SST1.tsv', sep='\\t')\n",
    "\n",
    "raw_docs_train      = df[df.split_ind == 1]['Phrases'].values\n",
    "sentiment_train     = df[df.split_ind == 1]['Label'].values\n",
    "raw_docs_test       = df[df.split_ind == 2]['Phrases'].values\n",
    "sentiment_test      = df[df.split_ind == 2]['Label'].values\n",
    "num_labels          = len(np.unique(sentiment_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9645, 37) (9645, 5) (2210, 37) (2210, 5)\n"
     ]
    }
   ],
   "source": [
    "fname = '../../Datasets/SST1_dataset/sst_data'\n",
    "with open(fname, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "x_train, y_train, x_test, y_test, dictionary = data\n",
    "seq_len = dictionary.seq_len\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_model(fname):\n",
    "    model = keras.models.model_from_json(open(fname + '.json').read())\n",
    "    model.load_weights(fname + '_weights.h5')\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model, fname):    \n",
    "    json_string = model.to_json()\n",
    "    open(fname + '.json', 'w').write(json_string)\n",
    "    model.save_weights(fname + '_weights.h5', overwrite=True)\n",
    "    with open( fname + '_history', 'wb') as output:\n",
    "        pickle.dump([model.history.history], output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = 'dodo_cnn_non_static5'\n",
    "model = load_model(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.64%\n",
      "Test Accuracy: 42.08%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Occlusion Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_ind = 11991\n",
    "\n",
    "def occlude_sent(sent, ind):\n",
    "    while ind < seq_len-1:\n",
    "        sent[0,ind] = sent[0,ind+1]\n",
    "        ind += 1 \n",
    "    sent[0,seq_len-1] = pad_ind\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the story and structure are well honed\n",
      "\n",
      " [[   29    65    31   986   182   242  1938 11991 11991 11991 11991 11991\n",
      "  11991 11991 11991 11991 11991 11991 11991 11991 11991 11991 11991 11991\n",
      "  11991 11991 11991 11991 11991 11991 11991 11991 11991 11991 11991 11991\n",
      "  11991]] \n",
      "\n",
      " [[ 0.  0.  0.  1.  0.]]\n",
      "(1, 37) (1, 5)\n"
     ]
    }
   ],
   "source": [
    "sent_id = 333\n",
    "print(raw_docs_train[sent_id])\n",
    "\n",
    "x_inp = x_train[sent_id, :]\n",
    "x_inp = x_inp.reshape((1,seq_len))\n",
    "\n",
    "y_inp = y_train[sent_id, :]\n",
    "y_inp = y_inp.reshape((1,num_labels))\n",
    "\n",
    "print('\\n', x_inp, '\\n\\n', y_inp)\n",
    "print(x_inp.shape, y_inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Prediction: [[ 0.0282236   0.15557611  0.3540647   0.38555646  0.07657916]] \n",
      "\n",
      "the [3]\n",
      "[[ 0.08516022  0.16722625  0.19886796  0.44121018  0.10753546]]\n",
      "stori [3]\n",
      "[[ 0.08998668  0.17700012  0.19637123  0.42389011  0.11275184]]\n",
      "and [3]\n",
      "[[ 0.05646797  0.21363001  0.26372549  0.40625682  0.05991968]]\n",
      "structur [3]\n",
      "[[ 0.06134174  0.23766007  0.27704486  0.30417785  0.11977552]]\n",
      "are [3]\n",
      "[[ 0.03414585  0.16855714  0.31971237  0.37483883  0.10274588]]\n",
      "well [1]\n",
      "[[ 0.06075473  0.31963122  0.30222267  0.2577638   0.05962766]]\n",
      "hone [3]\n",
      "[[ 0.07638688  0.19144639  0.29416445  0.34355244  0.0944498 ]]\n"
     ]
    }
   ],
   "source": [
    "print('Original Prediction:', model.predict(sent), '\\n')\n",
    "\n",
    "for ind in range(0,seq_len):\n",
    "    if(x_inp[0,ind] == pad_ind):\n",
    "        break\n",
    "    \n",
    "    sent = x_inp.copy()\n",
    "    sent = occlude_sent(sent, ind)\n",
    "    pred = model.predict(sent)\n",
    "    \n",
    "    print(dictionary.id2token[x_inp[0,ind]], np.argmax(model.predict(sent), axis=1))\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sent_id in range(x_train.shape[0]):\n",
    "    x_inp = x_train[sent_id, :]\n",
    "    x_inp = x_inp.reshape((1,seq_len))\n",
    "\n",
    "    y_inp = y_train[sent_id, :]\n",
    "    y_inp = y_inp.reshape((1,num_labels))\n",
    "    \n",
    "    y_actual = np.argmax(y_inp)\n",
    "                         \n",
    "    for ind in range(0,seq_len):\n",
    "        if(x_inp[0,ind] == pad_ind):\n",
    "            break\n",
    "\n",
    "        sent = x_inp.copy()\n",
    "        sent = occlude_sent(sent, ind)\n",
    "        pred = model.predict(sent)\n",
    "                         \n",
    "        y_pred = np.argmax(model.predict(sent), axis=1)\n",
    "        \n",
    "        if(y_actual != y_pred):\n",
    "            print(sent_id, dictionary.id2token[x_inp[0,ind]], '|||', raw_docs_train[sent_id])\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1 (None, 37) (None, 37)\n",
      "****************************************\n",
      "1 embedding (None, 37) (None, 37, 300)\n",
      "(11992, 300)\n",
      "****************************************\n",
      "2 dropout_1 (None, 37, 300) (None, 37, 300)\n",
      "****************************************\n",
      "3 conv1d_1 (None, 37, 300) (None, 35, 100)\n",
      "(3, 300, 100)\n",
      "(100,)\n",
      "****************************************\n",
      "4 conv1d_2 (None, 37, 300) (None, 34, 100)\n",
      "(4, 300, 100)\n",
      "(100,)\n",
      "****************************************\n",
      "5 conv1d_3 (None, 37, 300) (None, 33, 100)\n",
      "(5, 300, 100)\n",
      "(100,)\n",
      "****************************************\n",
      "6 max_pooling1d_1 (None, 35, 100) (None, 1, 100)\n",
      "****************************************\n",
      "7 max_pooling1d_2 (None, 34, 100) (None, 1, 100)\n",
      "****************************************\n",
      "8 max_pooling1d_3 (None, 33, 100) (None, 1, 100)\n",
      "****************************************\n",
      "9 flatten_1 (None, 1, 100) (None, 100)\n",
      "****************************************\n",
      "10 flatten_2 (None, 1, 100) (None, 100)\n",
      "****************************************\n",
      "11 flatten_3 (None, 1, 100) (None, 100)\n",
      "****************************************\n",
      "12 concatenate_1 [(None, 100), (None, 100), (None, 100)] (None, 300)\n",
      "****************************************\n",
      "13 dropout_2 (None, 300) (None, 300)\n",
      "****************************************\n",
      "14 dense_1 (None, 300) (None, 100)\n",
      "(300, 100)\n",
      "(100,)\n",
      "****************************************\n",
      "15 dense_2 (None, 100) (None, 5)\n",
      "(100, 5)\n",
      "(5,)\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for layer in model.layers:\n",
    "    print(count, layer.name, layer.input_shape, layer.output_shape)\n",
    "    count += 1\n",
    "    wts = layer.get_weights()\n",
    "    for wt in wts:\n",
    "        print(wt.shape)\n",
    "    print('****************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire model in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ftr_sz = [3,4,5]\n",
    "\n",
    "inp = tf.placeholder(tf.int32, shape=(None, seq_len))\n",
    "lbl = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "\n",
    "wts = model.get_layer('embedding').get_weights()\n",
    "wEmb = tf.constant(wts[0])\n",
    "\n",
    "emb = tf.nn.embedding_lookup(wEmb, inp)\n",
    "\n",
    "# First convolution layer\n",
    "wts = model.get_layer('conv1d_1').get_weights()\n",
    "wConv1 = tf.constant(wts[0])\n",
    "bConv1  = tf.constant(wts[1])\n",
    "\n",
    "conv1 = tf.nn.conv1d(emb, wConv1, stride = 1, padding='VALID')\n",
    "bias1 = conv1 + bConv1\n",
    "relu1 = tf.nn.relu(bias1)\n",
    "\n",
    "pool1 = tf.nn.max_pool([relu1], ksize = [1, 1, seq_len - ftr_sz[0] + 1, 1],\n",
    "                       strides = [1,1,1,1], padding = 'VALID')\n",
    "\n",
    "# Second convolution layer\n",
    "wts = model.get_layer('conv1d_2').get_weights()\n",
    "wConv2 = tf.constant(wts[0])\n",
    "bConv2  = tf.constant(wts[1])\n",
    "\n",
    "conv2 = tf.nn.conv1d(emb, wConv2, stride = 1, padding='VALID')\n",
    "bias2 = conv2 + bConv2\n",
    "relu2 = tf.nn.relu(bias2)\n",
    "\n",
    "pool2 = tf.nn.max_pool([relu2], ksize = [1, 1, seq_len - ftr_sz[1] + 1, 1],\n",
    "                       strides = [1,1,1,1], padding = 'VALID')\n",
    "\n",
    "# Third convolution layer\n",
    "wts = model.get_layer('conv1d_3').get_weights()\n",
    "wConv3 = tf.constant(wts[0])\n",
    "bConv3  = tf.constant(wts[1])\n",
    "\n",
    "conv3 = tf.nn.conv1d(emb, wConv3, stride = 1, padding='VALID')\n",
    "bias3 = conv3 + bConv3\n",
    "relu3 = tf.nn.relu(bias3)\n",
    "\n",
    "pool3 = tf.nn.max_pool([relu3], ksize = [1, 1, seq_len - ftr_sz[2] + 1, 1],\n",
    "                       strides = [1,1,1,1], padding = 'VALID')\n",
    "\n",
    "\n",
    "flat = tf.concat([pool1[0,:,0,:], pool2[0,:,0,:], pool3[0,:,0,:]], axis = 1)\n",
    "\n",
    "wts = model.get_layer('dense_1').get_weights()\n",
    "wDen1 = tf.constant(wts[0])\n",
    "bDen1 = tf.constant(np.reshape(wts[1], (1, wts[1].shape[0],)))\n",
    "\n",
    "den1 = tf.matmul(flat, wDen1)\n",
    "den1f = tf.add(bDen1, den1)\n",
    "den1f = tf.nn.relu(den1f)\n",
    "\n",
    "wts = model.get_layer('dense_2').get_weights()\n",
    "wDen2 = tf.constant(wts[0])\n",
    "bDen2 = tf.constant(np.reshape(wts[1], (1, wts[1].shape[0],)))\n",
    "\n",
    "den2 = tf.matmul(den1f, wDen2)\n",
    "den2f = tf.add(bDen2, den2)\n",
    "\n",
    "final = tf.nn.softmax_cross_entropy_with_logits(labels=den2f, logits=lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69845384, -0.02364065,  0.14965281,  0.94653302, -0.4651677 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    res = den2f.eval(feed_dict={inp:x_inp, lbl:y_inp})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool_arg(inp):\n",
    "    ch, r, c = inp.shape\n",
    "    out_shape = (ch, c)\n",
    "    max_args = np.empty(out_shape, np.int32)\n",
    "\n",
    "    for k in range(ch):\n",
    "        for i in range(c):\n",
    "            max_args[k, i] = np.argmax(inp[k,:,i])\n",
    "    return max_args\n",
    "\n",
    "def unpool(ind_mat, val_mat, out_shape):\n",
    "    out_mat = np.zeros(out_shape, np.float32)\n",
    "    b, r,c,ch = out_shape\n",
    "        \n",
    "    for k in range(ch):\n",
    "        c1 = 0\n",
    "        for i in range(0, r-2, 2):\n",
    "            c2 = 0\n",
    "            for j in range(0, c-2, 2):\n",
    "                ind = ind_mat[0, c1, c2, k]\n",
    "                val = val_mat[0, c1, c2, k]\n",
    "                coord = np.unravel_index(ind, (3,3))                \n",
    "                out_mat[0, i + coord[0] , j + coord[1],k] = val\n",
    "                \n",
    "                c2 += 1\n",
    "            c1 +=1\n",
    "            \n",
    "    return out_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 's fun lite\n",
      "\n",
      " [[  134     7   149  2274 11991 11991 11991 11991 11991 11991 11991 11991\n",
      "  11991 11991 11991 11991 11991 11991 11991 11991 11991 11991 11991 11991\n",
      "  11991 11991 11991 11991 11991 11991 11991 11991 11991 11991 11991 11991\n",
      "  11991]] \n",
      "\n",
      " [[ 0.  0.  0.  1.  0.]]\n",
      "(1, 37) (1, 5)\n"
     ]
    }
   ],
   "source": [
    "sent_id = 439\n",
    "print(raw_docs_train[sent_id])\n",
    "\n",
    "x_inp = x_train[sent_id, :]\n",
    "x_inp = x_inp.reshape((1,seq_len))\n",
    "\n",
    "y_inp = y_train[sent_id, :]\n",
    "y_inp = y_inp.reshape((1,num_labels))\n",
    "\n",
    "print('\\n', x_inp, '\\n\\n', y_inp)\n",
    "print(x_inp.shape, y_inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300) (100,) (100,) (100,) (100,) (100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "flat_res_fwd = flat.eval(feed_dict={inp:x_inp, lbl:y_inp})\n",
    "\n",
    "pool1_ind = maxpool_arg(relu1.eval(feed_dict={inp:x_inp, lbl:y_inp}))[0,:]\n",
    "pool2_ind = maxpool_arg(relu2.eval(feed_dict={inp:x_inp, lbl:y_inp}))[0,:]\n",
    "pool3_ind = maxpool_arg(relu3.eval(feed_dict={inp:x_inp, lbl:y_inp}))[0,:]\n",
    "\n",
    "pool1_res_fwd = pool1.eval(feed_dict={inp:x_inp, lbl:y_inp})[0,0,0,:]\n",
    "pool2_res_fwd = pool2.eval(feed_dict={inp:x_inp, lbl:y_inp})[0,0,0,:]\n",
    "pool3_res_fwd = pool3.eval(feed_dict={inp:x_inp, lbl:y_inp})[0,0,0,:]\n",
    "\n",
    "print(flat_res_fwd.shape, pool1_ind.shape, pool2_ind.shape, pool3_ind.shape, \n",
    "      pool1_res_fwd.shape, pool2_res_fwd.shape, pool3_res_fwd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "y_inp = np.float32(np.reshape(y_test[0,:], (1,5), ))\n",
    "den2_out = y_inp - bDen2\n",
    "den1_out = tf.matmul(den2_out, wDen2, transpose_b=True)\n",
    "\n",
    "den1_out = tf.nn.relu(den1_out)\n",
    "den1_out = den1_out - bDen1\n",
    "flat_out = tf.matmul(den1_out, wDen1, transpose_b=True)\n",
    "\n",
    "flat_res_bwd = flat_out.eval(feed_dict={inp:x_inp, lbl:y_inp})\n",
    "\n",
    "pool1_res_bwd = flat_res_bwd[0,0:100]\n",
    "pool2_res_bwd = flat_res_bwd[0,100:200]\n",
    "pool3_res_bwd = flat_res_bwd[0,200:300]\n",
    "\n",
    "print(pool1_res_bwd.shape, pool2_res_bwd.shape, pool3_res_bwd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_imp(t1,t2,t3):\n",
    "    tsum = np.sum(t1) + np.sum(t2) + np.sum(t3)\n",
    "    contri = np.zeros((seq_len,), np.float32)\n",
    "\n",
    "    for i in range(100):\n",
    "        for j in range(3):\n",
    "            contri[pool1_ind[i] + j] += 1/3*t1[i]/tsum\n",
    "\n",
    "    for i in range(100):\n",
    "        for j in range(4):\n",
    "            contri[pool2_ind[i] + j] += 1/4*t2[i]/tsum\n",
    "\n",
    "    for i in range(100):\n",
    "        for j in range(5):\n",
    "            contri[pool3_ind[i] + j] += 1/5*t3[i]/tsum\n",
    "\n",
    "    wrds = []\n",
    "    sent = raw_docs_train[sent_id]\n",
    "    wrd_prob = dict()\n",
    "\n",
    "    counter = 0\n",
    "    for i in x_inp[0,:]:\n",
    "        wrd_prob[dictionary.id2token[i]] = contri[counter]\n",
    "        counter += 1\n",
    "\n",
    "    import operator\n",
    "    # srt_list = sorted(wrd_prob.items())\n",
    "    sorted_x = sorted(wrd_prob.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lite', 0.30395886),\n",
       " ('fun', 0.26758641),\n",
       " (\"'s\", 0.16971132),\n",
       " ('it', 0.094893411),\n",
       " ('<PAD/>', 0.0)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = pool1_res_fwd * pool1_res_bwd\n",
    "t2 = pool2_res_fwd * pool2_res_bwd\n",
    "t3 = pool3_res_fwd * pool3_res_bwd\n",
    "\n",
    "calc_imp(t1,t2,t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fun', 0.29317465),\n",
       " ('lite', 0.27859962),\n",
       " (\"'s\", 0.1769509),\n",
       " ('it', 0.067789622),\n",
       " ('<PAD/>', 0.0)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = pool1_res_fwd * np.abs(pool1_res_bwd)\n",
    "t2 = pool2_res_fwd * np.abs(pool2_res_bwd)\n",
    "t3 = pool3_res_fwd * np.abs(pool3_res_bwd)\n",
    "calc_imp(t1,t2,t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconvnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fun', 0.25750187),\n",
       " (\"'s\", 0.250965),\n",
       " ('it', 0.24174465),\n",
       " ('lite', 0.16361547),\n",
       " ('<PAD/>', 0.0)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = np.abs(pool1_res_bwd)\n",
    "t2 = np.abs(pool2_res_bwd)\n",
    "t3 = np.abs(pool3_res_bwd)\n",
    "calc_imp(t1,t2,t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
