{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Embedding, regularizers\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from w2v import train_word2vec \n",
    "from keras.utils import np_utils\n",
    "import pickle, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "import tensorflow as tf\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "from w2v import train_word2vec \n",
    "import pickle, datetime\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Datasets/SST1_dataset/Processed_SST1.tsv', sep='\\t')\n",
    "\n",
    "raw_docs_train      = df[df.split_ind == 1]['Phrases'].values\n",
    "sentiment_train     = df[df.split_ind == 1]['Label'].values\n",
    "raw_docs_test       = df[df.split_ind == 2]['Phrases'].values\n",
    "sentiment_test      = df[df.split_ind == 2]['Label'].values\n",
    "num_labels          = len(np.unique(sentiment_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9645, 37) (9645, 5) (2210, 37) (2210, 5)\n"
     ]
    }
   ],
   "source": [
    "fname = '../../Datasets/SST1_dataset/sst_data'\n",
    "with open(fname, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "x_train, y_train, x_test, y_test, dictionary = data\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(fname):\n",
    "    model = keras.models.model_from_json(open(fname + '.json').read())\n",
    "    model.load_weights(fname + '_weights.h5')\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '/home/shikhar/Datasets/Models/dodo_cnn_non_static2'\n",
    "model = load_model(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 96.02%\n",
      "Test Accuracy: 40.95%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1 = x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = np.argmax(test_pred, axis=1)\n",
    "y_actual = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.409502262443\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(test_pred == y_actual)*1.0 / y_actual.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crct_pred = (test_pred == y_actual)\n",
    "ind = []\n",
    "for i in range(crct_pred.shape[0]):\n",
    "    if crct_pred[i] == 1:\n",
    "        ind.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4, ..., 4, 3, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_id_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1924d46a939c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_actual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_id_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0morg_pred\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_id_test' is not defined"
     ]
    }
   ],
   "source": [
    "k = -1\n",
    "print(y_actual[ind[k]])\n",
    "vec = word_id_test[ind[k]]\n",
    "\n",
    "org_pred =  model.predict(x_test[ind[k]:ind[k]+1, :])\n",
    "print('Original Prediction', org_pred)\n",
    "\n",
    "for ele in list(set(vec)):\n",
    "    vec_temp = [i for i in vec if i != ele]    \n",
    "    x_test1 = sequence.pad_sequences([vec_temp], maxlen=seq_len, padding=\"post\", truncating=\"post\")\n",
    "    pred = model.predict(x_test1)\n",
    "    print(dictionary.id2token[ele])\n",
    "    print(pred)\n",
    "#     print(dictionary.id2token[ele], '\\t', pred )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1 (None, 37) (None, 37)\n",
      "****************************************\n",
      "1 embedding (None, 37) (None, 37, 300)\n",
      "(11992, 300)\n",
      "****************************************\n",
      "2 dropout_1 (None, 37, 300) (None, 37, 300)\n",
      "****************************************\n",
      "3 conv1d_1 (None, 37, 300) (None, 35, 100)\n",
      "(3, 300, 100)\n",
      "(100,)\n",
      "****************************************\n",
      "4 conv1d_2 (None, 37, 300) (None, 34, 100)\n",
      "(4, 300, 100)\n",
      "(100,)\n",
      "****************************************\n",
      "5 conv1d_3 (None, 37, 300) (None, 33, 100)\n",
      "(5, 300, 100)\n",
      "(100,)\n",
      "****************************************\n",
      "6 max_pooling1d_1 (None, 35, 100) (None, 1, 100)\n",
      "****************************************\n",
      "7 max_pooling1d_2 (None, 34, 100) (None, 1, 100)\n",
      "****************************************\n",
      "8 max_pooling1d_3 (None, 33, 100) (None, 1, 100)\n",
      "****************************************\n",
      "9 flatten_1 (None, 1, 100) (None, 100)\n",
      "****************************************\n",
      "10 flatten_2 (None, 1, 100) (None, 100)\n",
      "****************************************\n",
      "11 flatten_3 (None, 1, 100) (None, 100)\n",
      "****************************************\n",
      "12 concatenate_1 [(None, 100), (None, 100), (None, 100)] (None, 300)\n",
      "****************************************\n",
      "13 dropout_2 (None, 300) (None, 300)\n",
      "****************************************\n",
      "14 dense_1 (None, 300) (None, 100)\n",
      "(300, 100)\n",
      "(100,)\n",
      "****************************************\n",
      "15 dense_2 (None, 100) (None, 5)\n",
      "(100, 5)\n",
      "(5,)\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for layer in model.layers:\n",
    "    print(count, layer.name, layer.input_shape, layer.output_shape)\n",
    "    count += 1\n",
    "    wts = layer.get_weights()\n",
    "    for wt in wts:\n",
    "        print(wt.shape)\n",
    "    print('****************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = dictionary.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_7:0' shape=(1, 35, 100) dtype=float32>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_len - 3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_sz = [3,4,5]\n",
    "\n",
    "inp = tf.constant(x_test[0:1,])\n",
    "lbl = tf.constant(y_test[0:1,])\n",
    "\n",
    "wts = model.get_layer('embedding').get_weights()\n",
    "wEmb = tf.constant(wts[0])\n",
    "\n",
    "emb = tf.nn.embedding_lookup(wEmb, inp)\n",
    "\n",
    "# First convolution layer\n",
    "wts = model.get_layer('conv1d_1').get_weights()\n",
    "wConv1 = tf.constant(wts[0])\n",
    "bConv1  = tf.constant(wts[1])\n",
    "\n",
    "conv1 = tf.nn.conv1d(emb, wConv1, stride = 1, padding='VALID')\n",
    "bias1 = conv1 + bConv1\n",
    "relu1 = tf.nn.relu(bias1)\n",
    "\n",
    "pool1 = tf.nn.max_pool([relu1], ksize = [1, 1, max_sent_len - ftr_sz[0] + 1, 1],\n",
    "                       strides = [1,1,1,1], padding = 'VALID')\n",
    "\n",
    "# Second convolution layer\n",
    "wts = model.get_layer('conv1d_2').get_weights()\n",
    "wConv2 = tf.constant(wts[0])\n",
    "bConv2  = tf.constant(wts[1])\n",
    "\n",
    "conv2 = tf.nn.conv1d(emb, wConv2, stride = 1, padding='VALID')\n",
    "bias2 = conv2 + bConv2\n",
    "relu2 = tf.nn.relu(bias2)\n",
    "\n",
    "pool2 = tf.nn.max_pool([relu2], ksize = [1, 1, max_sent_len - ftr_sz[1] + 1, 1],\n",
    "                       strides = [1,1,1,1], padding = 'VALID')\n",
    "\n",
    "# Third convolution layer\n",
    "wts = model.get_layer('conv1d_3').get_weights()\n",
    "wConv3 = tf.constant(wts[0])\n",
    "bConv3  = tf.constant(wts[1])\n",
    "\n",
    "conv3 = tf.nn.conv1d(emb, wConv3, stride = 1, padding='VALID')\n",
    "bias3 = conv3 + bConv3\n",
    "relu3 = tf.nn.relu(bias3)\n",
    "\n",
    "pool3 = tf.nn.max_pool([relu3], ksize = [1, 1, max_sent_len - ftr_sz[2] + 1, 1],\n",
    "                       strides = [1,1,1,1], padding = 'VALID')\n",
    "\n",
    "\n",
    "flat = tf.concat([pool1[0,:,0,:], pool2[0,:,0,:], pool3[0,:,0,:]], axis = 1)\n",
    "\n",
    "wts = model.get_layer('dense_1').get_weights()\n",
    "wDen1 = tf.constant(wts[0])\n",
    "bDen1 = tf.constant(np.reshape(wts[1], (1, wts[1].shape[0],)))\n",
    "\n",
    "den1 = tf.matmul(flat, wDen1)\n",
    "den1f = tf.add(bDen1, den1)\n",
    "den1f = tf.nn.relu(den1f)\n",
    "\n",
    "wts = model.get_layer('dense_2').get_weights()\n",
    "wDen2 = tf.constant(wts[0])\n",
    "bDen1 = tf.constant(np.reshape(wts[1], (1, wts[1].shape[0],)))\n",
    "\n",
    "den2 = tf.matmul(den1f, wDen2)\n",
    "den2f = tf.add(bDen2, den2)\n",
    "\n",
    "final = tf.nn.softmax_cross_entropy_with_logits(labels=den2f, logits=lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_441:0' shape=(2, 5) dtype=float64>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.eval(feed_dict={inp:})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_wt = out[1][0]\n",
    "# 0 456 134 CNN rand\n",
    "k = 0\n",
    "list_ind = word_id_test[ind[k]]\n",
    "x_input = embed_wt[list_ind, :]\n",
    "y_final = y_out[:,y_actual[ind[k]]]\n",
    "y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  551,    68,  1079,  4577,   433, 11991, 11991, 11991, 11991,\n",
       "       11991, 11991, 11991, 11991, 11991, 11991, 11991, 11991, 11991,\n",
       "       11991, 11991, 11991, 11991, 11991, 11991, 11991, 11991, 11991,\n",
       "       11991, 11991, 11991, 11991, 11991, 11991, 11991, 11991, 11991, 11991], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0,]\n",
    "x_test[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "(3, 100, 50) (4, 100, 50) (5, 100, 50)\n",
      "(50,) (50,) (50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = out[1][9]\n",
    "y1 = w1 @ y_final\n",
    "y1 = y1 * (y1 > 0)\n",
    "\n",
    "w2 = out[1][7]\n",
    "y2 = w2 @ y1\n",
    "y2 = y2 * (y2 > 0) # Guided backward pass: backprop\n",
    "print(y2.shape)\n",
    "\n",
    "z1 = y2[0:50,]\n",
    "z2 = y2[50:100,]\n",
    "z3 = y2[100:150,]\n",
    "\n",
    "W1 = out[1][1]\n",
    "W2 = out[1][3]\n",
    "W3 = out[1][5]\n",
    "print(W1.shape, W2.shape, W3.shape)\n",
    "\n",
    "x_viz = x_input\n",
    "\n",
    "\n",
    "o1 = np.empty((50, ), np.float32)\n",
    "\n",
    "i1 = np.empty((50, ), np.int32)\n",
    "for i in range(50):\n",
    "    output = convolve2d(x_viz, W1[:,:,i], mode='valid')\n",
    "    output = output * (output > 0) \n",
    "    o1[i] = np.max(output, axis = 0)\n",
    "    i1[i] = np.argmax(output, axis = 0)\n",
    "    \n",
    "o2 = np.empty((50, ), np.float32)\n",
    "i2 = np.empty((50, ), np.int32)\n",
    "for i in range(50):\n",
    "    output = convolve2d(x_viz, W2[:,:,i], mode='valid')\n",
    "    output = output * (output > 0) \n",
    "    o2[i] = np.max(output, axis=0)\n",
    "    i2[i] = np.argmax(output, axis=0)\n",
    "    \n",
    "o3 = np.empty((50, ), np.float32)\n",
    "i3 = np.empty((50, ), np.int32)\n",
    "for i in range(50):\n",
    "    output = convolve2d(x_viz, W3[:,:,i], mode='valid')\n",
    "    output = output * (output > 0) \n",
    "    o3[i] = np.max(output, axis=0)\n",
    "    i3[i] = np.argmax(output, axis=0)\n",
    "    \n",
    "print(o1.shape, o2.shape, o3.shape)\n",
    "\n",
    "# Backward pass guided backpropogation (backprop)\n",
    "t1 = o1 * z1\n",
    "t2 = o2 * z2\n",
    "t3 = o3 * z3\n",
    "\n",
    "# Backward pass guided backpropogation\n",
    "# t1 = z1\n",
    "# t2 = z2\n",
    "# t3 = z3\n",
    "\n",
    "# t1 = o1 \n",
    "# t2 = o2 \n",
    "# t3 = o3 \n",
    "\n",
    "tsum = np.sum(t1) + np.sum(t2) + np.sum(t3)\n",
    "\n",
    "c = np.zeros((21,), np.float32)\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    for j in range(3):\n",
    "        c[i1[i] + j] += 1/3*t1[i]/tsum\n",
    "        \n",
    "for i in range(50):\n",
    "    for j in range(4):\n",
    "        c[i2[i] + j] += 1/4*t2[i]/tsum\n",
    "\n",
    "for i in range(50):\n",
    "    for j in range(5):\n",
    "        c[i3[i] + j] += 1/5*t3[i]/tsum\n",
    "        \n",
    "c\n",
    "np.sum(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .'"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs_test[ind[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wasabi', 0.23152487),\n",
       " ('good', 0.2180485),\n",
       " ('fun', 0.17998767),\n",
       " ('movi', 0.15142322),\n",
       " ('place', 0.12892881),\n",
       " ('go', 0.056666099),\n",
       " ('start', 0.020311475),\n",
       " ('like', 0.0084058866),\n",
       " ('sometim', 0.0033888198),\n",
       " ('if', 0.001314683)]"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds = []\n",
    "sent = processed_docs_test[ind[k]]\n",
    "wrd_prob = dict()\n",
    "\n",
    "counter = 0\n",
    "for i in sent:\n",
    "    wrd_prob[i] = c[counter]\n",
    "    counter += 1\n",
    "    \n",
    "import operator\n",
    "# srt_list = sorted(wrd_prob.items())\n",
    "sorted_x = sorted(wrd_prob.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00131468,  0.00338882,  0.00840589,  0.0566661 ,  0.15142322,\n",
       "        0.17998767,  0.23152487,  0.2180485 ,  0.12892881,  0.02031147,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
