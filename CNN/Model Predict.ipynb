{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "from w2v import train_word2vec \n",
    "import pickle, datetime\n",
    "import difflib\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Embedding, regularizers\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "import re\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9645, 37) (9645, 5) (2210, 37) (2210, 5)\n"
     ]
    }
   ],
   "source": [
    "fname = '../../Datasets/SST1_dataset/sst_data'\n",
    "with open(fname, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "x_train, y_train, x_test, y_test, dictionary = data\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing Word2Vec model '300features_1minwords_10context'\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "embedding_wts = train_word2vec( np.vstack((x_train, x_test)), \n",
    "                                dictionary.id2token, num_features = embedding_dim)\n",
    "x_train = embedding_wts[0][x_train]\n",
    "x_test  = embedding_wts[0][x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(fname):\n",
    "    model = keras.models.model_from_json(open(fname + '.json').read())\n",
    "    model.load_weights(fname + '_weights.h5')\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat('static_res.mat', {'static_pred':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'dodo_cnn_static3'\n",
    "model = load_model(fname)\n",
    "y_pred = model.predict(x_test)\n",
    "with open('static_res.txt', 'wb') as f:\n",
    "    pickle.dump([y_pred], f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38325791855203623"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label = np.argmax(y_test, axis =1 )\n",
    "y_prediction = np.argmax(y_pred, axis=1)\n",
    "\n",
    "np.sum(y_label == y_prediction) / 2210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 93.99%\n",
      "Test Accuracy: 38.33%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_wts = train_word2vec( np.vstack((x_train, x_test)), \n",
    "                                dictionary.id2token, num_features = embedding_dim)\n",
    "if model_type == 'CNN-static':\n",
    "    x_train = embedding_wts[0][x_train]\n",
    "    x_test  = embedding_wts[0][x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2210.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
